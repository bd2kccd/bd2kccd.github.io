{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to CCD Docs\n\n\nThis site hosts documentation for the \nCenter for Causal Discovery\n.\n\n\nTools and Software\n\n\ncausal-cmd\n - a Java API and command line implementation of algorithms for performing causal discovery on Big Data. Use this software if you are interested incorporating analysis via a shell script or in a Java-based program. The software currently includes Fast Greedy Search (\nFGES\n) for continuous or discrete variables \u2013 an optimized version of Greedy Equivalence Search (\nGES\n) tested with datasets that contain as many as 1 million continuous variables, and Greedy Fast Causal Inference (\nGFCI\n) for continuous or discrete variables.\n\n\n\n\nDownload the latest release i.e., causal-cmd-X.X.X-jar-with-dependencies.jar\n\n\nDocumentation for the big data enabled causal discovery algorithms\n\n\nReport bugs or issues with the software\n\n\nGithub project\n\n\n\n\nTetrad\n - a Java API, and desktop environment for learning, performing analyses and experimenting with causal discovery algorithms.\n\n\n\n\nDownload the latest Tetrad GUI Application i.e., tetrad-gui-x.x.x-launch.jar\n\n\nTetrad Project Website\n\n\nDocumentation for the big data enabled causal discovery algorithms\n\n\nGithub project\n\n\n\n\nCausal Web App\n \u2013 our user-friendly web-based graphical interface for performing causal discovery analysis on big data using large memory servers at the Pittsburgh Supercomputing Center. Use this software if you want to quickly try out a causal discovery algorithm or if you have big data which cannot be analyzed on your local hardware.\n\n\n\n\nCCD Web Application\n\n\nDocumentation for the big data enabled causal discovery algorithms\n\n\nGithub project\n\n\n\n\nCausal REST API\n \u2013 our RESTful API for Causal Web App. Once you create a new user account via Causal Web App, you can use this REST API to upload data files and run Causal Discovery Algorithms.\n\n\n\n\nGithub project\n\n\n\n\nPy-causal\n - a python module that wraps algorithms for performing causal discovery on big data. The software currently includes Fast Greedy Search (\nFGES\n) for both continuous and discrete variables, and Greedy Fast Causal Inference (\nGFCI\n) for continuous and discretevariables.\n\n\n\n\nGithub project\n\n\nDocker container of Jupyter Notebook with Py-causal configured\n\n\nDocumentation for the big data enabled causal discovery algorithms\n\n\n\n\nR-causal\n - an R module that that wraps algorithms for performing causal discovery on big data. The software currently includes Fast Greedy Search (\nFGES\n) for both continuous and discrete variables, and Greedy Fast Causal Inference (\nGFCI\n) for continuous variables.\n\n\n\n\nGithub project\n\n\nDocker container of Jupyter Notebook with R-causal configured\n\n\nDocumentation for the big data enabled causal discovery algorithms\n\n\n\n\ncytoscape-tetrad\n - a native cytoscape plugin that imports tetrad txt output files that contain the structure of a causal graph. It handles causal graphs and partial ancestral graphs.\n- \nPlugin\n\n- \nGithub project\n\n\nIf you use our software in your research, please acknowledge the Center for Causal Discovery, supported by \ngrant U54HG008540\n, in any papers, presentations, or other dissemination of your work.\n\n\nAll software is open-source and released under a dual licensing model. For non-profit institutions, the software is available under the GNU General Public License (GPL) v2 license. For-profit organizations that wish to commercialize enhanced or customized versions of the software will be able to purchase a commercial license on a case-by-case basis. The GPL license permits individuals to modify the source code and to share modifications with other colleagues/investigators. Specifically, it permits the dissemination and commercialization of enhanced or customized versions as well as incorporation of the software or its pieces into other license-compatible software packages, as long as modifications or enhancements are made open source.\n\n\nBy using software provided by the Center for Causal Discovery, you agree that no warranties of any kind are made by Carnegie Mellon University or the University of Pittsburgh with respect to the data provided by the software or any use thereof, and the universities hereby disclaim the implied warranties of merchantability, fitness for a particular purpose, and non-infringement. The universities shall not be liable for any claims, losses, or damages of any kind arising from the data provided by the software or any use thereof.", 
            "title": "Welcome to CCD Docs"
        }, 
        {
            "location": "/#welcome-to-ccd-docs", 
            "text": "This site hosts documentation for the  Center for Causal Discovery .", 
            "title": "Welcome to CCD Docs"
        }, 
        {
            "location": "/#tools-and-software", 
            "text": "causal-cmd  - a Java API and command line implementation of algorithms for performing causal discovery on Big Data. Use this software if you are interested incorporating analysis via a shell script or in a Java-based program. The software currently includes Fast Greedy Search ( FGES ) for continuous or discrete variables \u2013 an optimized version of Greedy Equivalence Search ( GES ) tested with datasets that contain as many as 1 million continuous variables, and Greedy Fast Causal Inference ( GFCI ) for continuous or discrete variables.   Download the latest release i.e., causal-cmd-X.X.X-jar-with-dependencies.jar  Documentation for the big data enabled causal discovery algorithms  Report bugs or issues with the software  Github project   Tetrad  - a Java API, and desktop environment for learning, performing analyses and experimenting with causal discovery algorithms.   Download the latest Tetrad GUI Application i.e., tetrad-gui-x.x.x-launch.jar  Tetrad Project Website  Documentation for the big data enabled causal discovery algorithms  Github project   Causal Web App  \u2013 our user-friendly web-based graphical interface for performing causal discovery analysis on big data using large memory servers at the Pittsburgh Supercomputing Center. Use this software if you want to quickly try out a causal discovery algorithm or if you have big data which cannot be analyzed on your local hardware.   CCD Web Application  Documentation for the big data enabled causal discovery algorithms  Github project   Causal REST API  \u2013 our RESTful API for Causal Web App. Once you create a new user account via Causal Web App, you can use this REST API to upload data files and run Causal Discovery Algorithms.   Github project   Py-causal  - a python module that wraps algorithms for performing causal discovery on big data. The software currently includes Fast Greedy Search ( FGES ) for both continuous and discrete variables, and Greedy Fast Causal Inference ( GFCI ) for continuous and discretevariables.   Github project  Docker container of Jupyter Notebook with Py-causal configured  Documentation for the big data enabled causal discovery algorithms   R-causal  - an R module that that wraps algorithms for performing causal discovery on big data. The software currently includes Fast Greedy Search ( FGES ) for both continuous and discrete variables, and Greedy Fast Causal Inference ( GFCI ) for continuous variables.   Github project  Docker container of Jupyter Notebook with R-causal configured  Documentation for the big data enabled causal discovery algorithms   cytoscape-tetrad  - a native cytoscape plugin that imports tetrad txt output files that contain the structure of a causal graph. It handles causal graphs and partial ancestral graphs.\n-  Plugin \n-  Github project  If you use our software in your research, please acknowledge the Center for Causal Discovery, supported by  grant U54HG008540 , in any papers, presentations, or other dissemination of your work.  All software is open-source and released under a dual licensing model. For non-profit institutions, the software is available under the GNU General Public License (GPL) v2 license. For-profit organizations that wish to commercialize enhanced or customized versions of the software will be able to purchase a commercial license on a case-by-case basis. The GPL license permits individuals to modify the source code and to share modifications with other colleagues/investigators. Specifically, it permits the dissemination and commercialization of enhanced or customized versions as well as incorporation of the software or its pieces into other license-compatible software packages, as long as modifications or enhancements are made open source.  By using software provided by the Center for Causal Discovery, you agree that no warranties of any kind are made by Carnegie Mellon University or the University of Pittsburgh with respect to the data provided by the software or any use thereof, and the universities hereby disclaim the implied warranties of merchantability, fitness for a particular purpose, and non-infringement. The universities shall not be liable for any claims, losses, or damages of any kind arising from the data provided by the software or any use thereof.", 
            "title": "Tools and Software"
        }, 
        {
            "location": "/causal-cmd/", 
            "text": "causal-cmd\n\n\nIntroduction\n\n\nCausal-cmd is a Java application that provides a Command-Line Interface (CLI) tool for causal discovery algorithms produced by the \nCenter for Causal Discovery\n.  The application currently includes the following algorithms:\n\n\n\n\nFGESc - a version of FGES (Fast Greedy Search is an optimization and parallelized version of the Greedy Equivalence Search algorithm (GES)) that works with continuous variables\n\n\nFGESd - a version of FGES that works with discrete variables\n\n\nFGESm - a version of FGES that works with discrete and mixed variables\n\n\nGFCIc - a version of GFCI (Greedy Fast Causal Inference) that works with continuous variables\n\n\nGFCId - a version of GFCI that works with discrete variables\n\n\nGFCIm - a version of GFCI that works with mixed and discrete variables\n\n\n\n\nCausal discovery algorithms are a class of search algorithms that explore a space of graphical causal models, i.e., graphical models where directed edges imply causation, for a model (or models) that are a good fit for a dataset. We suggest that newcomers to the field review Causation, Prediction and Search by Spirtes, Glymour and Scheines for a primer on the subject.\n\n\nCausal discovery algorithms allow a user to uncover the causal relationships between variables in a dataset. These discovered causal relationships may be used further--understanding the underlying the processes of a system (e.g., the metabolic pathways of an organism), hypothesis generation (e.g., variables that best explain an outcome), guide experimentation (e.g., what gene knockout experiments should be performed) or prediction (e.g. parameterization of the causal graph using data and then using it as a classifier).\n\n\nCommand Line Usage\n\n\nJava 8 or higher is the only prerequisite to run the software. Note that by default Java will allocate the smaller of 1/4 system memory or 1GB to the Java virtual machine (JVM). If you run out of memory (heap memory space) running your analyses you should increase the memory allocated to the JVM with the following switch '-XmxXXG' where XX is the number of gigabytes of ram you allow the JVM to utilize. For example to allocate 8 gigabytes of ram you would add -Xmx8G immediately after the java command.\n\n\nIn this example, we'll use download the \nRetention.txt\n file, which is a dataset containing information on college graduation and used in the publication of \"What Do College Ranking Data Tell Us About Student Retention?\" by Drudzel and Glymour, 1994.\n\n\nKeep in mind that causal-cmd has different switches for different algorithms. To start, type the following command in your terminal:\n\n\njava -jar causal-cmd-x.x.x.jar\n\n\n\n\n Note: we are using \ncausal-cmd-x.x.x.jar\n to indicate the actual executable jar of specific version number that is being used. \n\n\nAnd you'll see the following instructions:\n\n\nusage: java -jar causal-cmd-x.x.x.jar --algorithm \narg\n | --simulate-data \narg\n  [--version]\n    --algorithm \narg\n       FGESc, FGESd, GFCIc, GFCId\n    --simulate-data \narg\n   sem-rand-fwd, bayes-net-rand-fwd\n    --version               Show software version.\n\n\n\n\n\nIn this example, we'll be running FGESc on this \nRetention.txt\n.\n\n\njava -jar causal-cmd-x.x.x.jar --algorithm FGESc --data Retention.txt\n\n\n\n\nThis command will output the  following messages in your terminal:\n\n\n================================================================================\nFGES Continuous (Wed, March 22, 2017 10:43:43 AM)\n================================================================================\ndata = Retention.txt\ndelimiter = tab\nverbose = false\nthread = 2\npenalty discount = 4.000000\nmax degree = 100\nfaithfulness assumed = false\nensure variable names are unique = true\nensure variables have non-zero variance = true\nout = .\noutput-prefix = FGESc_Retention.txt_1490193823839\nno-validation-output = false\n\nRunning version 0.1.0 but unable to contact latest version server.  To disable checking use the skip-latest option.\nThere are 170 cases and 8 variables.\nWed, March 22, 2017 10:43:45 AM: Start reading in data file.\nWed, March 22, 2017 10:43:45 AM: End reading in data file.\nWed, March 22, 2017 10:43:45 AM: Start running algorithm FGES (Fast Greedy Equivalence Search) using Sem BIC Score.\nWed, March 22, 2017 10:43:45 AM: End running algorithm FGES (Fast Greedy Equivalence Search) using Sem BIC Score.\n\n\n\n\nAt the same time, this program will also write the results of the FGESc search procedure into a text file named like \"FGESc_Retention.txt_1490193823839.txt\". Below is the content of this result file:\n\n\n================================================================================\nFGES Continuous (Wed, March 22, 2017 10:43:43 AM)\n================================================================================\n\nRuntime Parameters:\nverbose = false\nnumber of threads = 2\n\nDataset:\nfile = Retention.txt\ndelimiter = tab\ncases read in = 170\nvariables read in = 8\n\nAlgorithm Parameters:\npenalty discount = 4.000000\nmax degree = 100\nfaithfulness assumed = false\n\nData Validations:\nensure variable names are unique = true\nensure variables have non-zero variance = true\n\n\nGraph Nodes:\nspending_per_stdt,grad_rate,stdt_clss_stndng,rjct_rate,tst_scores,stdt_accept_rate,stdt_tchr_ratio,fac_salary\n\nGraph Edges:\n1. grad_rate --- tst_scores\n2. spending_per_stdt --- stdt_tchr_ratio\n3. stdt_clss_stndng --- rjct_rate\n4. tst_scores --- fac_salary\n5. tst_scores --- spending_per_stdt\n6. tst_scores --- stdt_clss_stndng\n\n\n\n\nThe end of the file contains the causal graph edgesfrom the search procedure. Here is a key to the edge types:\n\n\n\n\nA --- B - There is causal relationship between variable A and B but we cannot determine the direction of the relationship\n\n\nA --\n B - There is a causal relationship from variable A to B\n\n\n\n\nThe GFCI algorithm has additional edge types:\n\n\n\n\nA \n-\n B - There is an unmeasured confounder of A and B\n\n\nA o-\n B - Either A is a cause of B or there is an unmeasured confounder of A and B or both\n\n\n\n\nA o-o B - Either (1) A is a cause of B or B is a cause of A, or (2) there is an unmeasured confounder of A and B, or both 1 and 2 hold.\n\n\n\n\n\n\nA --\n B dd nl - Definitely direct causal relationship and no latent confounder\n\n\n\n\nA --\n B pd nl - Possibly direct and no latent confounder\n\n\nA --\n B pd pl - Possibly direct and possibly latent confounder\n\n\n\n\nNote: the generated result file name is based on the system clock.\n\n\nComplete Usage Guide\n\n\nusage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm \narg\n | --simulate-data \narg\n  [--version]\n    --algorithm \narg\n       FGESc, FGESd, FGESm-cg, GFCIc, GFCId, GFCIm-cg\n    --simulate-data \narg\n   sem-rand-fwd, bayes-net-rand-fwd, lee-hastie\n    --version               Show software version.\n\n\n\n\nYou can use the \n--algorithm \narg\n parameter to see specific algorithm usage information, which we'll also list below.\n\n\nFGESc\n\n\nusage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm FGESc [-d \narg\n] [--exclude-variables \narg\n] -f \narg\n [--faithfulness-assumed] [--help] [--json] [--knowledge \narg\n] [--max-degree \narg\n] [--no-validation-output] [-o \narg\n] [--output-prefix \narg\n] [--penalty-discount \narg\n] [--skip-latest] [--skip-nonzero-variance] [--skip-unique-var-name] [--structure-prior \narg\n] [--symmetric-first-step] [--tetrad-graph-json] [--thread \narg\n] [--verbose]\n -d,--delimiter \narg\n           Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables \narg\n   A file containing variables to exclude.\n -f,--data \narg\n                Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge \narg\n           A file containing prior knowledge.\n    --max-degree \narg\n          The maximum degree of the graph.. Default is 100.\n    --no-validation-output      No validation output files created.\n -o,--out \narg\n                 Output directory.\n    --output-prefix \narg\n       Prefix name for output files.\n    --penalty-discount \narg\n    Penalty discount. Default is 2.0.\n    --skip-latest               Skip checking for latest software version\n    --skip-nonzero-variance     Skip check for zero variance variables.\n    --skip-unique-var-name      Skip check for unique variable names.\n    --structure-prior \narg\n     Structure prior coefficient. Default is 1.0.\n    --symmetric-first-step      Yes if the first step step for FGES should do scoring for both X-\nY and Y-\nX. Default is false.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread \narg\n              Number of threads.\n    --verbose                   Print additional information.\n\n\n\n\nFGESd\n\n\nusage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm FGESd [-d \narg\n] [--exclude-variables \narg\n] -f \narg\n [--faithfulness-assumed] [--help] [--json] [--knowledge \narg\n] [--max-degree \narg\n] [--no-validation-output] [-o \narg\n] [--output-prefix \narg\n] [--sample-prior \narg\n] [--skip-category-limit] [--skip-latest] [--skip-unique-var-name] [--structure-prior \narg\n] [--symmetric-first-step] [--tetrad-graph-json] [--thread \narg\n] [--verbose]\n -d,--delimiter \narg\n           Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables \narg\n   A file containing variables to exclude.\n -f,--data \narg\n                Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge \narg\n           A file containing prior knowledge.\n    --max-degree \narg\n          The maximum degree of the graph.. Default is 100.\n    --no-validation-output      No validation output files created.\n -o,--out \narg\n                 Output directory.\n    --output-prefix \narg\n       Prefix name for output files.\n    --sample-prior \narg\n        Sample prior. Default is 1.0.\n    --skip-category-limit       Skip 'limit number of categories' check.\n    --skip-latest               Skip checking for latest software version\n    --skip-unique-var-name      Skip check for unique variable names.\n    --structure-prior \narg\n     Structure prior coefficient. Default is 1.0.\n    --symmetric-first-step      Yes if the first step step for FGES should do scoring for both X-\nY and Y-\nX. Default is false.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread \narg\n              Number of threads.\n    --verbose                   Print additional information.\n\n\n\n\nFGESm-cg\n\n\nusage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm FGESm-cg [-d \narg\n] [--discretize] [--exclude-variables \narg\n] -f \narg\n [--faithfulness-assumed] [--help] [--json] [--knowledge \narg\n] [--max-degree \narg\n] [--no-validation-output] [--num-categories-to-discretize \narg\n] [--num-discrete-categories \narg\n] [-o \narg\n] [--output-prefix \narg\n] [--penalty-discount \narg\n] [--skip-latest] [--structure-prior \narg\n] [--symmetric-first-step] [--tetrad-graph-json] [--thread \narg\n] [--verbose]\n -d,--delimiter \narg\n                      Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --discretize                           Yes if continuous variables should be discretized when child is discrete. Default is true.\n    --exclude-variables \narg\n              A file containing variables to exclude.\n -f,--data \narg\n                           Data file.\n    --faithfulness-assumed                 Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                                 Show help.\n    --json                                 Create JSON output.\n    --knowledge \narg\n                      A file containing prior knowledge.\n    --max-degree \narg\n                     The maximum degree of the graph.. Default is 100.\n    --no-validation-output                 No validation output files created.\n    --num-categories-to-discretize \narg\n   The number of categories used to discretize continuous variables, if necessary. Default is 3.\n    --num-discrete-categories \narg\n        Number of category considered discrete variable.\n -o,--out \narg\n                            Output directory.\n    --output-prefix \narg\n                  Prefix name for output files.\n    --penalty-discount \narg\n               Penalty discount. Default is 2.0.\n    --skip-latest                          Skip checking for latest software version\n    --structure-prior \narg\n                Structure prior coefficient. Default is 1.0.\n    --symmetric-first-step                 Yes if the first step step for FGES should do scoring for both X-\nY and Y-\nX. Default is false.\n    --tetrad-graph-json                    Create Tetrad Graph JSON output.\n    --thread \narg\n                         Number of threads.\n    --verbose                              Print additional information.\n\n\n\n\nGFCIc\n\n\nusage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm GFCIc [--alpha \narg\n] [-d \narg\n] [--exclude-variables \narg\n] -f \narg\n [--faithfulness-assumed] [--help] [--json] [--knowledge \narg\n] [--max-degree \narg\n] [--max-path-length \narg\n] [--no-validation-output] [-o \narg\n] [--output-prefix \narg\n] [--penalty-discount \narg\n] [--skip-latest] [--skip-nonzero-variance] [--skip-unique-var-name] [--tetrad-graph-json] [--thread \narg\n] [--use-complete-rule-set] [--verbose]\n    --alpha \narg\n               Cutoff for p values (alpha). Default is 0.01.\n -d,--delimiter \narg\n           Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables \narg\n   A file containing variables to exclude.\n -f,--data \narg\n                Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge \narg\n           A file containing prior knowledge.\n    --max-degree \narg\n          The maximum degree of the graph.. Default is 100.\n    --max-path-length \narg\n     The maximum length for any discriminating path. -1 if unlimited. Default is -1.\n    --no-validation-output      No validation output files created.\n -o,--out \narg\n                 Output directory.\n    --output-prefix \narg\n       Prefix name for output files.\n    --penalty-discount \narg\n    Penalty discount. Default is 2.0.\n    --skip-latest               Skip checking for latest software version\n    --skip-nonzero-variance     Skip check for zero variance variables.\n    --skip-unique-var-name      Skip check for unique variable names.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread \narg\n              Number of threads.\n    --use-complete-rule-set\n    --verbose                   Print additional information.\n\n\n\n\nGFCId\n\n\nusage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm GFCId [--alpha \narg\n] [-d \narg\n] [--exclude-variables \narg\n] -f \narg\n [--faithfulness-assumed] [--help] [--json] [--knowledge \narg\n] [--max-degree \narg\n] [--max-path-length \narg\n] [--no-validation-output] [-o \narg\n] [--output-prefix \narg\n] [--sample-prior \narg\n] [--skip-category-limit] [--skip-latest] [--skip-unique-var-name] [--structure-prior \narg\n] [--tetrad-graph-json] [--thread \narg\n] [--use-complete-rule-set] [--verbose]\n    --alpha \narg\n               Cutoff for p values (alpha). Default is 0.01.\n -d,--delimiter \narg\n           Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables \narg\n   A file containing variables to exclude.\n -f,--data \narg\n                Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge \narg\n           A file containing prior knowledge.\n    --max-degree \narg\n          The maximum degree of the graph.. Default is 100.\n    --max-path-length \narg\n     The maximum length for any discriminating path. -1 if unlimited. Default is -1.\n    --no-validation-output      No validation output files created.\n -o,--out \narg\n                 Output directory.\n    --output-prefix \narg\n       Prefix name for output files.\n    --sample-prior \narg\n        Sample prior. Default is 1.0.\n    --skip-category-limit       Skip 'limit number of categories' check.\n    --skip-latest               Skip checking for latest software version\n    --skip-unique-var-name      Skip check for unique variable names.\n    --structure-prior \narg\n     Structure prior coefficient. Default is 1.0.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread \narg\n              Number of threads.\n    --use-complete-rule-set\n    --verbose                   Print additional information.\n\n\n\n\nGFCIm-cg\n\n\nusage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm GFCIm-cg [--alpha \narg\n] [-d \narg\n] [--discretize] [--exclude-variables \narg\n] -f \narg\n [--faithfulness-assumed] [--help] [--json] [--knowledge \narg\n] [--max-degree \narg\n] [--max-path-length \narg\n] [--no-validation-output] [--num-categories-to-discretize \narg\n] [--num-discrete-categories \narg\n] [-o \narg\n] [--output-prefix \narg\n] [--penalty-discount \narg\n] [--skip-latest] [--structure-prior \narg\n] [--tetrad-graph-json] [--thread \narg\n] [--use-complete-rule-set] [--verbose]\n    --alpha \narg\n                          Cutoff for p values (alpha). Default is 0.01.\n -d,--delimiter \narg\n                      Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --discretize                           Yes if continuous variables should be discretized when child is discrete. Default is true.\n    --exclude-variables \narg\n              A file containing variables to exclude.\n -f,--data \narg\n                           Data file.\n    --faithfulness-assumed                 Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                                 Show help.\n    --json                                 Create JSON output.\n    --knowledge \narg\n                      A file containing prior knowledge.\n    --max-degree \narg\n                     The maximum degree of the graph.. Default is 100.\n    --max-path-length \narg\n                The maximum length for any discriminating path. -1 if unlimited. Default is -1.\n    --no-validation-output                 No validation output files created.\n    --num-categories-to-discretize \narg\n   The number of categories used to discretize continuous variables, if necessary. Default is 3.\n    --num-discrete-categories \narg\n        Number of category considered discrete variable.\n -o,--out \narg\n                            Output directory.\n    --output-prefix \narg\n                  Prefix name for output files.\n    --penalty-discount \narg\n               Penalty discount. Default is 2.0.\n    --skip-latest                          Skip checking for latest software version\n    --structure-prior \narg\n                Structure prior coefficient. Default is 1.0.\n    --tetrad-graph-json                    Create Tetrad Graph JSON output.\n    --thread \narg\n                         Number of threads.\n    --use-complete-rule-set\n    --verbose                              Print additional information.\n\n\n\n\nSample Prior Knowledge File\n\n\nFrom the above useage guide, we see the option of \n--knowledge \narg\n, with which we can specify the prior knowledge file. Below is the content of a sample prior knowledge file:\n\n\n/knowledge\n\naddtemporal\n1 spending_per_stdt fac_salary stdt_tchr_ratio \n2 rjct_rate stdt_accept_rate \n3 tst_scores stdt_clss_stndng \n4* grad_rate \n\nforbiddirect\nx3 x4\n\nrequiredirect\nx1 x2\n\n\n\n\nThe first line of the prior knowledge file must say \n/knowledge\n. And a prior knowledge file consists of three sections:\n\n\n\n\naddtemporal - tiers of variables where the first tier preceeds the last. Adding a asterisk next to the tier id prohibits edges between tier variables\n\n\nforbiddirect - forbidden edges indicated by a list of pairs of variables\n\n\nrequireddirect - required edges indicated by a list of pairs of variables\n\n\n\n\nAPI Usage\n\n\nIn addition to using causal-cmd directly in the command line interface, you can also use the Tetred library that is includedin in causal-cmd as an \nJava API\n. Here we provide an example of how to run \nFGESc\n algorithm using this API.\n\n\nimport edu.cmu.tetrad.data.DataModel;\nimport edu.cmu.tetrad.data.DataSet;\nimport edu.cmu.tetrad.graph.Graph;\nimport edu.cmu.tetrad.algcomparison.score.SemBicScore;\nimport edu.cmu.tetrad.algcomparison.algorithm.oracle.pattern.Fges;\nimport edu.cmu.tetrad.util.Parameters;\nimport edu.pitt.dbmi.causal.cmd.ParamAttrs;\nimport edu.pitt.dbmi.causal.cmd.util.TetradDataUtils;\nimport edu.pitt.dbmi.causal.cmd.validation.TetradDataValidation;\nimport edu.pitt.dbmi.causal.cmd.validation.UniqueVariableValidation;\nimport edu.pitt.dbmi.data.Dataset;\nimport edu.pitt.dbmi.data.Delimiter;\nimport edu.pitt.dbmi.data.reader.tabular.ContinuousTabularDataFileReader;\nimport edu.pitt.dbmi.data.reader.tabular.TabularDataReader;\nimport edu.pitt.dbmi.data.validation.ValidationCode;\nimport edu.pitt.dbmi.data.validation.ValidationResult;\nimport edu.pitt.dbmi.data.validation.tabular.ContinuousTabularDataFileValidation;\nimport edu.pitt.dbmi.data.validation.tabular.TabularDataValidation;\n\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.List;\n\npublic class FGEScApiExample {\n\n    /**\n    * @param args the command line arguments\n    */\n    public static void main(String[] args) throws Exception {\n        // Set the data file and its properites\n        Path dataFile = Paths.get(\ntest\n, \ndata\n, \ncmu\n, \nRetention.txt\n);\n        Delimiter delimiter = Delimiter.TAB;\n        char quoteCharacter = '\n';\n        String missingValueMarker = \n*\n;\n        String commentMarker = \n//\n;\n\n        // Data file validation: ensure the data file is valid format\n        // This is optional if you don't need to validate the data file\n        TabularDataValidation dataFileValidation = new ContinuousTabularDataFileValidation(dataFile.toFile(), delimiter);\n        dataFileValidation.setQuoteCharacter(quoteCharacter);\n        dataFileValidation.setMissingValueMarker(missingValueMarker);\n        dataFileValidation.setCommentMarker(commentMarker);\n        dataFileValidation.validate();\n\n        // Ensure there is no error\n        int errorCount = 0;\n        List\nValidationResult\n fileValidResults = dataFileValidation.getValidationResults();\n        for (ValidationResult validation : fileValidResults) {\n            if (validation.getCode() == ValidationCode.ERROR) {\n                errorCount++;\n            }\n        }\n\n        // Read in data\n        // This is the actual data loading/reading section\n        TabularDataReader reader = new ContinuousTabularDataFileReader(dataFile.toFile(), delimiter);\n        reader.setQuoteCharacter(quoteCharacter);\n        reader.setMissingValueMarker(missingValueMarker);\n        reader.setCommentMarker(commentMarker);\n        Dataset dataset = reader.readInData();\n\n        // Convert to Tetrad data model\n        DataModel dataModel = TetradDataUtils.toDataModel(dataset);\n\n        // Data validation: ensure the data read in is valid\n        TetradDataValidation dataValidation = new UniqueVariableValidation((DataSet) dataModel);\n        boolean isValidData = dataValidation.validate(System.err, true);\n\n        // Set algorithm parameters\n        Parameters parameters = new Parameters();\n        parameters.set(ParamAttrs.PENALTY_DISCOUNT, 2.0);\n        parameters.set(ParamAttrs.MAX_DEGREE, -1);\n        parameters.set(ParamAttrs.FAITHFULNESS_ASSUMED, false);\n        parameters.set(ParamAttrs.VERBOSE, false);\n\n        // Specify which algorithm to use\n        Fges fges = new Fges(new SemBicScore());\n\n        // Run the algorithm on this data with specified parameters\n        // and return the Graph object\n        Graph graph = fges.search(dataModel, parameters);\n\n        System.out.println();\n        System.out.println(graph.toString().trim());\n        System.out.flush();\n    }\n\n}", 
            "title": "Causal CMD"
        }, 
        {
            "location": "/causal-cmd/#causal-cmd", 
            "text": "", 
            "title": "causal-cmd"
        }, 
        {
            "location": "/causal-cmd/#introduction", 
            "text": "Causal-cmd is a Java application that provides a Command-Line Interface (CLI) tool for causal discovery algorithms produced by the  Center for Causal Discovery .  The application currently includes the following algorithms:   FGESc - a version of FGES (Fast Greedy Search is an optimization and parallelized version of the Greedy Equivalence Search algorithm (GES)) that works with continuous variables  FGESd - a version of FGES that works with discrete variables  FGESm - a version of FGES that works with discrete and mixed variables  GFCIc - a version of GFCI (Greedy Fast Causal Inference) that works with continuous variables  GFCId - a version of GFCI that works with discrete variables  GFCIm - a version of GFCI that works with mixed and discrete variables   Causal discovery algorithms are a class of search algorithms that explore a space of graphical causal models, i.e., graphical models where directed edges imply causation, for a model (or models) that are a good fit for a dataset. We suggest that newcomers to the field review Causation, Prediction and Search by Spirtes, Glymour and Scheines for a primer on the subject.  Causal discovery algorithms allow a user to uncover the causal relationships between variables in a dataset. These discovered causal relationships may be used further--understanding the underlying the processes of a system (e.g., the metabolic pathways of an organism), hypothesis generation (e.g., variables that best explain an outcome), guide experimentation (e.g., what gene knockout experiments should be performed) or prediction (e.g. parameterization of the causal graph using data and then using it as a classifier).", 
            "title": "Introduction"
        }, 
        {
            "location": "/causal-cmd/#command-line-usage", 
            "text": "Java 8 or higher is the only prerequisite to run the software. Note that by default Java will allocate the smaller of 1/4 system memory or 1GB to the Java virtual machine (JVM). If you run out of memory (heap memory space) running your analyses you should increase the memory allocated to the JVM with the following switch '-XmxXXG' where XX is the number of gigabytes of ram you allow the JVM to utilize. For example to allocate 8 gigabytes of ram you would add -Xmx8G immediately after the java command.  In this example, we'll use download the  Retention.txt  file, which is a dataset containing information on college graduation and used in the publication of \"What Do College Ranking Data Tell Us About Student Retention?\" by Drudzel and Glymour, 1994.  Keep in mind that causal-cmd has different switches for different algorithms. To start, type the following command in your terminal:  java -jar causal-cmd-x.x.x.jar   Note: we are using  causal-cmd-x.x.x.jar  to indicate the actual executable jar of specific version number that is being used.   And you'll see the following instructions:  usage: java -jar causal-cmd-x.x.x.jar --algorithm  arg  | --simulate-data  arg   [--version]\n    --algorithm  arg        FGESc, FGESd, GFCIc, GFCId\n    --simulate-data  arg    sem-rand-fwd, bayes-net-rand-fwd\n    --version               Show software version.  In this example, we'll be running FGESc on this  Retention.txt .  java -jar causal-cmd-x.x.x.jar --algorithm FGESc --data Retention.txt  This command will output the  following messages in your terminal:  ================================================================================\nFGES Continuous (Wed, March 22, 2017 10:43:43 AM)\n================================================================================\ndata = Retention.txt\ndelimiter = tab\nverbose = false\nthread = 2\npenalty discount = 4.000000\nmax degree = 100\nfaithfulness assumed = false\nensure variable names are unique = true\nensure variables have non-zero variance = true\nout = .\noutput-prefix = FGESc_Retention.txt_1490193823839\nno-validation-output = false\n\nRunning version 0.1.0 but unable to contact latest version server.  To disable checking use the skip-latest option.\nThere are 170 cases and 8 variables.\nWed, March 22, 2017 10:43:45 AM: Start reading in data file.\nWed, March 22, 2017 10:43:45 AM: End reading in data file.\nWed, March 22, 2017 10:43:45 AM: Start running algorithm FGES (Fast Greedy Equivalence Search) using Sem BIC Score.\nWed, March 22, 2017 10:43:45 AM: End running algorithm FGES (Fast Greedy Equivalence Search) using Sem BIC Score.  At the same time, this program will also write the results of the FGESc search procedure into a text file named like \"FGESc_Retention.txt_1490193823839.txt\". Below is the content of this result file:  ================================================================================\nFGES Continuous (Wed, March 22, 2017 10:43:43 AM)\n================================================================================\n\nRuntime Parameters:\nverbose = false\nnumber of threads = 2\n\nDataset:\nfile = Retention.txt\ndelimiter = tab\ncases read in = 170\nvariables read in = 8\n\nAlgorithm Parameters:\npenalty discount = 4.000000\nmax degree = 100\nfaithfulness assumed = false\n\nData Validations:\nensure variable names are unique = true\nensure variables have non-zero variance = true\n\n\nGraph Nodes:\nspending_per_stdt,grad_rate,stdt_clss_stndng,rjct_rate,tst_scores,stdt_accept_rate,stdt_tchr_ratio,fac_salary\n\nGraph Edges:\n1. grad_rate --- tst_scores\n2. spending_per_stdt --- stdt_tchr_ratio\n3. stdt_clss_stndng --- rjct_rate\n4. tst_scores --- fac_salary\n5. tst_scores --- spending_per_stdt\n6. tst_scores --- stdt_clss_stndng  The end of the file contains the causal graph edgesfrom the search procedure. Here is a key to the edge types:   A --- B - There is causal relationship between variable A and B but we cannot determine the direction of the relationship  A --  B - There is a causal relationship from variable A to B   The GFCI algorithm has additional edge types:   A  -  B - There is an unmeasured confounder of A and B  A o-  B - Either A is a cause of B or there is an unmeasured confounder of A and B or both   A o-o B - Either (1) A is a cause of B or B is a cause of A, or (2) there is an unmeasured confounder of A and B, or both 1 and 2 hold.    A --  B dd nl - Definitely direct causal relationship and no latent confounder   A --  B pd nl - Possibly direct and no latent confounder  A --  B pd pl - Possibly direct and possibly latent confounder   Note: the generated result file name is based on the system clock.", 
            "title": "Command Line Usage"
        }, 
        {
            "location": "/causal-cmd/#complete-usage-guide", 
            "text": "usage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm  arg  | --simulate-data  arg   [--version]\n    --algorithm  arg        FGESc, FGESd, FGESm-cg, GFCIc, GFCId, GFCIm-cg\n    --simulate-data  arg    sem-rand-fwd, bayes-net-rand-fwd, lee-hastie\n    --version               Show software version.  You can use the  --algorithm  arg  parameter to see specific algorithm usage information, which we'll also list below.", 
            "title": "Complete Usage Guide"
        }, 
        {
            "location": "/causal-cmd/#fgesc", 
            "text": "usage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm FGESc [-d  arg ] [--exclude-variables  arg ] -f  arg  [--faithfulness-assumed] [--help] [--json] [--knowledge  arg ] [--max-degree  arg ] [--no-validation-output] [-o  arg ] [--output-prefix  arg ] [--penalty-discount  arg ] [--skip-latest] [--skip-nonzero-variance] [--skip-unique-var-name] [--structure-prior  arg ] [--symmetric-first-step] [--tetrad-graph-json] [--thread  arg ] [--verbose]\n -d,--delimiter  arg            Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables  arg    A file containing variables to exclude.\n -f,--data  arg                 Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge  arg            A file containing prior knowledge.\n    --max-degree  arg           The maximum degree of the graph.. Default is 100.\n    --no-validation-output      No validation output files created.\n -o,--out  arg                  Output directory.\n    --output-prefix  arg        Prefix name for output files.\n    --penalty-discount  arg     Penalty discount. Default is 2.0.\n    --skip-latest               Skip checking for latest software version\n    --skip-nonzero-variance     Skip check for zero variance variables.\n    --skip-unique-var-name      Skip check for unique variable names.\n    --structure-prior  arg      Structure prior coefficient. Default is 1.0.\n    --symmetric-first-step      Yes if the first step step for FGES should do scoring for both X- Y and Y- X. Default is false.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread  arg               Number of threads.\n    --verbose                   Print additional information.", 
            "title": "FGESc"
        }, 
        {
            "location": "/causal-cmd/#fgesd", 
            "text": "usage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm FGESd [-d  arg ] [--exclude-variables  arg ] -f  arg  [--faithfulness-assumed] [--help] [--json] [--knowledge  arg ] [--max-degree  arg ] [--no-validation-output] [-o  arg ] [--output-prefix  arg ] [--sample-prior  arg ] [--skip-category-limit] [--skip-latest] [--skip-unique-var-name] [--structure-prior  arg ] [--symmetric-first-step] [--tetrad-graph-json] [--thread  arg ] [--verbose]\n -d,--delimiter  arg            Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables  arg    A file containing variables to exclude.\n -f,--data  arg                 Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge  arg            A file containing prior knowledge.\n    --max-degree  arg           The maximum degree of the graph.. Default is 100.\n    --no-validation-output      No validation output files created.\n -o,--out  arg                  Output directory.\n    --output-prefix  arg        Prefix name for output files.\n    --sample-prior  arg         Sample prior. Default is 1.0.\n    --skip-category-limit       Skip 'limit number of categories' check.\n    --skip-latest               Skip checking for latest software version\n    --skip-unique-var-name      Skip check for unique variable names.\n    --structure-prior  arg      Structure prior coefficient. Default is 1.0.\n    --symmetric-first-step      Yes if the first step step for FGES should do scoring for both X- Y and Y- X. Default is false.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread  arg               Number of threads.\n    --verbose                   Print additional information.", 
            "title": "FGESd"
        }, 
        {
            "location": "/causal-cmd/#fgesm-cg", 
            "text": "usage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm FGESm-cg [-d  arg ] [--discretize] [--exclude-variables  arg ] -f  arg  [--faithfulness-assumed] [--help] [--json] [--knowledge  arg ] [--max-degree  arg ] [--no-validation-output] [--num-categories-to-discretize  arg ] [--num-discrete-categories  arg ] [-o  arg ] [--output-prefix  arg ] [--penalty-discount  arg ] [--skip-latest] [--structure-prior  arg ] [--symmetric-first-step] [--tetrad-graph-json] [--thread  arg ] [--verbose]\n -d,--delimiter  arg                       Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --discretize                           Yes if continuous variables should be discretized when child is discrete. Default is true.\n    --exclude-variables  arg               A file containing variables to exclude.\n -f,--data  arg                            Data file.\n    --faithfulness-assumed                 Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                                 Show help.\n    --json                                 Create JSON output.\n    --knowledge  arg                       A file containing prior knowledge.\n    --max-degree  arg                      The maximum degree of the graph.. Default is 100.\n    --no-validation-output                 No validation output files created.\n    --num-categories-to-discretize  arg    The number of categories used to discretize continuous variables, if necessary. Default is 3.\n    --num-discrete-categories  arg         Number of category considered discrete variable.\n -o,--out  arg                             Output directory.\n    --output-prefix  arg                   Prefix name for output files.\n    --penalty-discount  arg                Penalty discount. Default is 2.0.\n    --skip-latest                          Skip checking for latest software version\n    --structure-prior  arg                 Structure prior coefficient. Default is 1.0.\n    --symmetric-first-step                 Yes if the first step step for FGES should do scoring for both X- Y and Y- X. Default is false.\n    --tetrad-graph-json                    Create Tetrad Graph JSON output.\n    --thread  arg                          Number of threads.\n    --verbose                              Print additional information.", 
            "title": "FGESm-cg"
        }, 
        {
            "location": "/causal-cmd/#gfcic", 
            "text": "usage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm GFCIc [--alpha  arg ] [-d  arg ] [--exclude-variables  arg ] -f  arg  [--faithfulness-assumed] [--help] [--json] [--knowledge  arg ] [--max-degree  arg ] [--max-path-length  arg ] [--no-validation-output] [-o  arg ] [--output-prefix  arg ] [--penalty-discount  arg ] [--skip-latest] [--skip-nonzero-variance] [--skip-unique-var-name] [--tetrad-graph-json] [--thread  arg ] [--use-complete-rule-set] [--verbose]\n    --alpha  arg                Cutoff for p values (alpha). Default is 0.01.\n -d,--delimiter  arg            Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables  arg    A file containing variables to exclude.\n -f,--data  arg                 Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge  arg            A file containing prior knowledge.\n    --max-degree  arg           The maximum degree of the graph.. Default is 100.\n    --max-path-length  arg      The maximum length for any discriminating path. -1 if unlimited. Default is -1.\n    --no-validation-output      No validation output files created.\n -o,--out  arg                  Output directory.\n    --output-prefix  arg        Prefix name for output files.\n    --penalty-discount  arg     Penalty discount. Default is 2.0.\n    --skip-latest               Skip checking for latest software version\n    --skip-nonzero-variance     Skip check for zero variance variables.\n    --skip-unique-var-name      Skip check for unique variable names.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread  arg               Number of threads.\n    --use-complete-rule-set\n    --verbose                   Print additional information.", 
            "title": "GFCIc"
        }, 
        {
            "location": "/causal-cmd/#gfcid", 
            "text": "usage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm GFCId [--alpha  arg ] [-d  arg ] [--exclude-variables  arg ] -f  arg  [--faithfulness-assumed] [--help] [--json] [--knowledge  arg ] [--max-degree  arg ] [--max-path-length  arg ] [--no-validation-output] [-o  arg ] [--output-prefix  arg ] [--sample-prior  arg ] [--skip-category-limit] [--skip-latest] [--skip-unique-var-name] [--structure-prior  arg ] [--tetrad-graph-json] [--thread  arg ] [--use-complete-rule-set] [--verbose]\n    --alpha  arg                Cutoff for p values (alpha). Default is 0.01.\n -d,--delimiter  arg            Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables  arg    A file containing variables to exclude.\n -f,--data  arg                 Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge  arg            A file containing prior knowledge.\n    --max-degree  arg           The maximum degree of the graph.. Default is 100.\n    --max-path-length  arg      The maximum length for any discriminating path. -1 if unlimited. Default is -1.\n    --no-validation-output      No validation output files created.\n -o,--out  arg                  Output directory.\n    --output-prefix  arg        Prefix name for output files.\n    --sample-prior  arg         Sample prior. Default is 1.0.\n    --skip-category-limit       Skip 'limit number of categories' check.\n    --skip-latest               Skip checking for latest software version\n    --skip-unique-var-name      Skip check for unique variable names.\n    --structure-prior  arg      Structure prior coefficient. Default is 1.0.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread  arg               Number of threads.\n    --use-complete-rule-set\n    --verbose                   Print additional information.", 
            "title": "GFCId"
        }, 
        {
            "location": "/causal-cmd/#gfcim-cg", 
            "text": "usage: java -jar causal-cmd-x.x.x-SNAPSHOT.jar --algorithm GFCIm-cg [--alpha  arg ] [-d  arg ] [--discretize] [--exclude-variables  arg ] -f  arg  [--faithfulness-assumed] [--help] [--json] [--knowledge  arg ] [--max-degree  arg ] [--max-path-length  arg ] [--no-validation-output] [--num-categories-to-discretize  arg ] [--num-discrete-categories  arg ] [-o  arg ] [--output-prefix  arg ] [--penalty-discount  arg ] [--skip-latest] [--structure-prior  arg ] [--tetrad-graph-json] [--thread  arg ] [--use-complete-rule-set] [--verbose]\n    --alpha  arg                           Cutoff for p values (alpha). Default is 0.01.\n -d,--delimiter  arg                       Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --discretize                           Yes if continuous variables should be discretized when child is discrete. Default is true.\n    --exclude-variables  arg               A file containing variables to exclude.\n -f,--data  arg                            Data file.\n    --faithfulness-assumed                 Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                                 Show help.\n    --json                                 Create JSON output.\n    --knowledge  arg                       A file containing prior knowledge.\n    --max-degree  arg                      The maximum degree of the graph.. Default is 100.\n    --max-path-length  arg                 The maximum length for any discriminating path. -1 if unlimited. Default is -1.\n    --no-validation-output                 No validation output files created.\n    --num-categories-to-discretize  arg    The number of categories used to discretize continuous variables, if necessary. Default is 3.\n    --num-discrete-categories  arg         Number of category considered discrete variable.\n -o,--out  arg                             Output directory.\n    --output-prefix  arg                   Prefix name for output files.\n    --penalty-discount  arg                Penalty discount. Default is 2.0.\n    --skip-latest                          Skip checking for latest software version\n    --structure-prior  arg                 Structure prior coefficient. Default is 1.0.\n    --tetrad-graph-json                    Create Tetrad Graph JSON output.\n    --thread  arg                          Number of threads.\n    --use-complete-rule-set\n    --verbose                              Print additional information.", 
            "title": "GFCIm-cg"
        }, 
        {
            "location": "/causal-cmd/#sample-prior-knowledge-file", 
            "text": "From the above useage guide, we see the option of  --knowledge  arg , with which we can specify the prior knowledge file. Below is the content of a sample prior knowledge file:  /knowledge\n\naddtemporal\n1 spending_per_stdt fac_salary stdt_tchr_ratio \n2 rjct_rate stdt_accept_rate \n3 tst_scores stdt_clss_stndng \n4* grad_rate \n\nforbiddirect\nx3 x4\n\nrequiredirect\nx1 x2  The first line of the prior knowledge file must say  /knowledge . And a prior knowledge file consists of three sections:   addtemporal - tiers of variables where the first tier preceeds the last. Adding a asterisk next to the tier id prohibits edges between tier variables  forbiddirect - forbidden edges indicated by a list of pairs of variables  requireddirect - required edges indicated by a list of pairs of variables", 
            "title": "Sample Prior Knowledge File"
        }, 
        {
            "location": "/causal-cmd/#api-usage", 
            "text": "In addition to using causal-cmd directly in the command line interface, you can also use the Tetred library that is includedin in causal-cmd as an  Java API . Here we provide an example of how to run  FGESc  algorithm using this API.  import edu.cmu.tetrad.data.DataModel;\nimport edu.cmu.tetrad.data.DataSet;\nimport edu.cmu.tetrad.graph.Graph;\nimport edu.cmu.tetrad.algcomparison.score.SemBicScore;\nimport edu.cmu.tetrad.algcomparison.algorithm.oracle.pattern.Fges;\nimport edu.cmu.tetrad.util.Parameters;\nimport edu.pitt.dbmi.causal.cmd.ParamAttrs;\nimport edu.pitt.dbmi.causal.cmd.util.TetradDataUtils;\nimport edu.pitt.dbmi.causal.cmd.validation.TetradDataValidation;\nimport edu.pitt.dbmi.causal.cmd.validation.UniqueVariableValidation;\nimport edu.pitt.dbmi.data.Dataset;\nimport edu.pitt.dbmi.data.Delimiter;\nimport edu.pitt.dbmi.data.reader.tabular.ContinuousTabularDataFileReader;\nimport edu.pitt.dbmi.data.reader.tabular.TabularDataReader;\nimport edu.pitt.dbmi.data.validation.ValidationCode;\nimport edu.pitt.dbmi.data.validation.ValidationResult;\nimport edu.pitt.dbmi.data.validation.tabular.ContinuousTabularDataFileValidation;\nimport edu.pitt.dbmi.data.validation.tabular.TabularDataValidation;\n\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.List;\n\npublic class FGEScApiExample {\n\n    /**\n    * @param args the command line arguments\n    */\n    public static void main(String[] args) throws Exception {\n        // Set the data file and its properites\n        Path dataFile = Paths.get( test ,  data ,  cmu ,  Retention.txt );\n        Delimiter delimiter = Delimiter.TAB;\n        char quoteCharacter = ' ';\n        String missingValueMarker =  * ;\n        String commentMarker =  // ;\n\n        // Data file validation: ensure the data file is valid format\n        // This is optional if you don't need to validate the data file\n        TabularDataValidation dataFileValidation = new ContinuousTabularDataFileValidation(dataFile.toFile(), delimiter);\n        dataFileValidation.setQuoteCharacter(quoteCharacter);\n        dataFileValidation.setMissingValueMarker(missingValueMarker);\n        dataFileValidation.setCommentMarker(commentMarker);\n        dataFileValidation.validate();\n\n        // Ensure there is no error\n        int errorCount = 0;\n        List ValidationResult  fileValidResults = dataFileValidation.getValidationResults();\n        for (ValidationResult validation : fileValidResults) {\n            if (validation.getCode() == ValidationCode.ERROR) {\n                errorCount++;\n            }\n        }\n\n        // Read in data\n        // This is the actual data loading/reading section\n        TabularDataReader reader = new ContinuousTabularDataFileReader(dataFile.toFile(), delimiter);\n        reader.setQuoteCharacter(quoteCharacter);\n        reader.setMissingValueMarker(missingValueMarker);\n        reader.setCommentMarker(commentMarker);\n        Dataset dataset = reader.readInData();\n\n        // Convert to Tetrad data model\n        DataModel dataModel = TetradDataUtils.toDataModel(dataset);\n\n        // Data validation: ensure the data read in is valid\n        TetradDataValidation dataValidation = new UniqueVariableValidation((DataSet) dataModel);\n        boolean isValidData = dataValidation.validate(System.err, true);\n\n        // Set algorithm parameters\n        Parameters parameters = new Parameters();\n        parameters.set(ParamAttrs.PENALTY_DISCOUNT, 2.0);\n        parameters.set(ParamAttrs.MAX_DEGREE, -1);\n        parameters.set(ParamAttrs.FAITHFULNESS_ASSUMED, false);\n        parameters.set(ParamAttrs.VERBOSE, false);\n\n        // Specify which algorithm to use\n        Fges fges = new Fges(new SemBicScore());\n\n        // Run the algorithm on this data with specified parameters\n        // and return the Graph object\n        Graph graph = fges.search(dataModel, parameters);\n\n        System.out.println();\n        System.out.println(graph.toString().trim());\n        System.out.flush();\n    }\n\n}", 
            "title": "API Usage"
        }, 
        {
            "location": "/causal-web/", 
            "text": "Causal Web Application Quick Start and User Guide\n\n\nCausal web is a Java web-based application that allows users to run causal modeling algorithms on their dataset.  The Center for Causal Discovery is hosting this application at the Pittsburgh Supercomputing Center(PSC) and you can access it via this URL: \nhttps://ccd4.vm.bridges.psc.edu/ccd/\n.\n\n\nCreating Your Account\n\n\nYou can create a new account by clicking \"Create an account\" link on the login page.\n\n\n\n\nFill your information in the signup page. Make sure to read the \nTerms \n Conditions\n agreement and check the agree box before clicking the signup button.\n\n\n\n\nUpon finishing registration, the system will send out an email with an activation link. Go to your email accont and click on that link, then the Causal Web shows a confirmation message.\n\n\n\n\nLogin to Causal Web Application\n\n\nInput your email address and password that you want to use to register with the Causal Web system. Check the \"Remember Me\" checkbox if you would like the browser automatically log you in next time you visit.\n\n\n\n\nHere we go! You are now in the Causal Web application.\n\n\n\n\nUploading Your Dataset\n\n\nClick on the Data Management link on the navigation bar on the left side. There is a sub menu that will appear. Click on the Import Data link.\n\n\n\n\nYou can \nEITHER\n drag \n drop dataset file(s) into the dash-surrounded canvas \nOR\n \nyou can click on the \nBrowse\n button and choose the dataset file(s) you would like to upload to the Causal Web application. For testing purposes download this dataset: \nRetention.txt\n and upload it.\n\n\n\n\nThe \nImport Data\n panel shows the dataset upload progress as a percentage along with MD5 checksums (confirms that an uploaded file's contents are unchanged after upload) for each of uploaded files.\n\n\n\n\nYou can also pause the upload of files and resume later. In the case of a disrupted connection, you can resume the upload by repeating the previous steps. The Causal Web application will detect the unfinished upload and resume from the latest point of the last attempt.\n\n\nOnce all your dataset file(s) are all uploaded, the progress bar will show the \n(completed)\n sign.\n\n\nSummarizing Your Dataset\n\n\nBefore any analysis can proceed, the datasets need to be summarized. Specifically, you must indicate the delimiter used in the data file (tab vs. comma), and the types of variables found in the file. Once this is done, the Causal Web application will determine the number of columns (features) and rows (records) in the dataset. \n\n\nClick on the \nData Management\n menu on the navigation bar on the left side. The sub menu will slowly appear. Click on the \nDatasets\n menu.\n\n\n\n\nThe dataset page shows a list of datasets and their attributes. On the second \nSummarized\n column from the right, the yellow warning buttons indicate that the system has not yet summarized.\n\n\n\n\nClick on the dataset's name's link to see the dataset information. From this stage, the data summary information is missing: the dataset needs to be summarized before conducting causal analysis.\n\n\n\n\nFrom the dataset page, click on the yellow warning button to summarize a dataset. The data summarization page shows information of the dataset, its basic information, and additional information that will be determined after summarization (a number of rows and columns). The bottom panel has two radio boxes for you to choose variable type (continuous, discrete, or mixed), and delimiter (tab or comma). The Retention.txt dataset described above is tab-delimited and contains continous variables.\n\n\n\n\nOnce the dataset is summarized, the dataset page changes its sign to be a green button. Click to see the additional information of this summarized dataset.\n\n\n\n\nClick on the dataset's name's link to see the additional information.\n\n\n\n\nAnnotating Your Dataset\n\n\nOn the Datasets main page, the blue icon is for viewing and entering annotations.\n\n\n\n\nClick the annotation icon, and you can add new annotation, just click the \"New annotation\" button.\n\n\n\n\nThe application will pop up the annotation form.\n\n\n\n\nYou can also add another annotation on top of the exisiting annotation.\n\n\n\n\nUploading the Prior Knowledge\n\n\nClick on the \nData Management\n menu on the navigation bar on the left side. There is a sub menu that will appear. \nClick on the \nImport Data\n menu.\n\n\n\n\nYou can EITHER drag \n drop prior knowledge file(s) into the dash-surrounded canvas OR \nyou can click on the Browse button and choose the prior knowledge file(s) you would like to upload to the CCD Web application. Note that the prior knowledge file needs to have \n.prior\n file extension.\n\n\n\n\nExecuting an Analysis on Your Dataset\n\n\nClick on the Causal Discovery menu on the navigation bar on the left side. The sub menu will slowly appear. \nFGES\n and \nGFCI\n are the currently supported algorithms.\n\n\nFGES\n algorithm can handle Continuous, Discrete, and Mixed data files.\n\n\n\n\nGFCI\n algorithm can handle Continuous, Discrete, and Mixed data files as well.\n\n\n\n\nThe \nDataset\n drop-down box contains a list of datasets that you have uploaded. If those datasets are already uploaded and they are not displayed in the dataset drop-down box, it means that the \nData Summarization\n process to be reviewed in the first place prior to execute a causal \nFGES\n \n(Continuous)\n analysis. \n\n\nIf a prior knowledge file needs to be included in the analysis, Prior Knowledge File drop-down box contains a list of knowledge files. \nBefore clicking the \nNext\n button, the data validation parameters need to be input.\n\n\n\n\nHere, the \nFGES Continuous\n algorithm page allows user to modify its parameters. \n\n\n\n\nThe first one is Penalty Discount and its default value is 2. \n\n\nThe second one is Search Maximum Degree and its default value is 100. \n\n\nThe third one is Faithfulness Assumed and its default value is checked.\n\n\nThe fifth one is Verbose output and its default value is checked.\n\n\n\n\nClick \nNext\n to proceed or click Advanced Options (JVM) for the JVM customization.\n\n\n\n\nExpert Mode\n: the JVM parameters allow users to customize JVM parameters such how much maximum memory (in Gigabyte scale) the process would allocate (e.g. 4).\n\n\n\n\nThis is the summary page before the FGES job analysis is put into the queue. \nClick on the number 1 (Select Dataset) or number 2 (Set Parameters) to go back to modify the parameters. \nOnce, everything is set. Click the \nRun Algorithm!\n button.\n\n\n\n\nThe application will redirect to the \nJob Queue\n page. The analysis job is added to the queue. The \nQueued\n status means that it waits for the scheduler to run it once the executing slot is available. However, the \nJob Queue\n page does not currently automatically update the jobs' status (at least in this development stage). Refresh the \nJob Queue\n page from time to time to see the latest jobs' status.\n\n\n\n\nOnce the job slot is available, the queued job is then executed and its status changes to \nRunning\n.\n\n\nWhen the job is finished, it is automatically removed from the \nJob Queue\n page. The result of the analysis is added to the Results page.\n\n\n\n\nIn case the \nqueued\n or \nrunning\n job needs to be killed or removed, click the \nRemove\n button on the first column on the \nJob Queue\n page from the right. \nThe \nRemove Job\n confirmation page is popped up. Click \nYes\n to kill the job or \nNo\n to cancel the kill operation.\n\n\n\n\nAfter the job cancellation is confirmed, the job's status changes to \nKill Request\n. The scheduler will take care of removing of the job from the queue or killing a job in the server.\n\n\n\n\nIf the running job was killed or any error happened during the process, the error result will appear in the \nResults\n page. Its background is highlighted in red.\n\n\n\n\nIf there is an error, you will see the details of the error by clicking on error result link.\n\n\n\n\nReviewing Your Results\n\n\nClick on the \nResults\n menu on the navigation bar on the left side. \nClick on the \nAlgorithm Results\n menu.\n\n\n\n\nThe \nAlgorithm Results\n page shows a list of results, their creation time and their size. In the first column from the right, the green \nSave\n buttons provide the ability for users to download results to their local computers. \nClick on the result's name's link to see a causal graph of the result.\n\n\nCheck the result files on their checkboxes to \ncompare the results\n. \n\nNote\n: a number of comparing datasets can be more than two files.\n\n\n\n\nThe results page details the graph, the original dataset, and its parameters. \nClick on the \nView Full Screen\n button to see the causal graph in more detail.\n\n\n\n\nBased on the nature of your data, sometimes you may see the generated graph (PAG) containing dashed links in addition to solid links. For example:\n\n\n\n\n\n\n\n\nIf an edge is dashed that means there is no latent confounder.  Otherwise, there is possibly a latent confounder.\n\n\n\n\n\n\nIf an edge is green that means it is definitely direct.  Otherwise, it is possibly direct.\n\n\n\n\n\n\nComparing Your Results\n\n\nClick on the \nResults\n menu on the navigation bar on the left side. To compare two results click on the Algorithm Results item on the left. Select at least two results (place a checkmark next to the results) and click on Compare. Now click on the \nResult Comparisions\n item on the left.\n\n\n\n\nThe Result Comparisons page shows a list of results, their creation time and their size. On the first column from the right, the green Save buttons provide the ability for users to download results to their local computers. Click on the result's name's the link to see the detail of the result comparisons.\n\n\n\n\nThe \nResult Comparisons\n page shows the datasets compared, and the table of edges, their mutual appearance in all comparing datasets, and their mutual endpoint types.\n\n\n\n\nDownloading Your Result And Comparision Result\n\n\nOn the first column from the right of the \nAlgorithm Results\n page, the green \nSave\n buttons provide the ability for users to download results to their local computers.\n\n\n\n\nOn the first column from the right of the \nResult Comparisions\n page, the green \nSave\n buttons provide the ability for users to download result comparisons to their local computers.\n\n\n\n\nSubmit Your Feedback\n\n\nClick the \nFeedback\n menu on the navigation menu bar on the left. \nThe \nFeedback\n page shows the email (optional), and the text area for the user feedback (required). \nOnce, the feedback is filled, click the \nSend Feedback\n button.\n\n\n\n\nThe green \nThank you for you feedback!\n banner shows that the feedback submitted successfully.", 
            "title": "Causal Web"
        }, 
        {
            "location": "/causal-web/#causal-web-application-quick-start-and-user-guide", 
            "text": "Causal web is a Java web-based application that allows users to run causal modeling algorithms on their dataset.  The Center for Causal Discovery is hosting this application at the Pittsburgh Supercomputing Center(PSC) and you can access it via this URL:  https://ccd4.vm.bridges.psc.edu/ccd/ .", 
            "title": "Causal Web Application Quick Start and User Guide"
        }, 
        {
            "location": "/causal-web/#creating-your-account", 
            "text": "You can create a new account by clicking \"Create an account\" link on the login page.   Fill your information in the signup page. Make sure to read the  Terms   Conditions  agreement and check the agree box before clicking the signup button.   Upon finishing registration, the system will send out an email with an activation link. Go to your email accont and click on that link, then the Causal Web shows a confirmation message.", 
            "title": "Creating Your Account"
        }, 
        {
            "location": "/causal-web/#login-to-causal-web-application", 
            "text": "Input your email address and password that you want to use to register with the Causal Web system. Check the \"Remember Me\" checkbox if you would like the browser automatically log you in next time you visit.   Here we go! You are now in the Causal Web application.", 
            "title": "Login to Causal Web Application"
        }, 
        {
            "location": "/causal-web/#uploading-your-dataset", 
            "text": "Click on the Data Management link on the navigation bar on the left side. There is a sub menu that will appear. Click on the Import Data link.   You can  EITHER  drag   drop dataset file(s) into the dash-surrounded canvas  OR  \nyou can click on the  Browse  button and choose the dataset file(s) you would like to upload to the Causal Web application. For testing purposes download this dataset:  Retention.txt  and upload it.   The  Import Data  panel shows the dataset upload progress as a percentage along with MD5 checksums (confirms that an uploaded file's contents are unchanged after upload) for each of uploaded files.   You can also pause the upload of files and resume later. In the case of a disrupted connection, you can resume the upload by repeating the previous steps. The Causal Web application will detect the unfinished upload and resume from the latest point of the last attempt.  Once all your dataset file(s) are all uploaded, the progress bar will show the  (completed)  sign.", 
            "title": "Uploading Your Dataset"
        }, 
        {
            "location": "/causal-web/#annotating-your-dataset", 
            "text": "On the Datasets main page, the blue icon is for viewing and entering annotations.   Click the annotation icon, and you can add new annotation, just click the \"New annotation\" button.   The application will pop up the annotation form.   You can also add another annotation on top of the exisiting annotation.", 
            "title": "Annotating Your Dataset"
        }, 
        {
            "location": "/causal-web/#uploading-the-prior-knowledge", 
            "text": "Click on the  Data Management  menu on the navigation bar on the left side. There is a sub menu that will appear. \nClick on the  Import Data  menu.   You can EITHER drag   drop prior knowledge file(s) into the dash-surrounded canvas OR \nyou can click on the Browse button and choose the prior knowledge file(s) you would like to upload to the CCD Web application. Note that the prior knowledge file needs to have  .prior  file extension.", 
            "title": "Uploading the Prior Knowledge"
        }, 
        {
            "location": "/causal-web/#executing-an-analysis-on-your-dataset", 
            "text": "Click on the Causal Discovery menu on the navigation bar on the left side. The sub menu will slowly appear.  FGES  and  GFCI  are the currently supported algorithms.  FGES  algorithm can handle Continuous, Discrete, and Mixed data files.   GFCI  algorithm can handle Continuous, Discrete, and Mixed data files as well.   The  Dataset  drop-down box contains a list of datasets that you have uploaded. If those datasets are already uploaded and they are not displayed in the dataset drop-down box, it means that the  Data Summarization  process to be reviewed in the first place prior to execute a causal  FGES   (Continuous)  analysis.   If a prior knowledge file needs to be included in the analysis, Prior Knowledge File drop-down box contains a list of knowledge files. \nBefore clicking the  Next  button, the data validation parameters need to be input.   Here, the  FGES Continuous  algorithm page allows user to modify its parameters.    The first one is Penalty Discount and its default value is 2.   The second one is Search Maximum Degree and its default value is 100.   The third one is Faithfulness Assumed and its default value is checked.  The fifth one is Verbose output and its default value is checked.   Click  Next  to proceed or click Advanced Options (JVM) for the JVM customization.   Expert Mode : the JVM parameters allow users to customize JVM parameters such how much maximum memory (in Gigabyte scale) the process would allocate (e.g. 4).   This is the summary page before the FGES job analysis is put into the queue. \nClick on the number 1 (Select Dataset) or number 2 (Set Parameters) to go back to modify the parameters. \nOnce, everything is set. Click the  Run Algorithm!  button.   The application will redirect to the  Job Queue  page. The analysis job is added to the queue. The  Queued  status means that it waits for the scheduler to run it once the executing slot is available. However, the  Job Queue  page does not currently automatically update the jobs' status (at least in this development stage). Refresh the  Job Queue  page from time to time to see the latest jobs' status.   Once the job slot is available, the queued job is then executed and its status changes to  Running .  When the job is finished, it is automatically removed from the  Job Queue  page. The result of the analysis is added to the Results page.   In case the  queued  or  running  job needs to be killed or removed, click the  Remove  button on the first column on the  Job Queue  page from the right. \nThe  Remove Job  confirmation page is popped up. Click  Yes  to kill the job or  No  to cancel the kill operation.   After the job cancellation is confirmed, the job's status changes to  Kill Request . The scheduler will take care of removing of the job from the queue or killing a job in the server.   If the running job was killed or any error happened during the process, the error result will appear in the  Results  page. Its background is highlighted in red.   If there is an error, you will see the details of the error by clicking on error result link.", 
            "title": "Executing an Analysis on Your Dataset"
        }, 
        {
            "location": "/causal-web/#reviewing-your-results", 
            "text": "Click on the  Results  menu on the navigation bar on the left side. \nClick on the  Algorithm Results  menu.   The  Algorithm Results  page shows a list of results, their creation time and their size. In the first column from the right, the green  Save  buttons provide the ability for users to download results to their local computers. \nClick on the result's name's link to see a causal graph of the result.  Check the result files on their checkboxes to  compare the results .  Note : a number of comparing datasets can be more than two files.   The results page details the graph, the original dataset, and its parameters. \nClick on the  View Full Screen  button to see the causal graph in more detail.   Based on the nature of your data, sometimes you may see the generated graph (PAG) containing dashed links in addition to solid links. For example:     If an edge is dashed that means there is no latent confounder.  Otherwise, there is possibly a latent confounder.    If an edge is green that means it is definitely direct.  Otherwise, it is possibly direct.", 
            "title": "Reviewing Your Results"
        }, 
        {
            "location": "/causal-web/#downloading-your-result-and-comparision-result", 
            "text": "On the first column from the right of the  Algorithm Results  page, the green  Save  buttons provide the ability for users to download results to their local computers.   On the first column from the right of the  Result Comparisions  page, the green  Save  buttons provide the ability for users to download result comparisons to their local computers.", 
            "title": "Downloading Your Result And Comparision Result"
        }, 
        {
            "location": "/causal-web/#submit-your-feedback", 
            "text": "Click the  Feedback  menu on the navigation menu bar on the left. \nThe  Feedback  page shows the email (optional), and the text area for the user feedback (required). \nOnce, the feedback is filled, click the  Send Feedback  button.   The green  Thank you for you feedback!  banner shows that the feedback submitted successfully.", 
            "title": "Submit Your Feedback"
        }, 
        {
            "location": "/causal-rest-api/", 
            "text": "Causal REST API v0.0.8\n\n\nThis RESTful API is designed for causal web. And it implements the \nJAX-RS\n specifications using Jersey.\n\n\nTable of Contents\n\n\n\n\nInstallation\n\n\nPrerequisites\n\n\nDependencies\n\n\nConfiguration\n\n\nStart the API Server\n\n\nAPI Usage and Examples\n\n\nGetting JSON Web Token(JWT)\n\n\n1. Data Management\n\n\nUpload small data file\n\n\nResumable data file upload\n\n\nList all dataset files of a user\n\n\nGet the detail information of a dataset file based on ID\n\n\nDelete physical dataset file and all records from database for a given file ID\n\n\nSummarize dataset file\n\n\nList all prior knowledge files of a given user\n\n\nGet the detail information of a prior knowledge file based on ID\n\n\nDelete physical prior knowledge file and all records from database for a given file ID\n\n\n\n\n\n\n2. Causal Discovery\n\n\nList all the available causal discovery algorithms\n\n\nAdd a new job to run the desired algorithm on a given data file\n\n\nList all running jobs\n\n\nCheck the job status for a given job ID\n\n\nCancel a running job\n\n\n\n\n\n\n3. Result Management\n\n\nList all result files generated by the algorithm\n\n\nDownload a specific result file generated by the algorithm based on file name\n\n\nCompare algorithm result files\n\n\nList all the comparison files\n\n\nDownload a specific comparison file based on file name\n\n\n\n\n\n\n\n\nInstallation\n\n\nThe following installation instructions are supposed to be used by the server admin who deploys this API server. API users can skip this section and just start reading from the \nAPI Usage and Examples\n section. \n\n\nPrerequisites\n\n\nYou must have the following installed to build/install Causal REST API:\n\n\n\n\nOracle Java SE Development Kit 8\n\n\nMaven 3.x\n\n\n\n\nDependencies\n\n\nIf you want to run this API server and expose the API to your users, you'll first need to have the \nCausal Web Application\n installed and running. Your API users will use this web app to create their user accounts before they can consume the API. \n\n\nNote: currently new users can also be created using Auth0 login option, but the API doesn't work for these users.\n\n\nIn order to build the API server, you'll need the released version of \nccd-commons-0.3.1\n by going to the repo and checkout this specific release version:\n\n\ngit clone https://github.com/bd2kccd/ccd-commons.git\ncd ccd-commons\ngit checkout tags/v0.3.1\nmvn clean install\n\n\n\n\nYou'll also need to download released \nccd-db-0.6.3\n:\n\n\ngit clone https://github.com/bd2kccd/ccd-db.git\ncd ccd-db\ngit checkout tags/v0.6.3\nmvn clean install\n\n\n\n\nThen you can go get and install \ncausal-rest-api\n:\n\n\ngit clone https://github.com/bd2kccd/causal-rest-api.git\ncd causal-rest-api\nmvn clean package\n\n\n\n\nConfiguration\n\n\nThere are 4 configuration files to configure located at \ncausal-rest-api/src/main/resources\n:\n- \napplication-hsqldb.properties\n: HSQLDB database configurations (for testing only).\n- \napplication-mysql.properties\n: MySQL database configurations\n- \napplication-slurm.properties\n: Slurm setting for HPC\n- \napplication.properties\n: Spring Boot application settings\n- \ncausal.properties\n: Data file directory path and folder settings\n\n\nBefor editing the \ncausal.properties\n file, you need to create a workspace for the application to work in. Create a directory called workspace, for an example \n/home/zhy19/ccd/workspace\n. Inside the workspace directory, create another folder called \nlib\n. Then build the jar file of Tetred using the \nlatest development branch\n. After that, copy the jar file to the \nlib\n folder created earlier.\n\n\nStart the API Server\n\n\nOnce you have all the settings configured, go to \ncausal-rest-api/target\n and you will find the jar file named \ncausal-rest-api.jar\n. Then simply run \n\n\njava -jar causal-rest-api.jar\n\n\n\n\nAPI Usage and Examples\n\n\nIn the following sections, we'll demonstrate the API usage with examples using the API server that is running on Pittsburgh Super Computing. The API base URI is https://ccd4.vm.bridges.psc.edu/ccd-api.\n\n\nThis API requires user to be authenticated. Before using this API, the user will need to go to \nCausal-Web App\n and create an account. \n\n\nGetting JSON Web Token(JWT)\n\n\nAfter registration in Causal Web App, the email and password can be used to authenticate against the Causal REST API to get the access token (we use JWT) via \nHTTP Basic Auth\n. \n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/jwt\n\n\n\n\nIn basic auth, the user provides the username and password, which the HTTP client concatenates (username + \":\" + password), and base64 encodes it. This encoded string is then sent using a \nAuthorization\n header with the \"Basic\" schema. For instance user email \ndemo@pitt.edu\n whose password is \n123\n.\n\n\nPOST /ccd-api/jwt HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Basic ZGVtb0BwaXR0LmVkdToxMjM=\n\n\n\n\nOnce the request is processed successfully, the user ID together with a JWT will be returned in the response for further API queries.\n\n\n{\n  \nuserId\n: 22,\n  \njwt\n: \neyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA0Mjg1OTcsImlhdCI6MTQ3NTg0NjgyODU5N30.FcE7aEpg0u2c-gUVugIjJkzjhlDu5qav_XHtgLu3c6E\n,\n  \nissuedTime\n: 1475846828597,\n  \nlifetime\n: 3600,\n  \nexpireTime\n: 1475850428597,\n  \nwallTime\n: [\n    1,\n    3,\n    6\n  ]\n}\n\n\n\n\nWe'll need to use this \nuserId\n in the URI path of all subsequent requests. And this \njwt\n expires in 3600 seconds(1 hour), so the API consumer will need to request for another JWT otherwise the API query to other API endpoints will be denied. And this JWT will need to be sent via the HTTP \nAuthorization\n header as well, but using the \nBearer\n schema.\n\n\nThe \nwallTime\n field is designed for users who want to specify the the maximum CPU time when Slurm handles the jobs on PSC. Normally, a job is expected to finish before the specified maximum walltime.  After the walltime reaches the maximum, the job terminates regardless whether the job processes are still running or not. In this example, you can pick 1 hour, 3 or 6 hours as the wallTime.\n\n\nNote: querying the JWT endpoint again before the current JWT expires will generate a new JWT, which makes the old JWT expired automatically. And this newly generated JWT will be valid in another hour unless there's another new JWT being queried.\n\n\nSince this API is developed with Jersey, which supports \nWADL\n. So you can view the generated WADL by going to \nhttps://ccd4.vm.bridges.psc.edu/ccd-api/application.wadl?detail=true\n and see all resource available in the application. Accessing to this endpoint doesn't require authentication.\n\n\nBasically, all the API usage examples are grouped into three categories: \n\n\n\n\nData Management\n\n\nCausal Discovery\n\n\nResult Management\n\n\n\n\nAnd all the following examples will be issued by user \n22\n whose password is \n123\n.\n\n\n1. Data Management\n\n\nUpload small data file\n\n\nAt this point, you can upload two types of data files: tabular dataset file(either tab delimited or comma delimited) and prior knowledge file.\n\n\nAPI Endpoint URI pattern:\n\n\nPOST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/dataset/upload\n\n\n\n\nThis is a multipart file upload via an HTML form, and the client is required to use \nname=\"file\"\n to name their file upload field in their form.\n\n\nGenerated HTTP request code example:\n\n\nPOST /ccd-api/22/dataset/upload HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW\n\n----WebKitFormBoundary7MA4YWxkTrZu0gW\nContent-Disposition: form-data; name=\nfile\n; filename=\n\nContent-Type: \n\n\n----WebKitFormBoundary7MA4YWxkTrZu0gW\n\n\n\n\nIf the Authorization header is not provided, the response will look like this:\n\n\n{\n  \ntimestamp\n: 1465414501443,\n  \nstatus\n: 401,\n  \nerror\n: \nUnauthorized\n,\n  \nmessage\n: \nUser credentials are required.\n,\n  \npath\n: \n/22/dataset/upload\n\n}\n\n\n\n\nThis POST request will upload the dataset file to the target server location and add corresponding records into database. And the response will contain the following pieces:\n\n\n{\n    \nid\n: 6,\n    \nname\n: \nLung-tetrad_hv.txt\n,\n    \ncreationTime\n: 1466622267000,\n    \nlastModifiedTime\n: 1466622267000,\n    \nfileSize\n: 3309465,\n    \nmd5checkSum\n: \nb1db7511ee293d297e3055d9a7b46c5e\n,\n    \nfileSummary\n: {\n      \nvariableType\n: null,\n      \nfileDelimiter\n: null,\n      \nnumOfRows\n: null,\n      \nnumOfColumns\n: null\n    }\n  }\n\n\n\n\nThe prior knowledge file upload uses a similar API endpoint:\n\n\nPOST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge/upload\n\n\n\n\nDue to there's no need to summarize a prior knowledge file, the response of a successful prior knowledge file upload will look like:\n\n\n{\n    \nid\n: 6,\n    \nname\n: \nLung-tetrad_hv.txt\n,\n    \ncreationTime\n: 1466622267000,\n    \nlastModifiedTime\n: 1466622267000,\n    \nfileSize\n: 3309465,\n    \nmd5checkSum\n: \nugdb7511rt293d29ke3055d9a7b46c9k\n\n  }\n\n\n\n\nResumable data file upload\n\n\nIn addition to the regular file upload described in Example 6, we also provide the option of stable and resumable large file upload. It requires the client side to have a resumable upload implementation. We currently support client integrated with \nResumable.js\n, whihc provides multiple simultaneous, stable \nand resumable uploads via the HTML5 File API. You can also create your own client as long as al the following parameters are set correctly.\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/chunkupload\n\nPOST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/chunkupload\n\n\n\n\nIn this example, the data file is splited into 3 chunks. The upload of each chunk consists of a GET request and a POST request. To handle the state of upload chunks, a number of extra parameters are sent along with all requests:\n\n\n\n\nresumableChunkNumber\n: The index of the chunk in the current upload. First chunk is \n1\n (no base-0 counting here).\n\n\nresumableChunkSize\n: The general chunk size. Using this value and \nresumableTotalSize\n you can calculate the total number of chunks. Please note that the size of the data received in the HTTP might be lower than \nresumableChunkSize\n of this for the last chunk for a file.\n\n\nresumableCurrentChunkSize\n: The size of the current resumable chuck.\n\n\nresumableTotalSize\n: The total file size.\n\n\nresumableType\n: The file type of the resumable chuck, e.e., \"text/plain\".\n\n\nresumableIdentifier\n: A unique identifier for the file contained in the request.\n\n\nresumableFilename\n: The original file name (since a bug in Firefox results in the file name not being transmitted in chunk multipart posts).\n\n\nresumableRelativePath\n: The file's relative path when selecting a directory (defaults to file name in all browsers except Chrome).\n\n\nresumableTotalChunks\n: The total number of chunks.  \n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/chunkupload?resumableChunkNumber=2\nresumableChunkSize=1048576\nresumableCurrentChunkSize=1048576\nresumableTotalSize=3309465\nresumableType=text%2Fplain\nresumableIdentifier=3309465-large-datatxt\nresumableFilename=large-data.txt\nresumableRelativePath=large-data.txt\nresumableTotalChunks=3 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nThis GET request checks if the data chunk is already on the server side. If the target file chunk is not found on the server, the client will issue a POST request to upload the actual data.\n\n\nGenerated HTTP request code example:\n\n\nPOST /ccd-api/22/chunkupload HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundaryMFjgApg56XGyeTnZ\n\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableChunkNumber\n\n\n2\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableChunkSize\n\n\n1048576\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableCurrentChunkSize\n\n\n1048576\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableTotalSize\n\n\n3309465\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableType\n\n\ntext/plain\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableIdentifier\n\n\n3309465-large-datatxt\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableFilename\n\n\nlarge-data.txt\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableRelativePath\n\n\nlarge-data.txt\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableTotalChunks\n\n\n3\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nfile\n; filename=\nblob\n\nContent-Type: application/octet-stream\n\n\n------WebKitFormBoundaryMFjgApg56XGyeTnZ--\n\n\n\n\nEach chunk upload POST will get a 200 status code from response if everything works fine.\n\n\nAnd finally the md5checkSum string of the reassemabled file will be returned once the whole file has been uploaded successfully. In this example, the POST request that uploads the third chunk will response this:\n\n\nb1db7511ee293d297e3055d9a7b46c5e\n\n\n\n\nList all dataset files of a user\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/dataset\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/dataset HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nAccept: application/json\n\n\n\n\nA \nJSON\n formatted list of all the input dataset files that are associated with user \n22\n will be returned.\n\n\n[\n  {\n    \nid\n: 8,\n    \nname\n: \ndata_small.txt\n,\n    \ncreationTime\n: 1467132449000,\n    \nlastModifiedTime\n: 1467132449000,\n    \nfileSize\n: 278428,\n    \nmd5checkSum\n: \ned5f27a2cf94fe3735a5d9ed9191c382\n,\n    \nfileSummary\n: {\n      \nvariableType\n: \ncontinuous\n,\n      \nfileDelimiter\n: \ntab\n,\n      \nnumOfRows\n: 302,\n      \nnumOfColumns\n: 123\n    }\n  },\n  {\n    \nid\n: 10,\n    \nname\n: \nlarge-data.txt\n,\n    \ncreationTime\n: 1467134048000,\n    \nlastModifiedTime\n: 1467134048000,\n    \nfileSize\n: 3309465,\n    \nmd5checkSum\n: \nb1db7511ee293d297e3055d9a7b46c5e\n,\n    \nfileSummary\n: {\n      \nvariableType\n: null,\n      \nfileDelimiter\n: null,\n      \nnumOfRows\n: null,\n      \nnumOfColumns\n: null\n    }\n  },\n  {\n    \nid\n: 11,\n    \nname\n: \nLung-tetrad_hv (copy).txt\n,\n    \ncreationTime\n: 1467140415000,\n    \nlastModifiedTime\n: 1467140415000,\n    \nfileSize\n: 3309465,\n    \nmd5checkSum\n: \nb1db7511ee293d297e3055d9a7b46c5e\n,\n    \nfileSummary\n: {\n      \nvariableType\n: \ncontinuous\n,\n      \nfileDelimiter\n: \ntab\n,\n      \nnumOfRows\n: 302,\n      \nnumOfColumns\n: 608\n    }\n  }\n]\n\n\n\n\nYou can also specify the response format as XML in your request\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/dataset HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nAccept: application/xml\n\n\n\n\nAnd the response will look like this:\n\n\n?xml version=\n1.0\n encoding=\nUTF-8\n standalone=\nyes\n?\n\n\ndatasetFileDTOes\n\n    \ndatasetFile\n\n        \nid\n8\n/id\n\n        \nname\ndata_small.txt\n/name\n\n        \ncreationTime\n2016-06-28T12:47:29-04:00\n/creationTime\n\n        \nlastModifiedTime\n2016-06-28T12:47:29-04:00\n/lastModifiedTime\n\n        \nfileSize\n278428\n/fileSize\n\n        \nmd5checkSum\ned5f27a2cf94fe3735a5d9ed9191c382\n/md5checkSum\n\n        \nfileSummary\n\n            \nfileDelimiter\ntab\n/fileDelimiter\n\n            \nnumOfColumns\n123\n/numOfColumns\n\n            \nnumOfRows\n302\n/numOfRows\n\n            \nvariableType\ncontinuous\n/variableType\n\n        \n/fileSummary\n\n    \n/datasetFile\n\n    \ndatasetFile\n\n        \nid\n10\n/id\n\n        \nname\nlarge-data.txt\n/name\n\n        \ncreationTime\n2016-06-28T13:14:08-04:00\n/creationTime\n\n        \nlastModifiedTime\n2016-06-28T13:14:08-04:00\n/lastModifiedTime\n\n        \nfileSize\n3309465\n/fileSize\n\n        \nmd5checkSum\nb1db7511ee293d297e3055d9a7b46c5e\n/md5checkSum\n\n        \nfileSummary\n\n            \nvariableType xmlns:xsi=\nhttp://www.w3.org/2001/XMLSchema-instance\n xsi:nil=\ntrue\n/\n\n            \nfileDelimiter xmlns:xsi=\nhttp://www.w3.org/2001/XMLSchema-instance\n xsi:nil=\ntrue\n/\n\n            \nnumOfRows xmlns:xsi=\nhttp://www.w3.org/2001/XMLSchema-instance\n xsi:nil=\ntrue\n/\n\n            \nnumOfColumns xmlns:xsi=\nhttp://www.w3.org/2001/XMLSchema-instance\n xsi:nil=\ntrue\n/\n\n        \n/fileSummary\n\n    \n/datasetFile\n\n    \ndatasetFile\n\n        \nid\n11\n/id\n\n        \nname\nLung-tetrad_hv (copy).txt\n/name\n\n        \ncreationTime\n2016-06-28T15:00:15-04:00\n/creationTime\n\n        \nlastModifiedTime\n2016-06-28T15:00:15-04:00\n/lastModifiedTime\n\n        \nfileSize\n3309465\n/fileSize\n\n        \nmd5checkSum\nb1db7511ee293d297e3055d9a7b46c5e\n/md5checkSum\n\n        \nfileSummary\n\n            \nfileDelimiter\ntab\n/fileDelimiter\n\n            \nnumOfColumns\n608\n/numOfColumns\n\n            \nnumOfRows\n302\n/numOfRows\n\n            \nvariableType\ncontinuous\n/variableType\n\n        \n/fileSummary\n\n    \n/datasetFile\n\n\n/datasetFileDTOes\n\n\n\n\n\nForm the above output, we can also tell that data file with ID 10 doesn't have all the \nfileSummary\n field values set, we'll cover this in the dataset summarization section.\n\n\nGet the detail information of a dataset file based on ID\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/dataset/{id}\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/dataset/8 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nAnd the resulting response looks like this:\n\n\n{\n  \nid\n: 8,\n  \nname\n: \ndata_small.txt\n,\n  \ncreationTime\n: 1467132449000,\n  \nlastModifiedTime\n: 1467132449000,\n  \nfileSize\n: 278428,\n  \nfileSummary\n: {\n    \nmd5checkSum\n: \ned5f27a2cf94fe3735a5d9ed9191c382\n,\n    \nvariableType\n: \ncontinuous\n,\n    \nfileDelimiter\n: \ntab\n,\n    \nnumOfRows\n: 302,\n    \nnumOfColumns\n: 123\n  }\n}\n\n\n\n\nDelete physical dataset file and all records from database for a given file ID\n\n\nAPI Endpoint URI pattern:\n\n\nDELETE https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/dataset/{id}\n\n\n\n\nGenerated HTTP request code example:\n\n\nDELETE /ccd-api/22/dataset/8 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nAnd this will result a HTTP 204 No Content status in response on success, which means the server successfully processed the deletion request but there's no content to response.\n\n\nSummarize dataset file\n\n\nSo from the first example we can tell that file with ID 10 doesn't have \nvariableType\n, \nfileDelimiter\n, \nnumOfRows\n, and \nnumOfColumns\n specified under \nfileSummary\n. Among these attributes, variableType\nand\nfileDelimiter` are the ones that users will need to provide during this summarization process.\n\n\nBefore we can go ahead to run the desired algorithm with the newly uploaded data file, we'll need to summarize the data by specifing the variable type and file delimiter.\n\n\n\n\n\n\n\n\nRequired Fields\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nid\n\n\nThe data file ID\n\n\n\n\n\n\nvariableType\n\n\ndiscrete or continuous\n\n\n\n\n\n\nfileDelimiter\n\n\ntab or comma\n\n\n\n\n\n\n\n\nAPI Endpoint URI pattern:\n\n\nPOST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/dataset/summarize\n\n\n\n\nGenerated HTTP request code example:\n\n\nPOST /ccd-api/22/dataset/summarize HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n{\n    \nid\n: 1,\n    \nvariableType\n: \ncontinuous\n,\n    \nfileDelimiter\n: \ncomma\n\n}\n\n\n\n\nThis POST request will summarize the dataset file and generate a response (JSON or XML) like below:\n\n\n{\n  \nid\n: 10,\n  \nname\n: \nlarge-data.txt\n,\n  \ncreationTime\n: 1467134048000,\n  \nlastModifiedTime\n: 1467134048000,\n  \nfileSize\n: 3309465,\n  \nmd5checkSum\n: \nb1db7511ee293d297e3055d9a7b46c5e\n,\n  \nfileSummary\n: {\n    \nvariableType\n: \ncontinuous\n,\n    \nfileDelimiter\n: \ntab\n,\n    \nnumOfRows\n: 302,\n    \nnumOfColumns\n: 608\n  }\n}\n\n\n\n\nList all prior knowledge files of a given user\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/priorknowledge HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nAccept: application/json\n\n\n\n\nA \nJSON\n formatted list of all the input dataset files that are associated with user \n22\n will be returned.\n\n\n[\n  {\n    \nid\n: 9,\n    \nname\n: \ndata_small.prior\n,\n    \ncreationTime\n: 1467132449000,\n    \nlastModifiedTime\n: 1467132449000,\n    \nfileSize\n: 278428,\n    \nmd5checkSum\n: \ned5f27a2cf94fe3735a5d9ed9191c382\n\n  },\n  {\n    \nid\n: 12,\n    \nname\n: \nlarge-data.prior\n,\n    \ncreationTime\n: 1467134048000,\n    \nlastModifiedTime\n: 1467134048000,\n    \nfileSize\n: 3309465,\n    \nmd5checkSum\n: \nb1db7511ee293d297e3055d9a7b46c5e\n\n  }\n]\n\n\n\n\nGet the detail information of a prior knowledge file based on ID\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge/{id}\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/priorknowledge/9 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nAnd the resulting response looks like this:\n\n\n{\n  \nid\n: 9,\n  \nname\n: \ndata_small.prior\n,\n  \ncreationTime\n: 1467132449000,\n  \nlastModifiedTime\n: 1467132449000,\n  \nfileSize\n: 278428,\n  \nmd5checkSum\n: \ned5f27a2cf94fe3735a5d9ed9191c382\n\n}\n\n\n\n\nDelete physical prior knowledge file and all records from database for a given file ID\n\n\nAPI Endpoint URI pattern:\n\n\nDELETE https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge/{id}\n\n\n\n\nGenerated HTTP request code example:\n\n\nDELETE /ccd-api/22/priorknowledge/9 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nAnd this will result a HTTP 204 No Content status in response on success, which means the server successfully processed the deletion request but there's no content to response.\n\n\n2. Causal Discovery\n\n\nOnce the data file is uploaded and summaried, you can start running a Causal Discovery Algorithm on the uploaded data file.\n\n\nList all the available causal discovery algorithms\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/algorithms\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/algorithms HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\n[\n  {\n    \nid\n: 1,\n    \nname\n: \nFGESc\n,\n    \ndescription\n: \nFGES continuous\n\n  },\n  {\n    \nid\n: 2,\n    \nname\n: \nFGESd\n,\n    \ndescription\n: \nFGES discrete\n\n  },\n  {\n    \nid\n: 3,\n    \nname\n: \nGFCIc\n,\n    \ndescription\n: \nGFCI continuous\n\n  },\n  {\n    \nid\n: 4,\n    \nname\n: \nGFCId\n,\n    \ndescription\n: \nGFCI discrete\n\n  }\n]\n\n\n\n\nCurrently we support \"FGES continuous\", \"FGES discrete\", \"GFCI continuous\", and \"GFCI discrete\". They also share a common JSON structure as of their input, for example:\n\n\n\n\n\n\n\n\nInput JSON Fields\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndatasetFileId\n\n\nThe dataset file ID, integer\n\n\n\n\n\n\npriorKnowledgeFileId\n\n\nThe optional prior knowledge file ID, integer\n\n\n\n\n\n\ndataValidation\n\n\nAlgorithm specific input data validation flags, JSON object\n\n\n\n\n\n\nalgorithmParameters\n\n\nAlgorithm specific parameters, JSON object\n\n\n\n\n\n\njvmOptions\n\n\nAdvanced Options For Java Virtual Machine (JVM), JSON object. Currently only support \nmaxHeapSize\n (Gigabyte, max value is 100)\n\n\n\n\n\n\nhpcParameters\n\n\nParameters for High-Performance Computing, JSON array of key-value objects. Currently only support \nwallTime\n\n\n\n\n\n\n\n\nBelow are the data validation flags and parameters that you can use for each algorithm.\n\n\nFGES continuous\n \n\n\nData validation:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nskipNonzeroVariance\n\n\nSkip check for zero variance variables\n\n\nfalse\n\n\n\n\n\n\nskipUniqueVarName\n\n\nSkip check for unique variable names\n\n\nfalse\n\n\n\n\n\n\n\n\nAlgorithm parameters:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nfaithfulnessAssumed\n\n\nYes if (one edge) faithfulness should be assumed\n\n\ntrue\n\n\n\n\n\n\nmaxDegree\n\n\nThe maximum degree of the output graph\n\n\n100\n\n\n\n\n\n\npenaltyDiscount\n\n\nPenalty discount\n\n\n4.0\n\n\n\n\n\n\nverbose\n\n\nPrint additional information\n\n\ntrue\n\n\n\n\n\n\n\n\nFGES discrete\n \n\n\nData validation:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nskipUniqueVarName\n\n\nSkip check for unique variable names\n\n\nfalse\n\n\n\n\n\n\nskipCategoryLimit\n\n\nSkip 'limit number of categories' check\n\n\nfalse\n\n\n\n\n\n\n\n\nAlgorithm parameters:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nstructurePrior\n\n\nStructure prior coefficient\n\n\n1.0\n\n\n\n\n\n\nsamplePrior\n\n\nSample prior\n\n\n1.0\n\n\n\n\n\n\nmaxDegree\n\n\nThe maximum degree of the output graph\n\n\n100\n\n\n\n\n\n\nfaithfulnessAssumed\n\n\nYes if (one edge) faithfulness should be assumed\n\n\ntrue\n\n\n\n\n\n\nverbose\n\n\nPrint additional information\n\n\ntrue\n\n\n\n\n\n\n\n\nGFCI continuous\n \n\n\nData validation:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nskipNonzeroVariance\n\n\nSkip check for zero variance variables\n\n\nfalse\n\n\n\n\n\n\nskipUniqueVarName\n\n\nSkip check for unique variable names\n\n\nfalse\n\n\n\n\n\n\n\n\nAlgorithm parameters:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nalpha\n\n\nCutoff for p values (alpha)\n\n\n0.01\n\n\n\n\n\n\npenaltyDiscount\n\n\nPenalty discount\n\n\n4.0\n\n\n\n\n\n\nmaxDegree\n\n\nThe maximum degree of the output graph\n\n\n100\n\n\n\n\n\n\nfaithfulnessAssumed\n\n\nYes if (one edge) faithfulness should be assumed\n\n\nfalse\n\n\n\n\n\n\nverbose\n\n\nPrint additional information\n\n\ntrue\n\n\n\n\n\n\n\n\nGFCI discrete\n \n\n\nData validation:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nskipUniqueVarName\n\n\nSkip check for unique variable names\n\n\nfalse\n\n\n\n\n\n\nskipCategoryLimit\n\n\nSkip 'limit number of categories' check\n\n\nfalse\n\n\n\n\n\n\n\n\nAlgorithm parameters:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nalpha\n\n\nCutoff for p values (alpha)\n\n\n0.01\n\n\n\n\n\n\nstructurePrior\n\n\nStructure prior coefficient\n\n\n1.0\n\n\n\n\n\n\nsamplePrior\n\n\nSample prior\n\n\n1.0\n\n\n\n\n\n\nmaxDegree\n\n\nThe maximum degree of the output graph\n\n\n100\n\n\n\n\n\n\nfaithfulnessAssumed\n\n\nYes if (one edge) faithfulness should be assumed\n\n\nfalse\n\n\n\n\n\n\nverbose\n\n\nPrint additional information\n\n\ntrue\n\n\n\n\n\n\n\n\nAdd a new job to run the desired algorithm on a given data file\n\n\nThis is a POST request and the algorithm details and data file id will need to be specified in the POST body as a JSON when you make the request.\n\n\nAPI Endpoint URI pattern:\n\n\nPOST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/jobs/FGESc\n\n\n\n\nGenerated HTTP request code example:\n\n\nPOST /ccd-api/22/jobs/FGESc HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n{\n    \ndatasetFileId\n: 8,\n    \npriorKnowledgeFileId\n: 9,\n    \ndataValidation\n: {\n      \nskipNonzeroVariance\n: true,\n      \nskipUniqueVarName\n: true\n    },\n    \nalgorithmParameters\n: {\n      \npenaltyDiscount\n: 5.0,\n      \nmaxDegree\n: 100\n    },\n    \njvmOptions\n: {\n      \nmaxHeapSize\n: 100\n    },\n    \nhpcParameters\n: [\n       {\n        \nkey\n:\nwallTime\n,\n        \nvalue\n:1\n       }\n    ]\n}\n\n\n\n\nIn this example, we are running the \"FGES continuous\" algorithm on the file of ID 8. We also set the wallTime as 1 hour. And this call will return the job info with a 201 Created response status code.\n\n\n{\n  \nid\n: 5,\n  \nalgorithmName\n: \nFGESc\n,\n  \nstatus\n: 0,\n  \naddedTime\n: 1472742564355,\n  \nresultFileName\n: \nFGESc_data_small.txt_1472742564353.txt\n,\n  \nerrorResultFileName\n: \nerror_FGESc_data_small.txt_1472742564353.txt\n\n}\n\n\n\n\nFrom this response we can tell that the job ID is 5, and the result file name will be \nFGESc_data_small.txt_1472742564353.txt\n if everything goes well. If something is wrong an error result file with name \nerror_FGEsc_data_small.txt_1472742564353.txt\n will be created.\n\n\nWhen you need to run \"FGES discrete\", just send the request to a different endpont URI:\n\n\nAPI Endpoint URI pattern:\n\n\nPOST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/jobs/FGESd\n\n\n\n\nGenerated HTTP request code example:\n\n\nPOST /ccd-api/22/jobs/FGESd HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n{\n    \ndatasetFileId\n: 10,\n    \npriorKnowledgeFileId\n: 12,\n    \ndataValidation\n: {\n      \nskipUniqueVarName\n: true,\n      \nskipCategoryLimit\n: true\n    },\n    \nalgorithmParameters\n: {\n      \nstructurePrior\n: 1.0,\n      \nsamplePrior\n: 1.0,\n      \nmaxDegree\n: 102\n    },\n    \njvmOptions\n: {\n      \nmaxHeapSize\n: 100\n    }\n}\n\n\n\n\nList all running jobs\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/jobs\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/jobs/ HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n\n\n\n\nThen you'll see the information of all jobs that are currently running:\n\n\n[\n  {\n    \nid\n: 32,\n    \nalgorithmName\n: \nFGESc\n,\n    \naddedTime\n: 1468436085000\n  },\n  {\n    \nid\n: 33,\n    \nalgorithmName\n: \nFGESd\n,\n    \naddedTime\n: 1468436087000\n  }\n]\n\n\n\n\nCheck the job status for a given job ID\n\n\nOnce the new job is submitted, it takes time and resources to run the algorithm on the server. During the waiting, you can check the status of a given job ID:\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/jobs/{id}\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/jobs/32 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nThis will either return \"Pending\" or \"Completed\".\n\n\nCancel a running job\n\n\nSometimes you may want to cancel a submitted job.\n\n\nAPI Endpoint URI pattern:\n\n\nDELETE https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/jobs/{id}\n\n\n\n\nGenerated HTTP request code example:\n\n\nDELETE /ccd-api/22/jobs/8 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nThis call will response either \"Job 8 has been canceled\" or \"Unable to cancel job 8\". It's not guranteed that the system can always cencal a job successfully.\n\n\n3. Result Management\n\n\nList all result files generated by the algorithm\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/results\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/results HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nThe response to this request will look like this:\n\n\n[\n  {\n    \nname\n: \nFGESc_sim_data_20vars_100cases.csv_1466171729046.txt\n,\n    \ncreationTime\n: 1466171732000,\n    \nlastModifiedTime\n: 1466171732000,\n    \nfileSize\n: 1660\n  },\n  {\n    \nname\n: \nFGESc_data_small.txt_1466172140585.txt\n,\n    \ncreationTime\n: 1466172145000,\n    \nlastModifiedTime\n: 1466172145000,\n    \nfileSize\n: 39559\n  }\n]\n\n\n\n\nDownload a specific result file generated by the algorithm based on file name\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/results/{result_file_name}\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/results/FGESc_data_small.txt_1466172140585.txt HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nOn success, you will get the result file back as text file content. If there's a typo in file name of the that file doesn't exist, you'll get either a JSON or XML message based on the \naccept\n header in your request:\n\n\nThe response to this request will look like this:\n\n\n{\n  \ntimestamp\n: 1467210996233,\n  \nstatus\n: 404,\n  \nerror\n: \nNot Found\n,\n  \nmessage\n: \nResource not found.\n,\n  \npath\n: \n/22/results/FGESc_data_small.txt_146172140585.txt\n\n}\n\n\n\n\nCompare algorithm result files\n\n\nSince we can list all the algorithm result files, based on the results, we can also choose multiple files and run a comparison. \n\n\nAPI Endpoint URI pattern:\n\n\nPOST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/results/compare\n\n\n\n\nThe request body is a JSON that contains an array of result files to be compared.\n\n\nGenerated HTTP request code example:\n\n\nPOST /ccd-api/22/results/compare HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n{\n  \nresultFiles\n: [\n    \nFGESc_sim_data_20vars_100cases.csv_1466171729046.txt\n,\n    \nFGESc_data_small.txt_1467305104859.txt\n\n  ]\n}\n\n\n\n\nWhen you specify multiple file names, use the \n!!\n as a delimiter. This request will generate a result comparison file with the following content (shortened version):\n\n\nFGESc_sim_data_20vars_100cases.csv_1466171729046.txt  FGESc_data_small.txt_1467305104859.txt\nEdges In All  Same End Point\nNR4A2,FOS 0 0\nX5,X17  0 0\nMMP11,ASB5  0 0\nX12,X8  0 0\nhsa_miR_654_3p,hsa_miR_337_3p 0 0\nRND1,FGA  0 0\nHHLA2,UBXN10  0 0\nHS6ST2,RND1 0 0\nSCRG1,hsa_miR_377 0 0\nCDH3,diag 0 0\nSERPINI2,FGG  0 0\nhsa_miR_451,hsa_miR_136_  0 0\n\n\n\n\nFrom this comparison, you can see if the two algorithm graphs have common edges and endpoints.\n\n\nList all the comparison files\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/results/comparisons\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/results/comparisons HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nThe response will show a list of comparison files:\n\n\n[\n  {\n    \nname\n: \nresult_comparison_1467385923407.txt\n,\n    \ncreationTime\n: 1467385923000,\n    \nlastModifiedTime\n: 1467385923000,\n    \nfileSize\n: 7505\n  },\n  {\n    \nname\n: \nresult_comparison_1467387034358.txt\n,\n    \ncreationTime\n: 1467387034000,\n    \nlastModifiedTime\n: 1467387034000,\n    \nfileSize\n: 7505\n  },\n  {\n    \nname\n: \nresult_comparison_1467388042261.txt\n,\n    \ncreationTime\n: 1467388042000,\n    \nlastModifiedTime\n: 1467388042000,\n    \nfileSize\n: 7533\n  }\n]\n\n\n\n\nDownload a specific comparison file based on file name\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/results/comparisons/{comparison_file_name}\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/results/comparisons/result_comparison_1467388042261.txt HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nThen it returns the content of that comparison file (shorted version):\n\n\nFGESc_sim_data_20vars_100cases.csv_1466171729046.txt  FGESc_data_small.txt_1467305104859.txt\nEdges In All  Same End Point\nNR4A2,FOS 0 0\nX5,X17  0 0\nMMP11,ASB5  0 0\nX12,X8  0 0\nhsa_miR_654_3p,hsa_miR_337_3p 0 0\nRND1,FGA  0 0\nHHLA2,UBXN10  0 0\nHS6ST2,RND1 0 0\nSCRG1,hsa_miR_377 0 0\nCDH3,diag 0 0\nSERPINI2,FGG  0 0", 
            "title": "Causal REST API"
        }, 
        {
            "location": "/causal-rest-api/#causal-rest-api-v008", 
            "text": "This RESTful API is designed for causal web. And it implements the  JAX-RS  specifications using Jersey.  Table of Contents   Installation  Prerequisites  Dependencies  Configuration  Start the API Server  API Usage and Examples  Getting JSON Web Token(JWT)  1. Data Management  Upload small data file  Resumable data file upload  List all dataset files of a user  Get the detail information of a dataset file based on ID  Delete physical dataset file and all records from database for a given file ID  Summarize dataset file  List all prior knowledge files of a given user  Get the detail information of a prior knowledge file based on ID  Delete physical prior knowledge file and all records from database for a given file ID    2. Causal Discovery  List all the available causal discovery algorithms  Add a new job to run the desired algorithm on a given data file  List all running jobs  Check the job status for a given job ID  Cancel a running job    3. Result Management  List all result files generated by the algorithm  Download a specific result file generated by the algorithm based on file name  Compare algorithm result files  List all the comparison files  Download a specific comparison file based on file name", 
            "title": "Causal REST API v0.0.8"
        }, 
        {
            "location": "/causal-rest-api/#installation", 
            "text": "The following installation instructions are supposed to be used by the server admin who deploys this API server. API users can skip this section and just start reading from the  API Usage and Examples  section.", 
            "title": "Installation"
        }, 
        {
            "location": "/causal-rest-api/#prerequisites", 
            "text": "You must have the following installed to build/install Causal REST API:   Oracle Java SE Development Kit 8  Maven 3.x", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/causal-rest-api/#dependencies", 
            "text": "If you want to run this API server and expose the API to your users, you'll first need to have the  Causal Web Application  installed and running. Your API users will use this web app to create their user accounts before they can consume the API.   Note: currently new users can also be created using Auth0 login option, but the API doesn't work for these users.  In order to build the API server, you'll need the released version of  ccd-commons-0.3.1  by going to the repo and checkout this specific release version:  git clone https://github.com/bd2kccd/ccd-commons.git\ncd ccd-commons\ngit checkout tags/v0.3.1\nmvn clean install  You'll also need to download released  ccd-db-0.6.3 :  git clone https://github.com/bd2kccd/ccd-db.git\ncd ccd-db\ngit checkout tags/v0.6.3\nmvn clean install  Then you can go get and install  causal-rest-api :  git clone https://github.com/bd2kccd/causal-rest-api.git\ncd causal-rest-api\nmvn clean package", 
            "title": "Dependencies"
        }, 
        {
            "location": "/causal-rest-api/#configuration", 
            "text": "There are 4 configuration files to configure located at  causal-rest-api/src/main/resources :\n-  application-hsqldb.properties : HSQLDB database configurations (for testing only).\n-  application-mysql.properties : MySQL database configurations\n-  application-slurm.properties : Slurm setting for HPC\n-  application.properties : Spring Boot application settings\n-  causal.properties : Data file directory path and folder settings  Befor editing the  causal.properties  file, you need to create a workspace for the application to work in. Create a directory called workspace, for an example  /home/zhy19/ccd/workspace . Inside the workspace directory, create another folder called  lib . Then build the jar file of Tetred using the  latest development branch . After that, copy the jar file to the  lib  folder created earlier.", 
            "title": "Configuration"
        }, 
        {
            "location": "/causal-rest-api/#start-the-api-server", 
            "text": "Once you have all the settings configured, go to  causal-rest-api/target  and you will find the jar file named  causal-rest-api.jar . Then simply run   java -jar causal-rest-api.jar", 
            "title": "Start the API Server"
        }, 
        {
            "location": "/causal-rest-api/#api-usage-and-examples", 
            "text": "In the following sections, we'll demonstrate the API usage with examples using the API server that is running on Pittsburgh Super Computing. The API base URI is https://ccd4.vm.bridges.psc.edu/ccd-api.  This API requires user to be authenticated. Before using this API, the user will need to go to  Causal-Web App  and create an account.", 
            "title": "API Usage and Examples"
        }, 
        {
            "location": "/causal-rest-api/#getting-json-web-tokenjwt", 
            "text": "After registration in Causal Web App, the email and password can be used to authenticate against the Causal REST API to get the access token (we use JWT) via  HTTP Basic Auth .   API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/jwt  In basic auth, the user provides the username and password, which the HTTP client concatenates (username + \":\" + password), and base64 encodes it. This encoded string is then sent using a  Authorization  header with the \"Basic\" schema. For instance user email  demo@pitt.edu  whose password is  123 .  POST /ccd-api/jwt HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Basic ZGVtb0BwaXR0LmVkdToxMjM=  Once the request is processed successfully, the user ID together with a JWT will be returned in the response for further API queries.  {\n   userId : 22,\n   jwt :  eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA0Mjg1OTcsImlhdCI6MTQ3NTg0NjgyODU5N30.FcE7aEpg0u2c-gUVugIjJkzjhlDu5qav_XHtgLu3c6E ,\n   issuedTime : 1475846828597,\n   lifetime : 3600,\n   expireTime : 1475850428597,\n   wallTime : [\n    1,\n    3,\n    6\n  ]\n}  We'll need to use this  userId  in the URI path of all subsequent requests. And this  jwt  expires in 3600 seconds(1 hour), so the API consumer will need to request for another JWT otherwise the API query to other API endpoints will be denied. And this JWT will need to be sent via the HTTP  Authorization  header as well, but using the  Bearer  schema.  The  wallTime  field is designed for users who want to specify the the maximum CPU time when Slurm handles the jobs on PSC. Normally, a job is expected to finish before the specified maximum walltime.  After the walltime reaches the maximum, the job terminates regardless whether the job processes are still running or not. In this example, you can pick 1 hour, 3 or 6 hours as the wallTime.  Note: querying the JWT endpoint again before the current JWT expires will generate a new JWT, which makes the old JWT expired automatically. And this newly generated JWT will be valid in another hour unless there's another new JWT being queried.  Since this API is developed with Jersey, which supports  WADL . So you can view the generated WADL by going to  https://ccd4.vm.bridges.psc.edu/ccd-api/application.wadl?detail=true  and see all resource available in the application. Accessing to this endpoint doesn't require authentication.  Basically, all the API usage examples are grouped into three categories:    Data Management  Causal Discovery  Result Management   And all the following examples will be issued by user  22  whose password is  123 .", 
            "title": "Getting JSON Web Token(JWT)"
        }, 
        {
            "location": "/causal-rest-api/#1-data-management", 
            "text": "", 
            "title": "1. Data Management"
        }, 
        {
            "location": "/causal-rest-api/#upload-small-data-file", 
            "text": "At this point, you can upload two types of data files: tabular dataset file(either tab delimited or comma delimited) and prior knowledge file.  API Endpoint URI pattern:  POST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/dataset/upload  This is a multipart file upload via an HTML form, and the client is required to use  name=\"file\"  to name their file upload field in their form.  Generated HTTP request code example:  POST /ccd-api/22/dataset/upload HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW\n\n----WebKitFormBoundary7MA4YWxkTrZu0gW\nContent-Disposition: form-data; name= file ; filename= \nContent-Type: \n\n\n----WebKitFormBoundary7MA4YWxkTrZu0gW  If the Authorization header is not provided, the response will look like this:  {\n   timestamp : 1465414501443,\n   status : 401,\n   error :  Unauthorized ,\n   message :  User credentials are required. ,\n   path :  /22/dataset/upload \n}  This POST request will upload the dataset file to the target server location and add corresponding records into database. And the response will contain the following pieces:  {\n     id : 6,\n     name :  Lung-tetrad_hv.txt ,\n     creationTime : 1466622267000,\n     lastModifiedTime : 1466622267000,\n     fileSize : 3309465,\n     md5checkSum :  b1db7511ee293d297e3055d9a7b46c5e ,\n     fileSummary : {\n       variableType : null,\n       fileDelimiter : null,\n       numOfRows : null,\n       numOfColumns : null\n    }\n  }  The prior knowledge file upload uses a similar API endpoint:  POST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge/upload  Due to there's no need to summarize a prior knowledge file, the response of a successful prior knowledge file upload will look like:  {\n     id : 6,\n     name :  Lung-tetrad_hv.txt ,\n     creationTime : 1466622267000,\n     lastModifiedTime : 1466622267000,\n     fileSize : 3309465,\n     md5checkSum :  ugdb7511rt293d29ke3055d9a7b46c9k \n  }", 
            "title": "Upload small data file"
        }, 
        {
            "location": "/causal-rest-api/#resumable-data-file-upload", 
            "text": "In addition to the regular file upload described in Example 6, we also provide the option of stable and resumable large file upload. It requires the client side to have a resumable upload implementation. We currently support client integrated with  Resumable.js , whihc provides multiple simultaneous, stable \nand resumable uploads via the HTML5 File API. You can also create your own client as long as al the following parameters are set correctly.  API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/chunkupload\n\nPOST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/chunkupload  In this example, the data file is splited into 3 chunks. The upload of each chunk consists of a GET request and a POST request. To handle the state of upload chunks, a number of extra parameters are sent along with all requests:   resumableChunkNumber : The index of the chunk in the current upload. First chunk is  1  (no base-0 counting here).  resumableChunkSize : The general chunk size. Using this value and  resumableTotalSize  you can calculate the total number of chunks. Please note that the size of the data received in the HTTP might be lower than  resumableChunkSize  of this for the last chunk for a file.  resumableCurrentChunkSize : The size of the current resumable chuck.  resumableTotalSize : The total file size.  resumableType : The file type of the resumable chuck, e.e., \"text/plain\".  resumableIdentifier : A unique identifier for the file contained in the request.  resumableFilename : The original file name (since a bug in Firefox results in the file name not being transmitted in chunk multipart posts).  resumableRelativePath : The file's relative path when selecting a directory (defaults to file name in all browsers except Chrome).  resumableTotalChunks : The total number of chunks.     Generated HTTP request code example:  GET /ccd-api/22/chunkupload?resumableChunkNumber=2 resumableChunkSize=1048576 resumableCurrentChunkSize=1048576 resumableTotalSize=3309465 resumableType=text%2Fplain resumableIdentifier=3309465-large-datatxt resumableFilename=large-data.txt resumableRelativePath=large-data.txt resumableTotalChunks=3 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  This GET request checks if the data chunk is already on the server side. If the target file chunk is not found on the server, the client will issue a POST request to upload the actual data.  Generated HTTP request code example:  POST /ccd-api/22/chunkupload HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundaryMFjgApg56XGyeTnZ\n\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableChunkNumber \n\n2\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableChunkSize \n\n1048576\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableCurrentChunkSize \n\n1048576\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableTotalSize \n\n3309465\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableType \n\ntext/plain\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableIdentifier \n\n3309465-large-datatxt\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableFilename \n\nlarge-data.txt\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableRelativePath \n\nlarge-data.txt\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableTotalChunks \n\n3\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= file ; filename= blob \nContent-Type: application/octet-stream\n\n\n------WebKitFormBoundaryMFjgApg56XGyeTnZ--  Each chunk upload POST will get a 200 status code from response if everything works fine.  And finally the md5checkSum string of the reassemabled file will be returned once the whole file has been uploaded successfully. In this example, the POST request that uploads the third chunk will response this:  b1db7511ee293d297e3055d9a7b46c5e", 
            "title": "Resumable data file upload"
        }, 
        {
            "location": "/causal-rest-api/#list-all-dataset-files-of-a-user", 
            "text": "API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/dataset  Generated HTTP request code example:  GET /ccd-api/22/dataset HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nAccept: application/json  A  JSON  formatted list of all the input dataset files that are associated with user  22  will be returned.  [\n  {\n     id : 8,\n     name :  data_small.txt ,\n     creationTime : 1467132449000,\n     lastModifiedTime : 1467132449000,\n     fileSize : 278428,\n     md5checkSum :  ed5f27a2cf94fe3735a5d9ed9191c382 ,\n     fileSummary : {\n       variableType :  continuous ,\n       fileDelimiter :  tab ,\n       numOfRows : 302,\n       numOfColumns : 123\n    }\n  },\n  {\n     id : 10,\n     name :  large-data.txt ,\n     creationTime : 1467134048000,\n     lastModifiedTime : 1467134048000,\n     fileSize : 3309465,\n     md5checkSum :  b1db7511ee293d297e3055d9a7b46c5e ,\n     fileSummary : {\n       variableType : null,\n       fileDelimiter : null,\n       numOfRows : null,\n       numOfColumns : null\n    }\n  },\n  {\n     id : 11,\n     name :  Lung-tetrad_hv (copy).txt ,\n     creationTime : 1467140415000,\n     lastModifiedTime : 1467140415000,\n     fileSize : 3309465,\n     md5checkSum :  b1db7511ee293d297e3055d9a7b46c5e ,\n     fileSummary : {\n       variableType :  continuous ,\n       fileDelimiter :  tab ,\n       numOfRows : 302,\n       numOfColumns : 608\n    }\n  }\n]  You can also specify the response format as XML in your request  Generated HTTP request code example:  GET /ccd-api/22/dataset HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nAccept: application/xml  And the response will look like this:  ?xml version= 1.0  encoding= UTF-8  standalone= yes ?  datasetFileDTOes \n     datasetFile \n         id 8 /id \n         name data_small.txt /name \n         creationTime 2016-06-28T12:47:29-04:00 /creationTime \n         lastModifiedTime 2016-06-28T12:47:29-04:00 /lastModifiedTime \n         fileSize 278428 /fileSize \n         md5checkSum ed5f27a2cf94fe3735a5d9ed9191c382 /md5checkSum \n         fileSummary \n             fileDelimiter tab /fileDelimiter \n             numOfColumns 123 /numOfColumns \n             numOfRows 302 /numOfRows \n             variableType continuous /variableType \n         /fileSummary \n     /datasetFile \n     datasetFile \n         id 10 /id \n         name large-data.txt /name \n         creationTime 2016-06-28T13:14:08-04:00 /creationTime \n         lastModifiedTime 2016-06-28T13:14:08-04:00 /lastModifiedTime \n         fileSize 3309465 /fileSize \n         md5checkSum b1db7511ee293d297e3055d9a7b46c5e /md5checkSum \n         fileSummary \n             variableType xmlns:xsi= http://www.w3.org/2001/XMLSchema-instance  xsi:nil= true / \n             fileDelimiter xmlns:xsi= http://www.w3.org/2001/XMLSchema-instance  xsi:nil= true / \n             numOfRows xmlns:xsi= http://www.w3.org/2001/XMLSchema-instance  xsi:nil= true / \n             numOfColumns xmlns:xsi= http://www.w3.org/2001/XMLSchema-instance  xsi:nil= true / \n         /fileSummary \n     /datasetFile \n     datasetFile \n         id 11 /id \n         name Lung-tetrad_hv (copy).txt /name \n         creationTime 2016-06-28T15:00:15-04:00 /creationTime \n         lastModifiedTime 2016-06-28T15:00:15-04:00 /lastModifiedTime \n         fileSize 3309465 /fileSize \n         md5checkSum b1db7511ee293d297e3055d9a7b46c5e /md5checkSum \n         fileSummary \n             fileDelimiter tab /fileDelimiter \n             numOfColumns 608 /numOfColumns \n             numOfRows 302 /numOfRows \n             variableType continuous /variableType \n         /fileSummary \n     /datasetFile  /datasetFileDTOes   Form the above output, we can also tell that data file with ID 10 doesn't have all the  fileSummary  field values set, we'll cover this in the dataset summarization section.", 
            "title": "List all dataset files of a user"
        }, 
        {
            "location": "/causal-rest-api/#get-the-detail-information-of-a-dataset-file-based-on-id", 
            "text": "API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/dataset/{id}  Generated HTTP request code example:  GET /ccd-api/22/dataset/8 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  And the resulting response looks like this:  {\n   id : 8,\n   name :  data_small.txt ,\n   creationTime : 1467132449000,\n   lastModifiedTime : 1467132449000,\n   fileSize : 278428,\n   fileSummary : {\n     md5checkSum :  ed5f27a2cf94fe3735a5d9ed9191c382 ,\n     variableType :  continuous ,\n     fileDelimiter :  tab ,\n     numOfRows : 302,\n     numOfColumns : 123\n  }\n}", 
            "title": "Get the detail information of a dataset file based on ID"
        }, 
        {
            "location": "/causal-rest-api/#delete-physical-dataset-file-and-all-records-from-database-for-a-given-file-id", 
            "text": "API Endpoint URI pattern:  DELETE https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/dataset/{id}  Generated HTTP request code example:  DELETE /ccd-api/22/dataset/8 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  And this will result a HTTP 204 No Content status in response on success, which means the server successfully processed the deletion request but there's no content to response.", 
            "title": "Delete physical dataset file and all records from database for a given file ID"
        }, 
        {
            "location": "/causal-rest-api/#summarize-dataset-file", 
            "text": "So from the first example we can tell that file with ID 10 doesn't have  variableType ,  fileDelimiter ,  numOfRows , and  numOfColumns  specified under  fileSummary . Among these attributes, variableType and fileDelimiter` are the ones that users will need to provide during this summarization process.  Before we can go ahead to run the desired algorithm with the newly uploaded data file, we'll need to summarize the data by specifing the variable type and file delimiter.     Required Fields  Description      id  The data file ID    variableType  discrete or continuous    fileDelimiter  tab or comma     API Endpoint URI pattern:  POST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/dataset/summarize  Generated HTTP request code example:  POST /ccd-api/22/dataset/summarize HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n{\n     id : 1,\n     variableType :  continuous ,\n     fileDelimiter :  comma \n}  This POST request will summarize the dataset file and generate a response (JSON or XML) like below:  {\n   id : 10,\n   name :  large-data.txt ,\n   creationTime : 1467134048000,\n   lastModifiedTime : 1467134048000,\n   fileSize : 3309465,\n   md5checkSum :  b1db7511ee293d297e3055d9a7b46c5e ,\n   fileSummary : {\n     variableType :  continuous ,\n     fileDelimiter :  tab ,\n     numOfRows : 302,\n     numOfColumns : 608\n  }\n}", 
            "title": "Summarize dataset file"
        }, 
        {
            "location": "/causal-rest-api/#list-all-prior-knowledge-files-of-a-given-user", 
            "text": "API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge  Generated HTTP request code example:  GET /ccd-api/22/priorknowledge HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nAccept: application/json  A  JSON  formatted list of all the input dataset files that are associated with user  22  will be returned.  [\n  {\n     id : 9,\n     name :  data_small.prior ,\n     creationTime : 1467132449000,\n     lastModifiedTime : 1467132449000,\n     fileSize : 278428,\n     md5checkSum :  ed5f27a2cf94fe3735a5d9ed9191c382 \n  },\n  {\n     id : 12,\n     name :  large-data.prior ,\n     creationTime : 1467134048000,\n     lastModifiedTime : 1467134048000,\n     fileSize : 3309465,\n     md5checkSum :  b1db7511ee293d297e3055d9a7b46c5e \n  }\n]", 
            "title": "List all prior knowledge files of a given user"
        }, 
        {
            "location": "/causal-rest-api/#get-the-detail-information-of-a-prior-knowledge-file-based-on-id", 
            "text": "API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge/{id}  Generated HTTP request code example:  GET /ccd-api/22/priorknowledge/9 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  And the resulting response looks like this:  {\n   id : 9,\n   name :  data_small.prior ,\n   creationTime : 1467132449000,\n   lastModifiedTime : 1467132449000,\n   fileSize : 278428,\n   md5checkSum :  ed5f27a2cf94fe3735a5d9ed9191c382 \n}", 
            "title": "Get the detail information of a prior knowledge file based on ID"
        }, 
        {
            "location": "/causal-rest-api/#delete-physical-prior-knowledge-file-and-all-records-from-database-for-a-given-file-id", 
            "text": "API Endpoint URI pattern:  DELETE https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge/{id}  Generated HTTP request code example:  DELETE /ccd-api/22/priorknowledge/9 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  And this will result a HTTP 204 No Content status in response on success, which means the server successfully processed the deletion request but there's no content to response.", 
            "title": "Delete physical prior knowledge file and all records from database for a given file ID"
        }, 
        {
            "location": "/causal-rest-api/#2-causal-discovery", 
            "text": "Once the data file is uploaded and summaried, you can start running a Causal Discovery Algorithm on the uploaded data file.", 
            "title": "2. Causal Discovery"
        }, 
        {
            "location": "/causal-rest-api/#list-all-the-available-causal-discovery-algorithms", 
            "text": "API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/algorithms  Generated HTTP request code example:  GET /ccd-api/22/algorithms HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  [\n  {\n     id : 1,\n     name :  FGESc ,\n     description :  FGES continuous \n  },\n  {\n     id : 2,\n     name :  FGESd ,\n     description :  FGES discrete \n  },\n  {\n     id : 3,\n     name :  GFCIc ,\n     description :  GFCI continuous \n  },\n  {\n     id : 4,\n     name :  GFCId ,\n     description :  GFCI discrete \n  }\n]  Currently we support \"FGES continuous\", \"FGES discrete\", \"GFCI continuous\", and \"GFCI discrete\". They also share a common JSON structure as of their input, for example:     Input JSON Fields  Description      datasetFileId  The dataset file ID, integer    priorKnowledgeFileId  The optional prior knowledge file ID, integer    dataValidation  Algorithm specific input data validation flags, JSON object    algorithmParameters  Algorithm specific parameters, JSON object    jvmOptions  Advanced Options For Java Virtual Machine (JVM), JSON object. Currently only support  maxHeapSize  (Gigabyte, max value is 100)    hpcParameters  Parameters for High-Performance Computing, JSON array of key-value objects. Currently only support  wallTime     Below are the data validation flags and parameters that you can use for each algorithm.  FGES continuous    Data validation:     Parameters  Description  Default Value      skipNonzeroVariance  Skip check for zero variance variables  false    skipUniqueVarName  Skip check for unique variable names  false     Algorithm parameters:     Parameters  Description  Default Value      faithfulnessAssumed  Yes if (one edge) faithfulness should be assumed  true    maxDegree  The maximum degree of the output graph  100    penaltyDiscount  Penalty discount  4.0    verbose  Print additional information  true     FGES discrete    Data validation:     Parameters  Description  Default Value      skipUniqueVarName  Skip check for unique variable names  false    skipCategoryLimit  Skip 'limit number of categories' check  false     Algorithm parameters:     Parameters  Description  Default Value      structurePrior  Structure prior coefficient  1.0    samplePrior  Sample prior  1.0    maxDegree  The maximum degree of the output graph  100    faithfulnessAssumed  Yes if (one edge) faithfulness should be assumed  true    verbose  Print additional information  true     GFCI continuous    Data validation:     Parameters  Description  Default Value      skipNonzeroVariance  Skip check for zero variance variables  false    skipUniqueVarName  Skip check for unique variable names  false     Algorithm parameters:     Parameters  Description  Default Value      alpha  Cutoff for p values (alpha)  0.01    penaltyDiscount  Penalty discount  4.0    maxDegree  The maximum degree of the output graph  100    faithfulnessAssumed  Yes if (one edge) faithfulness should be assumed  false    verbose  Print additional information  true     GFCI discrete    Data validation:     Parameters  Description  Default Value      skipUniqueVarName  Skip check for unique variable names  false    skipCategoryLimit  Skip 'limit number of categories' check  false     Algorithm parameters:     Parameters  Description  Default Value      alpha  Cutoff for p values (alpha)  0.01    structurePrior  Structure prior coefficient  1.0    samplePrior  Sample prior  1.0    maxDegree  The maximum degree of the output graph  100    faithfulnessAssumed  Yes if (one edge) faithfulness should be assumed  false    verbose  Print additional information  true", 
            "title": "List all the available causal discovery algorithms"
        }, 
        {
            "location": "/causal-rest-api/#add-a-new-job-to-run-the-desired-algorithm-on-a-given-data-file", 
            "text": "This is a POST request and the algorithm details and data file id will need to be specified in the POST body as a JSON when you make the request.  API Endpoint URI pattern:  POST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/jobs/FGESc  Generated HTTP request code example:  POST /ccd-api/22/jobs/FGESc HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n{\n     datasetFileId : 8,\n     priorKnowledgeFileId : 9,\n     dataValidation : {\n       skipNonzeroVariance : true,\n       skipUniqueVarName : true\n    },\n     algorithmParameters : {\n       penaltyDiscount : 5.0,\n       maxDegree : 100\n    },\n     jvmOptions : {\n       maxHeapSize : 100\n    },\n     hpcParameters : [\n       {\n         key : wallTime ,\n         value :1\n       }\n    ]\n}  In this example, we are running the \"FGES continuous\" algorithm on the file of ID 8. We also set the wallTime as 1 hour. And this call will return the job info with a 201 Created response status code.  {\n   id : 5,\n   algorithmName :  FGESc ,\n   status : 0,\n   addedTime : 1472742564355,\n   resultFileName :  FGESc_data_small.txt_1472742564353.txt ,\n   errorResultFileName :  error_FGESc_data_small.txt_1472742564353.txt \n}  From this response we can tell that the job ID is 5, and the result file name will be  FGESc_data_small.txt_1472742564353.txt  if everything goes well. If something is wrong an error result file with name  error_FGEsc_data_small.txt_1472742564353.txt  will be created.  When you need to run \"FGES discrete\", just send the request to a different endpont URI:  API Endpoint URI pattern:  POST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/jobs/FGESd  Generated HTTP request code example:  POST /ccd-api/22/jobs/FGESd HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n{\n     datasetFileId : 10,\n     priorKnowledgeFileId : 12,\n     dataValidation : {\n       skipUniqueVarName : true,\n       skipCategoryLimit : true\n    },\n     algorithmParameters : {\n       structurePrior : 1.0,\n       samplePrior : 1.0,\n       maxDegree : 102\n    },\n     jvmOptions : {\n       maxHeapSize : 100\n    }\n}", 
            "title": "Add a new job to run the desired algorithm on a given data file"
        }, 
        {
            "location": "/causal-rest-api/#list-all-running-jobs", 
            "text": "API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/jobs  Generated HTTP request code example:  GET /ccd-api/22/jobs/ HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json  Then you'll see the information of all jobs that are currently running:  [\n  {\n     id : 32,\n     algorithmName :  FGESc ,\n     addedTime : 1468436085000\n  },\n  {\n     id : 33,\n     algorithmName :  FGESd ,\n     addedTime : 1468436087000\n  }\n]", 
            "title": "List all running jobs"
        }, 
        {
            "location": "/causal-rest-api/#check-the-job-status-for-a-given-job-id", 
            "text": "Once the new job is submitted, it takes time and resources to run the algorithm on the server. During the waiting, you can check the status of a given job ID:  API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/jobs/{id}  Generated HTTP request code example:  GET /ccd-api/22/jobs/32 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  This will either return \"Pending\" or \"Completed\".", 
            "title": "Check the job status for a given job ID"
        }, 
        {
            "location": "/causal-rest-api/#cancel-a-running-job", 
            "text": "Sometimes you may want to cancel a submitted job.  API Endpoint URI pattern:  DELETE https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/jobs/{id}  Generated HTTP request code example:  DELETE /ccd-api/22/jobs/8 HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  This call will response either \"Job 8 has been canceled\" or \"Unable to cancel job 8\". It's not guranteed that the system can always cencal a job successfully.", 
            "title": "Cancel a running job"
        }, 
        {
            "location": "/causal-rest-api/#3-result-management", 
            "text": "", 
            "title": "3. Result Management"
        }, 
        {
            "location": "/causal-rest-api/#list-all-result-files-generated-by-the-algorithm", 
            "text": "API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/results  Generated HTTP request code example:  GET /ccd-api/22/results HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  The response to this request will look like this:  [\n  {\n     name :  FGESc_sim_data_20vars_100cases.csv_1466171729046.txt ,\n     creationTime : 1466171732000,\n     lastModifiedTime : 1466171732000,\n     fileSize : 1660\n  },\n  {\n     name :  FGESc_data_small.txt_1466172140585.txt ,\n     creationTime : 1466172145000,\n     lastModifiedTime : 1466172145000,\n     fileSize : 39559\n  }\n]", 
            "title": "List all result files generated by the algorithm"
        }, 
        {
            "location": "/causal-rest-api/#download-a-specific-result-file-generated-by-the-algorithm-based-on-file-name", 
            "text": "API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/results/{result_file_name}  Generated HTTP request code example:  GET /ccd-api/22/results/FGESc_data_small.txt_1466172140585.txt HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  On success, you will get the result file back as text file content. If there's a typo in file name of the that file doesn't exist, you'll get either a JSON or XML message based on the  accept  header in your request:  The response to this request will look like this:  {\n   timestamp : 1467210996233,\n   status : 404,\n   error :  Not Found ,\n   message :  Resource not found. ,\n   path :  /22/results/FGESc_data_small.txt_146172140585.txt \n}", 
            "title": "Download a specific result file generated by the algorithm based on file name"
        }, 
        {
            "location": "/causal-rest-api/#compare-algorithm-result-files", 
            "text": "Since we can list all the algorithm result files, based on the results, we can also choose multiple files and run a comparison.   API Endpoint URI pattern:  POST https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/results/compare  The request body is a JSON that contains an array of result files to be compared.  Generated HTTP request code example:  POST /ccd-api/22/results/compare HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n{\n   resultFiles : [\n     FGESc_sim_data_20vars_100cases.csv_1466171729046.txt ,\n     FGESc_data_small.txt_1467305104859.txt \n  ]\n}  When you specify multiple file names, use the  !!  as a delimiter. This request will generate a result comparison file with the following content (shortened version):  FGESc_sim_data_20vars_100cases.csv_1466171729046.txt  FGESc_data_small.txt_1467305104859.txt\nEdges In All  Same End Point\nNR4A2,FOS 0 0\nX5,X17  0 0\nMMP11,ASB5  0 0\nX12,X8  0 0\nhsa_miR_654_3p,hsa_miR_337_3p 0 0\nRND1,FGA  0 0\nHHLA2,UBXN10  0 0\nHS6ST2,RND1 0 0\nSCRG1,hsa_miR_377 0 0\nCDH3,diag 0 0\nSERPINI2,FGG  0 0\nhsa_miR_451,hsa_miR_136_  0 0  From this comparison, you can see if the two algorithm graphs have common edges and endpoints.", 
            "title": "Compare algorithm result files"
        }, 
        {
            "location": "/causal-rest-api/#list-all-the-comparison-files", 
            "text": "API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/results/comparisons  Generated HTTP request code example:  GET /ccd-api/22/results/comparisons HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  The response will show a list of comparison files:  [\n  {\n     name :  result_comparison_1467385923407.txt ,\n     creationTime : 1467385923000,\n     lastModifiedTime : 1467385923000,\n     fileSize : 7505\n  },\n  {\n     name :  result_comparison_1467387034358.txt ,\n     creationTime : 1467387034000,\n     lastModifiedTime : 1467387034000,\n     fileSize : 7505\n  },\n  {\n     name :  result_comparison_1467388042261.txt ,\n     creationTime : 1467388042000,\n     lastModifiedTime : 1467388042000,\n     fileSize : 7533\n  }\n]", 
            "title": "List all the comparison files"
        }, 
        {
            "location": "/causal-rest-api/#download-a-specific-comparison-file-based-on-file-name", 
            "text": "API Endpoint URI pattern:  GET https://ccd4.vm.bridges.psc.edu/ccd-api/{userId}/results/comparisons/{comparison_file_name}  Generated HTTP request code example:  GET /ccd-api/22/results/comparisons/result_comparison_1467388042261.txt HTTP/1.1\nHost: ccd4.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  Then it returns the content of that comparison file (shorted version):  FGESc_sim_data_20vars_100cases.csv_1466171729046.txt  FGESc_data_small.txt_1467305104859.txt\nEdges In All  Same End Point\nNR4A2,FOS 0 0\nX5,X17  0 0\nMMP11,ASB5  0 0\nX12,X8  0 0\nhsa_miR_654_3p,hsa_miR_337_3p 0 0\nRND1,FGA  0 0\nHHLA2,UBXN10  0 0\nHS6ST2,RND1 0 0\nSCRG1,hsa_miR_377 0 0\nCDH3,diag 0 0\nSERPINI2,FGG  0 0", 
            "title": "Download a specific comparison file based on file name"
        }, 
        {
            "location": "/py-causal/", 
            "text": "py-causal\n\n\nPython APIs\n for causal modeling algorithms developed by the University of Pittsburgh/Carnegie Mellon University \nCenter for Causal Discovery\n. \n\n\nThis code is distributed under the LGPL 2.1 license.\n\n\nRequirements:\n\n\nPython 2.7 (does not work with Python 3)\n\n\n\n\njavabridge\n=1.0.11\n\n\npandas\n\n\nnumpy \n\n\npydot\n\n\nGraphViz\n\n\nJDK 1.7+\n\n\n\n\nDocker Image\n\n\nA pre-installed py-causal Docker image is available at \nDocker Hub\n\n\nInstallation overview:\n\n\nTo install on existing Python installation, we have found two approaches to be useful:\n\n Direct python installation with pip, possibly including use of \nJupyter\n. This approach is likely best for users who have Python installed and are familiar with installing Python modules.\n\n Installation via \nAnaconda\n, which  installs Python and related utilities.\n\n\nDirections for both approaches are given below...\n\n\nInstallation with pip\n\n\nIf you do not have pip installed already, try \nthese instructions\n.\n\n\nOnce pip is installed, execute these commands\n\n\npip install numpy\npip install pandas\npip install javabridge\npip install pydot \npip install GraphViz\n\n\n\nNote: you also need to install the GraphViz engine by following \nthese instructions\n.\n\n\nWe have observed that on some OS X installations, pydot may provide the following response\n    Couldn't import dot_parser, loading of dot files will not be possible.\n\n\nIf you see this, try the following\n\n\n pip uninstall pydot\n pip install pyparsing==1.5.7\n pip install pydot\n\n\n\nThen, from within the py-causal directory, run the following command:\n\n\npython setup.py install\n\n\n\nAfter running this command, enter a python shell and attempt the follwing import\n    import pandas as pd\n    import pydot\n    from tetrad import search as s\n\n\nFinally, try to run the python \nexample\n\n\npython py-causal-fgs-continuous-example.py\n\n\n\nBe sure to run this from within the py-causal directory.\n\n\nThis program will create a file named tetrad.svg, which should be viewable in any SVG capable program. If you see a causal graph, everything is working correctly.\n\n\nRunning Jupyter/IPython\n\n\nWe have found \nJupyter\n notebooks to be helpful. (Those who have run IPython in the past should know that Jupyter is simply a new name for IPython). To add Jupyter to your completed python install, simply run\n\n\npip -U jupyter\njupyter notebook\n\n\n\nand then load one of the Jupyter notebooks found in this installation. \n\n\nAnaconda/Jupyter\n\n\nInstalling Python with Anaconda and Jupyter may be easier for some users:\n\n\n\n\nDownload and install Anaconda\n\n\nconda install python-javabridge\n\n\n\n\nFor OS X, this default install does not seem to work well. try the following instead:\n\n\nconda install --channel https://conda.anaconda.org/david_baddeley python-javabridge\n\n\n\nThen run the following to configure anacoda\n\n\nconda install pandas  \nconda install numpy\nconda install pydot\nconda install graphviz \nconda install -c https://conda.anaconda.org/chirayu pycausal \njupyter notebook\n\n\n\nand then load one of the Jupyter notebooks.", 
            "title": "Py-causal"
        }, 
        {
            "location": "/py-causal/#py-causal", 
            "text": "Python APIs  for causal modeling algorithms developed by the University of Pittsburgh/Carnegie Mellon University  Center for Causal Discovery .   This code is distributed under the LGPL 2.1 license.", 
            "title": "py-causal"
        }, 
        {
            "location": "/py-causal/#requirements", 
            "text": "Python 2.7 (does not work with Python 3)   javabridge =1.0.11  pandas  numpy   pydot  GraphViz  JDK 1.7+", 
            "title": "Requirements:"
        }, 
        {
            "location": "/py-causal/#docker-image", 
            "text": "A pre-installed py-causal Docker image is available at  Docker Hub", 
            "title": "Docker Image"
        }, 
        {
            "location": "/py-causal/#installation-overview", 
            "text": "To install on existing Python installation, we have found two approaches to be useful:  Direct python installation with pip, possibly including use of  Jupyter . This approach is likely best for users who have Python installed and are familiar with installing Python modules.  Installation via  Anaconda , which  installs Python and related utilities.  Directions for both approaches are given below...", 
            "title": "Installation overview:"
        }, 
        {
            "location": "/py-causal/#installation-with-pip", 
            "text": "If you do not have pip installed already, try  these instructions .  Once pip is installed, execute these commands  pip install numpy\npip install pandas\npip install javabridge\npip install pydot \npip install GraphViz  Note: you also need to install the GraphViz engine by following  these instructions .  We have observed that on some OS X installations, pydot may provide the following response\n    Couldn't import dot_parser, loading of dot files will not be possible.  If you see this, try the following   pip uninstall pydot\n pip install pyparsing==1.5.7\n pip install pydot  Then, from within the py-causal directory, run the following command:  python setup.py install  After running this command, enter a python shell and attempt the follwing import\n    import pandas as pd\n    import pydot\n    from tetrad import search as s  Finally, try to run the python  example  python py-causal-fgs-continuous-example.py  Be sure to run this from within the py-causal directory.  This program will create a file named tetrad.svg, which should be viewable in any SVG capable program. If you see a causal graph, everything is working correctly.", 
            "title": "Installation with pip"
        }, 
        {
            "location": "/py-causal/#running-jupyteripython", 
            "text": "We have found  Jupyter  notebooks to be helpful. (Those who have run IPython in the past should know that Jupyter is simply a new name for IPython). To add Jupyter to your completed python install, simply run  pip -U jupyter\njupyter notebook  and then load one of the Jupyter notebooks found in this installation.", 
            "title": "Running Jupyter/IPython"
        }, 
        {
            "location": "/py-causal/#anacondajupyter", 
            "text": "Installing Python with Anaconda and Jupyter may be easier for some users:   Download and install Anaconda  conda install python-javabridge   For OS X, this default install does not seem to work well. try the following instead:  conda install --channel https://conda.anaconda.org/david_baddeley python-javabridge  Then run the following to configure anacoda  conda install pandas  \nconda install numpy\nconda install pydot\nconda install graphviz \nconda install -c https://conda.anaconda.org/chirayu pycausal \njupyter notebook  and then load one of the Jupyter notebooks.", 
            "title": "Anaconda/Jupyter"
        }, 
        {
            "location": "/r-causal/", 
            "text": "r-causal\n\n\nR Wrapper\n for Tetrad Library\n\n\nR Library Requirement\n\n\nR \n= 3.2.0, \n\nstringr\n,\n\nrJava\n, \n\ngraph\n, \n\nRBGL\n, \n\nRgraphviz\n\n\nDocker\n\n\nAs an alternative to installing the library and getting rJava working with your installation (i.e., does not work well on mac) we have a \nDocker image\n\n\nInstallation\n\n\n\n\nInstall the R library requirements:\n\n\n\n\ninstall.packages(\nstringr\n)\ninstall.packages(\nrJava\n)\n## try http:// if https:// URLs are not supported\nsource(\nhttps://bioconductor.org/biocLite.R\n) \nbiocLite(\ngraph\n)\nbiocLite(\nRBGL\n)\nbiocLite(\nRgraphviz\n) # For plotting graph\n\n\n\n\n\n\nInstall r-causal from github:\n\n\n\n\nlibrary(devtools)\ninstall_github(\nbd2kccd/r-causal\n)\n\n\n\n\nExample\n\n\nContinuous Dataset\n\n\nlibrary(rcausal)\ndata(\ncharity\n)   #Load the charity dataset\n\n#Compute FGES search\nfgs \n- fgs(df = charity, penaltydiscount = 2, maxDegree = -1,  \nfaithfulnessAssumed = TRUE, numOfThreads = 2, verbose = TRUE)    \n\nfgs$parameters #Show the FGES's parameters\nfgs$datasets #Show the dataset\nfgs$nodes #Show the result's nodes\nfgs$edges #Show the result's edges\n\nlibrary(Rgraphviz)\nplot(fgs$graphNEL) #Plot the causal model\n\n\n\n\nDiscrete Dataset\n\n\nlibrary(rcausal)\ndata(\naudiology\n)    #Load the charity dataset\n#Compute FGES search\nfgs.discrete \n- fgs.discrete(df=audiology,structurePrior=1.0,samplePrior=1.0, \nmaxDegree = -1, faithfulnessAssumed = TRUE, numOfThreads = 2,verbose = TRUE)\nfgs.discrete$parameters #Show the FGES Discrete's parameters\nfgs.discrete$datasets #Show the dataset\nfgs.discrete$nodes #Show the result's nodes\nfgs.discrete$edges #Show the result's edges\nlibrary(Rgraphviz)\nplot(fgs.discrete$graphNEL) #Plot the causal model\n\n\n\n\nPrior Knowledge\n\n\nCreate PriorKnowledge Object\n\n\nforbid \n- list(c('TangibilityCondition','Impact')) # List of forbidden directed edges\nrequire \n- list(c('Sympathy','TangibilityCondition')) # List of required directed edges\nforbiddenWithin \n- c('TangibilityCondition','Imaginability')\nclass(forbiddenWithin) \n- 'forbiddenWithin' # Make this tier forbidden within\ntemporal \n- list(forbiddenWithin, c('Sympathy','AmountDonated'),c('Impact')) # List of temporal node tiers\nprior \n- priorKnowledge(forbiddirect = forbid, requiredirect = require, addtemporal = temporal)\nfgs \n- fgs(df = charity, penaltydiscount = 2, depth = -1, ignoreLinearDependence = TRUE, \nheuristicSpeedup = TRUE, numOfThreads = 2, verbose = TRUE, priorKnowledge = prior)\n\n\n\n\nLoad Knowledge File\n\n\n# knowledge file: audiology.prior\n# /knowledge\n# forbiddirect\n# class tymp\n# class age_gt_60\n# class notch_at_4k\n# \n# requiredirect\n# history_noise class\n#\n# addtemporal\n# 0* bser late_wave_poor tymp notch_at_4k o_ar_c ar_c airBoneGap air bone o_ar_u airBoneGap\n# 1 history_noise history_dizziness history_buzzing history_roaring history_recruitment history_fluctuating history_heredity history_nausea\n# 2 class\n\nprior \n- priorKnowledgeFromFile('audiology.prior')\nfgs.discrete \n- fgs.discrete(df=audiology,structurePrior=1.0,samplePrior=1.0, \ndepth = -1, heuristicSpeedup = TRUE, numOfThreads = 2,verbose = TRUE, priorKnowledge = prior)\n\n\n\n\nConvert Rgraphviz to igraph one\n\n\nlibrary(igraph)\nigraph \n- igraph.from.graphNEL(fgs.discrete$graphNEL)\nplot(igraph)\n\n\n\n\nUseful \nrJava\n Trouble-shooting Installation in Mac OS X Links\n\n\n\n\nhttp://stackoverflow.com/questions/26948777/how-can-i-make-rjava-use-the-newer-version-of-java-on-osx/32544358#32544358\n\n\nhttp://andrewgoldstone.com/blog/2015/02/03/rjava/", 
            "title": "R-causal"
        }, 
        {
            "location": "/r-causal/#r-causal", 
            "text": "R Wrapper  for Tetrad Library", 
            "title": "r-causal"
        }, 
        {
            "location": "/r-causal/#r-library-requirement", 
            "text": "R  = 3.2.0,  stringr , rJava ,  graph ,  RBGL ,  Rgraphviz", 
            "title": "R Library Requirement"
        }, 
        {
            "location": "/r-causal/#docker", 
            "text": "As an alternative to installing the library and getting rJava working with your installation (i.e., does not work well on mac) we have a  Docker image", 
            "title": "Docker"
        }, 
        {
            "location": "/r-causal/#installation", 
            "text": "Install the R library requirements:   install.packages( stringr )\ninstall.packages( rJava )\n## try http:// if https:// URLs are not supported\nsource( https://bioconductor.org/biocLite.R ) \nbiocLite( graph )\nbiocLite( RBGL )\nbiocLite( Rgraphviz ) # For plotting graph   Install r-causal from github:   library(devtools)\ninstall_github( bd2kccd/r-causal )", 
            "title": "Installation"
        }, 
        {
            "location": "/r-causal/#example", 
            "text": "", 
            "title": "Example"
        }, 
        {
            "location": "/r-causal/#continuous-dataset", 
            "text": "library(rcausal)\ndata( charity )   #Load the charity dataset\n\n#Compute FGES search\nfgs  - fgs(df = charity, penaltydiscount = 2, maxDegree = -1,  \nfaithfulnessAssumed = TRUE, numOfThreads = 2, verbose = TRUE)    \n\nfgs$parameters #Show the FGES's parameters\nfgs$datasets #Show the dataset\nfgs$nodes #Show the result's nodes\nfgs$edges #Show the result's edges\n\nlibrary(Rgraphviz)\nplot(fgs$graphNEL) #Plot the causal model", 
            "title": "Continuous Dataset"
        }, 
        {
            "location": "/r-causal/#discrete-dataset", 
            "text": "library(rcausal)\ndata( audiology )    #Load the charity dataset\n#Compute FGES search\nfgs.discrete  - fgs.discrete(df=audiology,structurePrior=1.0,samplePrior=1.0, \nmaxDegree = -1, faithfulnessAssumed = TRUE, numOfThreads = 2,verbose = TRUE)\nfgs.discrete$parameters #Show the FGES Discrete's parameters\nfgs.discrete$datasets #Show the dataset\nfgs.discrete$nodes #Show the result's nodes\nfgs.discrete$edges #Show the result's edges\nlibrary(Rgraphviz)\nplot(fgs.discrete$graphNEL) #Plot the causal model", 
            "title": "Discrete Dataset"
        }, 
        {
            "location": "/r-causal/#prior-knowledge", 
            "text": "", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/r-causal/#create-priorknowledge-object", 
            "text": "forbid  - list(c('TangibilityCondition','Impact')) # List of forbidden directed edges\nrequire  - list(c('Sympathy','TangibilityCondition')) # List of required directed edges\nforbiddenWithin  - c('TangibilityCondition','Imaginability')\nclass(forbiddenWithin)  - 'forbiddenWithin' # Make this tier forbidden within\ntemporal  - list(forbiddenWithin, c('Sympathy','AmountDonated'),c('Impact')) # List of temporal node tiers\nprior  - priorKnowledge(forbiddirect = forbid, requiredirect = require, addtemporal = temporal)\nfgs  - fgs(df = charity, penaltydiscount = 2, depth = -1, ignoreLinearDependence = TRUE, \nheuristicSpeedup = TRUE, numOfThreads = 2, verbose = TRUE, priorKnowledge = prior)", 
            "title": "Create PriorKnowledge Object"
        }, 
        {
            "location": "/r-causal/#load-knowledge-file", 
            "text": "# knowledge file: audiology.prior\n# /knowledge\n# forbiddirect\n# class tymp\n# class age_gt_60\n# class notch_at_4k\n# \n# requiredirect\n# history_noise class\n#\n# addtemporal\n# 0* bser late_wave_poor tymp notch_at_4k o_ar_c ar_c airBoneGap air bone o_ar_u airBoneGap\n# 1 history_noise history_dizziness history_buzzing history_roaring history_recruitment history_fluctuating history_heredity history_nausea\n# 2 class\n\nprior  - priorKnowledgeFromFile('audiology.prior')\nfgs.discrete  - fgs.discrete(df=audiology,structurePrior=1.0,samplePrior=1.0, \ndepth = -1, heuristicSpeedup = TRUE, numOfThreads = 2,verbose = TRUE, priorKnowledge = prior)", 
            "title": "Load Knowledge File"
        }, 
        {
            "location": "/r-causal/#convert-rgraphviz-to-igraph-one", 
            "text": "library(igraph)\nigraph  - igraph.from.graphNEL(fgs.discrete$graphNEL)\nplot(igraph)", 
            "title": "Convert Rgraphviz to igraph one"
        }, 
        {
            "location": "/r-causal/#useful-rjava-trouble-shooting-installation-in-mac-os-x-links", 
            "text": "http://stackoverflow.com/questions/26948777/how-can-i-make-rjava-use-the-newer-version-of-java-on-osx/32544358#32544358  http://andrewgoldstone.com/blog/2015/02/03/rjava/", 
            "title": "Useful rJava Trouble-shooting Installation in Mac OS X Links"
        }, 
        {
            "location": "/cytoscape-tetrad/", 
            "text": "cytoscape-tetrad\n\n\nInstallation\n\n\nDownload the latest version of the plugin from our software repository\n\n\nDownload the plugin\n\n\nTo install the plugin start the Cytoscape application and click on apps-\ninstall from file and select the jar file.\n\n\nUsing the Plugin\n\n\n\n\nWhen viewing a graph in Tetrad save the graph as Text.  Causal Command and Causal Web will also output this text file by default.\n\n\nIn Cytoscape select the File-\nimport-\nnetwork-\nTetrad option and select the file that you saved previously.  \n\n\nApply a layout: By default Cytoscape doesn't apply a layout so the initial rendering will look like a single node.  Apply a layout by selecting Layout in the top menu and then choosing a layout to see your graph (e.g., Layouts-\nyFiles Layouts-\nOrganic).", 
            "title": "Cytoscape Tetrad Plugin"
        }, 
        {
            "location": "/cytoscape-tetrad/#cytoscape-tetrad", 
            "text": "", 
            "title": "cytoscape-tetrad"
        }, 
        {
            "location": "/cytoscape-tetrad/#installation", 
            "text": "Download the latest version of the plugin from our software repository  Download the plugin  To install the plugin start the Cytoscape application and click on apps- install from file and select the jar file.", 
            "title": "Installation"
        }, 
        {
            "location": "/cytoscape-tetrad/#using-the-plugin", 
            "text": "When viewing a graph in Tetrad save the graph as Text.  Causal Command and Causal Web will also output this text file by default.  In Cytoscape select the File- import- network- Tetrad option and select the file that you saved previously.    Apply a layout: By default Cytoscape doesn't apply a layout so the initial rendering will look like a single node.  Apply a layout by selecting Layout in the top menu and then choosing a layout to see your graph (e.g., Layouts- yFiles Layouts- Organic).", 
            "title": "Using the Plugin"
        }
    ]
}