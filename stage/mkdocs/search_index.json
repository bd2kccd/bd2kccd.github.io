{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to CCD Docs\n\n\nThis site hosts documentation for the \nCenter for Causal Discovery\n.\n\n\nTools and Software\n\n\ncausal-cmd\n - a Java API and command line implementation of algorithms for performing causal discovery on Big Data. Use this software if you are interested incorporating analysis via a shell script or in a Java-based program. The software currently includes Fast Greedy Search (\nFGES\n) for continuous or discrete variables \u2013 an optimized version of Greedy Equivalence Search (\nGES\n) tested with datasets that contain as many as 1 million continuous variables, and Greedy Fast Causal Inference (\nGFCI\n) for continuous variables.\n\n\n\n\nDownload the latest release i.e., causal-cmd-X.X.X-jar-with-dependencies.jar\n\n\nDocumentation for the big data enabled causal discovery algorithms\n\n\nReport bugs or issues with the software\n\n\nCausal-cmd Github project\n\n\n\n\nTetrad\n - a Java API, and desktop environment for learning, performing analyses and experimenting with causal discovery algorithms.\n\n\n\n\nTetrad Project Website\n\n\nDownload latest release of tetrad i.e., tetrad-gui-X.X.X-launch.jar\n\n\nDownload command line version i.e., tetrad-lib-X.X.X-tetradcmd.jar\n\n\nDocumentation for the big data enabled causal discovery algorithms\n\n\nCausal-cmd Github project\n\n\n\n\nCausal Web App\n \u2013 our user-friendly web-based graphical interface for performing causal discovery analysis on big data using large memory servers at the Pittsburgh Supercomputing Center. Use this software if you want to quickly try out a causal discovery algorithm or if you have big data which cannot be analyzed on your local hardware.\n\n\n\n\nCCD Web Application\n\n\nReport bugs or issues with the software\n\n\nDocumentation for the big data enabled causal discovery algorithms\n\n-\nGithub project\n\n\n\n\nCausal REST API\n \u2013 our RESTful API for Causal Web App. Once you create a new user account via Causal Web App, you can use this REST API to upload data files and run Causal Discovery Algorithms.\n\n\n\n\nGithub project\n\n\n\n\nPy-causal\n - a python module that wraps algorithms for performing causal discovery on big data. The software currently includes Fast Greedy Search (\nFGES\n) for both continuous and discrete variables, and Greedy Fast Causal Inference (\nGFCI\n) for continuous variables.\n\n\n\n\nGithub project\n\n\nDocker container of Jupyter Notebook with Py-causal configured\n\n\nDocumentation for the big data enabled causal discovery algorithms\n\n\n\n\nR-causal\n - an R module that that wraps algorithms for performing causal discovery on big data. The software currently includes Fast Greedy Search (\nFGES\n) for both continuous and discrete variables, and Greedy Fast Causal Inference (\nGFCI\n) for continuous variables.\n\n\n\n\nGithub project\n\n\nDocker container of Jupyter Notebook with R-causal configured\n\n\nDocumentation for the big data enabled causal discovery algorithms\n\n\n\n\ncytoscape-tetrad\n - a native cytoscape plugin that imports tetrad txt output files that contain the structure of a causal graph. It handles causal graphs and partial ancestral graphs.\n\n\nIf you use our software in your research, please acknowledge the Center for Causal Discovery, supported by \ngrant U54HG008540\n, in any papers, presentations, or other dissemination of your work.\n\n\nAll software is open-source and released under a dual licensing model. For non-profit institutions, the software is available under the GNU General Public License (GPL) v2 license. For-profit organizations that wish to commercialize enhanced or customized versions of the software will be able to purchase a commercial license on a case-by-case basis. The GPL license permits individuals to modify the source code and to share modifications with other colleagues/investigators. Specifically, it permits the dissemination and commercialization of enhanced or customized versions as well as incorporation of the software or its pieces into other license-compatible software packages, as long as modifications or enhancements are made open source.\n\n\nBy using software provided by the Center for Causal Discovery, you agree that no warranties of any kind are made by Carnegie Mellon University or the University of Pittsburgh with respect to the data provided by the software or any use thereof, and the universities hereby disclaim the implied warranties of merchantability, fitness for a particular purpose, and non-infringement. The universities shall not be liable for any claims, losses, or damages of any kind arising from the data provided by the software or any use thereof.", 
            "title": "Welcome to CCD Docs"
        }, 
        {
            "location": "/#welcome-to-ccd-docs", 
            "text": "This site hosts documentation for the  Center for Causal Discovery .", 
            "title": "Welcome to CCD Docs"
        }, 
        {
            "location": "/#tools-and-software", 
            "text": "causal-cmd  - a Java API and command line implementation of algorithms for performing causal discovery on Big Data. Use this software if you are interested incorporating analysis via a shell script or in a Java-based program. The software currently includes Fast Greedy Search ( FGES ) for continuous or discrete variables \u2013 an optimized version of Greedy Equivalence Search ( GES ) tested with datasets that contain as many as 1 million continuous variables, and Greedy Fast Causal Inference ( GFCI ) for continuous variables.   Download the latest release i.e., causal-cmd-X.X.X-jar-with-dependencies.jar  Documentation for the big data enabled causal discovery algorithms  Report bugs or issues with the software  Causal-cmd Github project   Tetrad  - a Java API, and desktop environment for learning, performing analyses and experimenting with causal discovery algorithms.   Tetrad Project Website  Download latest release of tetrad i.e., tetrad-gui-X.X.X-launch.jar  Download command line version i.e., tetrad-lib-X.X.X-tetradcmd.jar  Documentation for the big data enabled causal discovery algorithms  Causal-cmd Github project   Causal Web App  \u2013 our user-friendly web-based graphical interface for performing causal discovery analysis on big data using large memory servers at the Pittsburgh Supercomputing Center. Use this software if you want to quickly try out a causal discovery algorithm or if you have big data which cannot be analyzed on your local hardware.   CCD Web Application  Report bugs or issues with the software  Documentation for the big data enabled causal discovery algorithms \n- Github project   Causal REST API  \u2013 our RESTful API for Causal Web App. Once you create a new user account via Causal Web App, you can use this REST API to upload data files and run Causal Discovery Algorithms.   Github project   Py-causal  - a python module that wraps algorithms for performing causal discovery on big data. The software currently includes Fast Greedy Search ( FGES ) for both continuous and discrete variables, and Greedy Fast Causal Inference ( GFCI ) for continuous variables.   Github project  Docker container of Jupyter Notebook with Py-causal configured  Documentation for the big data enabled causal discovery algorithms   R-causal  - an R module that that wraps algorithms for performing causal discovery on big data. The software currently includes Fast Greedy Search ( FGES ) for both continuous and discrete variables, and Greedy Fast Causal Inference ( GFCI ) for continuous variables.   Github project  Docker container of Jupyter Notebook with R-causal configured  Documentation for the big data enabled causal discovery algorithms   cytoscape-tetrad  - a native cytoscape plugin that imports tetrad txt output files that contain the structure of a causal graph. It handles causal graphs and partial ancestral graphs.  If you use our software in your research, please acknowledge the Center for Causal Discovery, supported by  grant U54HG008540 , in any papers, presentations, or other dissemination of your work.  All software is open-source and released under a dual licensing model. For non-profit institutions, the software is available under the GNU General Public License (GPL) v2 license. For-profit organizations that wish to commercialize enhanced or customized versions of the software will be able to purchase a commercial license on a case-by-case basis. The GPL license permits individuals to modify the source code and to share modifications with other colleagues/investigators. Specifically, it permits the dissemination and commercialization of enhanced or customized versions as well as incorporation of the software or its pieces into other license-compatible software packages, as long as modifications or enhancements are made open source.  By using software provided by the Center for Causal Discovery, you agree that no warranties of any kind are made by Carnegie Mellon University or the University of Pittsburgh with respect to the data provided by the software or any use thereof, and the universities hereby disclaim the implied warranties of merchantability, fitness for a particular purpose, and non-infringement. The universities shall not be liable for any claims, losses, or damages of any kind arising from the data provided by the software or any use thereof.", 
            "title": "Tools and Software"
        }, 
        {
            "location": "/causal-cmd/", 
            "text": "Documentation on using the causal-cmd software\n\n\nWhat is causal-cmd\n\n\nCausal-cmd is a Java application that provides a command-line interface (CLI) and application programming interface (API) for causal discovery algorithms produced by the Center for Causal Discovery.  The application currently includes the algorithm(s):\n\n\n\n\nFGESc (Fast Greedy Search) for continuous data - is an optimization of the Greedy Equivalence Search algorithm    (GES,   Meek    1995;   Chickering  2003). The optimizations are described in Scaling up Greedy Causal Search for Continuous Variables\n\n\nFGESd (Fast Greedy Search) for discrete data\n\n\nGFCIc (Greedy Fast Causal Inference) for continuous data\n\n\n\n\nNote that in previous versions released by the Center, FGES was called FGS.\n\n\nCausal discovery algorithms are a class of search algorithms that explore a space of graphical causal models, i.e., graphical models where directed edges imply causation, for a model (or models) that are a good fit for a dataset. We suggest that newcomers to the field review Causation, Prediction and Search by Spirtes, Glymour and Scheines for a primer on the subject.\n\n\nCausal discovery algorithms allow a user to uncover the causal relationships between variables in a dataset. These discovered causal relationships may be used further--understanding the underlying the processes of a system (e.g., the metabolic pathways of an organism), hypothesis generation (e.g., variables that best explain an outcome), guide experimentation (e.g., what gene knockout experiments should be performed) or prediction (e.g. parameterization of the causal graph using data and then using it as a classifier).\n\n\nHow can I use it?\n\n\nJava 7 or higher is the only prerequisite to run the software. Note that by default Java will allocate the smaller of 1/4 system memory or 1GB to the Java virtual machine (JVM). If you run out of memory (heap memory space) running your analyses you should increase the memory allocated to the JVM with the following switch '-XmxXXG' where XX is the number of gigabytes of ram you allow the JVM to utilize. For example to allocate 8 gigabytes of ram you would add -Xmx8G immediately after the java command.\n\n\nRun an example output using known data via command line\n\n\nDownload the this file, \nRetention.txt\n, which is a dataset containing information on college graduation and used in the publication \"What Do College Ranking Data Tell Us About Student Retention?\" by Drudzel and Glymour, 1994.\n\n\njava -jar causal-cmd-6.0.1-jar-with-dependencies.jar --algorithm FGESc --data Retention.txt\n\n\n\n\nNote that the filename causal-cmd-x.x.x-jar-with-dependencies.jar should match the version you have downloaded. The program will output the results of the FGES search procedure as a text file (in this example to output). The beginning of the file contains the algorithm parameters used in the search.\n\n\nInspect the output which should show a graph with the following edges.\n\n\nGraph Edges:\n1. fac_salary --- spending_per_stdt\n2. spending_per_stdt --\n rjct_rate\n3. spending_per_stdt --- stdt_tchr_ratio\n4. stdt_accept_rate --- fac_salary\n5. stdt_clss_stndng --\n rjct_rate\n6. tst_scores --- fac_salary\n7. tst_scores --- grad_rate\n8. tst_scores --- spending_per_stdt\n9. tst_scores --- stdt_clss_stndng\n\n\n\n\nIn FGES, \"Elapsed getEffectEdges = XXms\" refers to the amount of time it took to evaluate all pairs of variables for correlation. The file then details each step taken in the greedy search procedure i.e., insertion or deletion of edges based on a scoring function (i.e., BIC score difference for each chosen search operation).\n\n\nThe end of the file contains the causal graph from the search procedure. Here is a key to the edge types:\n\n\n\n\nA --- B - There is causal relationship between variable A and B but we cannot determine the direction of the relationship\n\n\nA --\n B - There is a causal relationship from variable A to B\n\n\n\n\nThe GFCI algorithm has additional edge types:\n\n\n\n\nA \n-\n B - There is an unmeasured confounder of A and B\n\n\nA o-\n B - Either A is a cause of B or there is an unmeasured confounder of A and B or both\n\n\nA o-o B - Either (1) A is a cause of B or B is a cause of A, or (2) there is an unmeasured confounder of A and B, or both 1 and 2 hold.\n\n\n\n\nUse as an API\n\n\nHere is an example of using the Tetrad library which is included in causal-cmd as an API. Javadocs for the API are \nhere\n.\n\n\nimport edu.cmu.tetrad.algcomparison.algorithm.Algorithm;\nimport edu.cmu.tetrad.algcomparison.algorithm.oracle.pattern.Fges;\nimport edu.cmu.tetrad.algcomparison.score.SemBicScore;\nimport edu.cmu.tetrad.cli.validation.DataValidation;\nimport edu.cmu.tetrad.cli.validation.TabularContinuousData;\nimport edu.cmu.tetrad.data.DataSet;\nimport edu.cmu.tetrad.graph.Graph;\nimport edu.cmu.tetrad.io.DataReader;\nimport edu.cmu.tetrad.io.TabularContinuousDataReader;\nimport edu.cmu.tetrad.util.Parameters;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\n\n/**\n *\n * Nov 29, 2016 4:42:42 PM\n *\n * @author Kevin V. Bui\n */\npublic class FgsApiExample {\n\n    /**\n     * @param args the command line arguments\n     */\n    public static void main(String[] args) throws Exception {\n        // set path to Retention data\n        Path dataFile = Paths.get(\ntest\n, \ndata\n, \nRetention.txt\n);\n\n        char delimiter = '\\t';\n\n        // perform data validation\n        // note: assuming data has unique variable names and does not contain zero covariance pairs\n        DataValidation dataValidation = new TabularContinuousData(dataFile, delimiter);\n        if (!dataValidation.validate(System.err, true)) {\n            System.exit(-128);\n        }\n\n        // create FGES algorithm that uses BIC score\n        Algorithm fges = new Fges(new SemBicScore());\n\n        // read in dataset\n        DataReader dataReader = new TabularContinuousDataReader(dataFile, delimiter);\n        DataSet dataSet = dataReader.readInData();\n\n        // set parameters for FGESc\n        Parameters parameters = new Parameters();\n        parameters.set(\npenaltyDiscount\n, 0.4);\n        parameters.set(\nmaxDegree\n, -1);\n        parameters.set(\nfaithfulnessAssumed\n, true);\n        parameters.set(\nverbose\n, true);\n        parameters.set(\nprintStream\n, System.out);\n\n        // run FGES search with given dataset and parameters\n        Graph graph = fges.search(dataSet, parameters);\n        System.out.println();\n        System.out.println(graph.toString().trim());\n        System.out.flush();\n    }\n\n}\n\n\n\n\nCommand line interface usage\n\n\nTetrad-cli has different switches for different algorithms. \n\n\nusage: java -jar causal-cmd-6.0.1-SNAPSHOT.jar --algorithm \narg\n |\n       --simulate-data \narg\n  [--version]\n    --algorithm \narg\n       FGESc, FGESd, GFCIc\n    --simulate-data \narg\n   sem-rand-fwd, bayes-net-rand-fwd\n    --version               Version.\n\n\n\n\nUse the \n--algorithm \narg\n parameter to see specific algorithm usage information.\n\n\ncausal-cmd usage for FGES for continuous data\n\n\nusage: java -jar causal-cmd-6.0.1-jar-with-dependencies.jar --algorithm FGESc [-d \narg\n] [--exclude-variables \narg\n] -f \narg\n [--faithfulness-assumed] [--help] [--json] [--knowledge \narg\n] [--max-degree \narg\n] [--no-validation-output] [-o \narg\n] [--output-prefix \narg\n] [--penalty-discount \narg\n] [--skip-latest] [--skip-nonzero-variance] [--skip-unique-var-name] [--tetrad-graph-json] [--thread \narg\n] [--verbose]\n -d,--delimiter \narg\n           Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables \narg\n   A file containing variables to exclude.\n -f,--data \narg\n                Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge \narg\n           A file containing prior knowledge.\n    --max-degree \narg\n          The maximum degree of the output graph. Use -1 for unbounded.. Default is -1.\n    --no-validation-output      No validation output files created.\n -o,--out \narg\n                 Output directory.\n    --output-prefix \narg\n       Prefix name for output files.\n    --penalty-discount \narg\n    Penalty discount. Default is 4.0.\n    --skip-latest               Skip checking for latest software version\n    --skip-nonzero-variance     Skip check for zero variance variables.\n    --skip-unique-var-name      Skip check for unique variable names.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread \narg\n              Number of threads.\n    --verbose                   Print additional information.\n\n\n\n\ncausal-cmd usage for FGES for discrete data\n\n\nusage: java -jar causal-cmd-6.0.1-jar-with-dependencies.jar --algorithm FGESd [-d \narg\n] [--exclude-variables \narg\n] -f \narg\n [--faithfulness-assumed] [--help] [--json] [--knowledge \narg\n] [--max-degree \narg\n] [--no-validation-output] [-o \narg\n] [--output-prefix \narg\n] [--sample-prior \narg\n] [--skip-category-limit] [--skip-latest] [--skip-unique-var-name] [--structure-prior \narg\n] [--tetrad-graph-json] [--thread \narg\n] [--verbose]\n -d,--delimiter \narg\n           Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables \narg\n   A file containing variables to exclude.\n -f,--data \narg\n                Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge \narg\n           A file containing prior knowledge.\n    --max-degree \narg\n          The maximum degree of the output graph. Use -1 for unbounded.. Default is -1.\n    --no-validation-output      No validation output files created.\n -o,--out \narg\n                 Output directory.\n    --output-prefix \narg\n       Prefix name for output files.\n    --sample-prior \narg\n        Sample prior. Default is 1.0.\n    --skip-category-limit       Skip 'limit number of categories' check.\n    --skip-latest               Skip checking for latest software version\n    --skip-unique-var-name      Skip check for unique variable names.\n    --structure-prior \narg\n     Structure prior coefficient. Default is 1.0.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread \narg\n              Number of threads.\n    --verbose                   Print additional information.\n\n\n\n\ncausal-cmd usage for GFCI for continuous data\n\n\nusage: java -jar causal-cmd-6.0.1-jar-with-dependencies.jar --algorithm GFCIc [--alpha \narg\n] [-d \narg\n] [--exclude-variables \narg\n] -f \narg\n [--faithfulness-assumed] [--help] [--json] [--knowledge \narg\n] [--max-degree \narg\n] [--no-validation-output] [-o \narg\n] [--output-prefix \narg\n] [--penalty-discount \narg\n] [--skip-latest] [--skip-nonzero-variance] [--skip-unique-var-name] [--tetrad-graph-json] [--thread \narg\n] [--verbose]\n    --alpha \narg\n               Cutoff for p values (alpha). Default is 0.01.\n -d,--delimiter \narg\n           Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables \narg\n   A file containing variables to exclude.\n -f,--data \narg\n                Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge \narg\n           A file containing prior knowledge.\n    --max-degree \narg\n          The maximum degree of the output graph. Use -1 for unbounded.. Default is -1.\n    --no-validation-output      No validation output files created.\n -o,--out \narg\n                 Output directory.\n    --output-prefix \narg\n       Prefix name for output files.\n    --penalty-discount \narg\n    Penalty discount. Default is 4.0.\n    --skip-latest               Skip checking for latest software version\n    --skip-nonzero-variance     Skip check for zero variance variables.\n    --skip-unique-var-name      Skip check for unique variable names.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread \narg\n              Number of threads.\n    --verbose                   Print additional information.\n\n\n\n\nPrior knowledge file example\n\n\n/knowledge\naddtemporal\n1 spending_per_stdt fac_salary stdt_tchr_ratio \n2 rjct_rate stdt_accept_rate \n3 tst_scores stdt_clss_stndng \n4* grad_rate \n\nforbiddirect\nx3 x4\n\nrequiredirect\nx1 x2\n\n\n\n\nThe first line must say /knowledge The three sections of knowledge are\n\n\n\n\nforbiddirect - forbidden edges indicated by a list of pairs of variables\n\n\nrequireddirect - required edges indicated by a list of pairs of variables\n\n\naddtemporal - tiers of variables where the first tier preceeds the last. Adding a asterisk next to the tier id prohibits edges between tier variables.", 
            "title": "Causal CMD"
        }, 
        {
            "location": "/causal-cmd/#documentation-on-using-the-causal-cmd-software", 
            "text": "", 
            "title": "Documentation on using the causal-cmd software"
        }, 
        {
            "location": "/causal-cmd/#what-is-causal-cmd", 
            "text": "Causal-cmd is a Java application that provides a command-line interface (CLI) and application programming interface (API) for causal discovery algorithms produced by the Center for Causal Discovery.  The application currently includes the algorithm(s):   FGESc (Fast Greedy Search) for continuous data - is an optimization of the Greedy Equivalence Search algorithm    (GES,   Meek    1995;   Chickering  2003). The optimizations are described in Scaling up Greedy Causal Search for Continuous Variables  FGESd (Fast Greedy Search) for discrete data  GFCIc (Greedy Fast Causal Inference) for continuous data   Note that in previous versions released by the Center, FGES was called FGS.  Causal discovery algorithms are a class of search algorithms that explore a space of graphical causal models, i.e., graphical models where directed edges imply causation, for a model (or models) that are a good fit for a dataset. We suggest that newcomers to the field review Causation, Prediction and Search by Spirtes, Glymour and Scheines for a primer on the subject.  Causal discovery algorithms allow a user to uncover the causal relationships between variables in a dataset. These discovered causal relationships may be used further--understanding the underlying the processes of a system (e.g., the metabolic pathways of an organism), hypothesis generation (e.g., variables that best explain an outcome), guide experimentation (e.g., what gene knockout experiments should be performed) or prediction (e.g. parameterization of the causal graph using data and then using it as a classifier).", 
            "title": "What is causal-cmd"
        }, 
        {
            "location": "/causal-cmd/#how-can-i-use-it", 
            "text": "Java 7 or higher is the only prerequisite to run the software. Note that by default Java will allocate the smaller of 1/4 system memory or 1GB to the Java virtual machine (JVM). If you run out of memory (heap memory space) running your analyses you should increase the memory allocated to the JVM with the following switch '-XmxXXG' where XX is the number of gigabytes of ram you allow the JVM to utilize. For example to allocate 8 gigabytes of ram you would add -Xmx8G immediately after the java command.", 
            "title": "How can I use it?"
        }, 
        {
            "location": "/causal-cmd/#run-an-example-output-using-known-data-via-command-line", 
            "text": "Download the this file,  Retention.txt , which is a dataset containing information on college graduation and used in the publication \"What Do College Ranking Data Tell Us About Student Retention?\" by Drudzel and Glymour, 1994.  java -jar causal-cmd-6.0.1-jar-with-dependencies.jar --algorithm FGESc --data Retention.txt  Note that the filename causal-cmd-x.x.x-jar-with-dependencies.jar should match the version you have downloaded. The program will output the results of the FGES search procedure as a text file (in this example to output). The beginning of the file contains the algorithm parameters used in the search.  Inspect the output which should show a graph with the following edges.  Graph Edges:\n1. fac_salary --- spending_per_stdt\n2. spending_per_stdt --  rjct_rate\n3. spending_per_stdt --- stdt_tchr_ratio\n4. stdt_accept_rate --- fac_salary\n5. stdt_clss_stndng --  rjct_rate\n6. tst_scores --- fac_salary\n7. tst_scores --- grad_rate\n8. tst_scores --- spending_per_stdt\n9. tst_scores --- stdt_clss_stndng  In FGES, \"Elapsed getEffectEdges = XXms\" refers to the amount of time it took to evaluate all pairs of variables for correlation. The file then details each step taken in the greedy search procedure i.e., insertion or deletion of edges based on a scoring function (i.e., BIC score difference for each chosen search operation).  The end of the file contains the causal graph from the search procedure. Here is a key to the edge types:   A --- B - There is causal relationship between variable A and B but we cannot determine the direction of the relationship  A --  B - There is a causal relationship from variable A to B   The GFCI algorithm has additional edge types:   A  -  B - There is an unmeasured confounder of A and B  A o-  B - Either A is a cause of B or there is an unmeasured confounder of A and B or both  A o-o B - Either (1) A is a cause of B or B is a cause of A, or (2) there is an unmeasured confounder of A and B, or both 1 and 2 hold.", 
            "title": "Run an example output using known data via command line"
        }, 
        {
            "location": "/causal-cmd/#use-as-an-api", 
            "text": "Here is an example of using the Tetrad library which is included in causal-cmd as an API. Javadocs for the API are  here .  import edu.cmu.tetrad.algcomparison.algorithm.Algorithm;\nimport edu.cmu.tetrad.algcomparison.algorithm.oracle.pattern.Fges;\nimport edu.cmu.tetrad.algcomparison.score.SemBicScore;\nimport edu.cmu.tetrad.cli.validation.DataValidation;\nimport edu.cmu.tetrad.cli.validation.TabularContinuousData;\nimport edu.cmu.tetrad.data.DataSet;\nimport edu.cmu.tetrad.graph.Graph;\nimport edu.cmu.tetrad.io.DataReader;\nimport edu.cmu.tetrad.io.TabularContinuousDataReader;\nimport edu.cmu.tetrad.util.Parameters;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\n\n/**\n *\n * Nov 29, 2016 4:42:42 PM\n *\n * @author Kevin V. Bui\n */\npublic class FgsApiExample {\n\n    /**\n     * @param args the command line arguments\n     */\n    public static void main(String[] args) throws Exception {\n        // set path to Retention data\n        Path dataFile = Paths.get( test ,  data ,  Retention.txt );\n\n        char delimiter = '\\t';\n\n        // perform data validation\n        // note: assuming data has unique variable names and does not contain zero covariance pairs\n        DataValidation dataValidation = new TabularContinuousData(dataFile, delimiter);\n        if (!dataValidation.validate(System.err, true)) {\n            System.exit(-128);\n        }\n\n        // create FGES algorithm that uses BIC score\n        Algorithm fges = new Fges(new SemBicScore());\n\n        // read in dataset\n        DataReader dataReader = new TabularContinuousDataReader(dataFile, delimiter);\n        DataSet dataSet = dataReader.readInData();\n\n        // set parameters for FGESc\n        Parameters parameters = new Parameters();\n        parameters.set( penaltyDiscount , 0.4);\n        parameters.set( maxDegree , -1);\n        parameters.set( faithfulnessAssumed , true);\n        parameters.set( verbose , true);\n        parameters.set( printStream , System.out);\n\n        // run FGES search with given dataset and parameters\n        Graph graph = fges.search(dataSet, parameters);\n        System.out.println();\n        System.out.println(graph.toString().trim());\n        System.out.flush();\n    }\n\n}", 
            "title": "Use as an API"
        }, 
        {
            "location": "/causal-cmd/#command-line-interface-usage", 
            "text": "Tetrad-cli has different switches for different algorithms.   usage: java -jar causal-cmd-6.0.1-SNAPSHOT.jar --algorithm  arg  |\n       --simulate-data  arg   [--version]\n    --algorithm  arg        FGESc, FGESd, GFCIc\n    --simulate-data  arg    sem-rand-fwd, bayes-net-rand-fwd\n    --version               Version.  Use the  --algorithm  arg  parameter to see specific algorithm usage information.", 
            "title": "Command line interface usage"
        }, 
        {
            "location": "/causal-cmd/#causal-cmd-usage-for-fges-for-continuous-data", 
            "text": "usage: java -jar causal-cmd-6.0.1-jar-with-dependencies.jar --algorithm FGESc [-d  arg ] [--exclude-variables  arg ] -f  arg  [--faithfulness-assumed] [--help] [--json] [--knowledge  arg ] [--max-degree  arg ] [--no-validation-output] [-o  arg ] [--output-prefix  arg ] [--penalty-discount  arg ] [--skip-latest] [--skip-nonzero-variance] [--skip-unique-var-name] [--tetrad-graph-json] [--thread  arg ] [--verbose]\n -d,--delimiter  arg            Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables  arg    A file containing variables to exclude.\n -f,--data  arg                 Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge  arg            A file containing prior knowledge.\n    --max-degree  arg           The maximum degree of the output graph. Use -1 for unbounded.. Default is -1.\n    --no-validation-output      No validation output files created.\n -o,--out  arg                  Output directory.\n    --output-prefix  arg        Prefix name for output files.\n    --penalty-discount  arg     Penalty discount. Default is 4.0.\n    --skip-latest               Skip checking for latest software version\n    --skip-nonzero-variance     Skip check for zero variance variables.\n    --skip-unique-var-name      Skip check for unique variable names.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread  arg               Number of threads.\n    --verbose                   Print additional information.", 
            "title": "causal-cmd usage for FGES for continuous data"
        }, 
        {
            "location": "/causal-cmd/#causal-cmd-usage-for-fges-for-discrete-data", 
            "text": "usage: java -jar causal-cmd-6.0.1-jar-with-dependencies.jar --algorithm FGESd [-d  arg ] [--exclude-variables  arg ] -f  arg  [--faithfulness-assumed] [--help] [--json] [--knowledge  arg ] [--max-degree  arg ] [--no-validation-output] [-o  arg ] [--output-prefix  arg ] [--sample-prior  arg ] [--skip-category-limit] [--skip-latest] [--skip-unique-var-name] [--structure-prior  arg ] [--tetrad-graph-json] [--thread  arg ] [--verbose]\n -d,--delimiter  arg            Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables  arg    A file containing variables to exclude.\n -f,--data  arg                 Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge  arg            A file containing prior knowledge.\n    --max-degree  arg           The maximum degree of the output graph. Use -1 for unbounded.. Default is -1.\n    --no-validation-output      No validation output files created.\n -o,--out  arg                  Output directory.\n    --output-prefix  arg        Prefix name for output files.\n    --sample-prior  arg         Sample prior. Default is 1.0.\n    --skip-category-limit       Skip 'limit number of categories' check.\n    --skip-latest               Skip checking for latest software version\n    --skip-unique-var-name      Skip check for unique variable names.\n    --structure-prior  arg      Structure prior coefficient. Default is 1.0.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread  arg               Number of threads.\n    --verbose                   Print additional information.", 
            "title": "causal-cmd usage for FGES for discrete data"
        }, 
        {
            "location": "/causal-cmd/#causal-cmd-usage-for-gfci-for-continuous-data", 
            "text": "usage: java -jar causal-cmd-6.0.1-jar-with-dependencies.jar --algorithm GFCIc [--alpha  arg ] [-d  arg ] [--exclude-variables  arg ] -f  arg  [--faithfulness-assumed] [--help] [--json] [--knowledge  arg ] [--max-degree  arg ] [--no-validation-output] [-o  arg ] [--output-prefix  arg ] [--penalty-discount  arg ] [--skip-latest] [--skip-nonzero-variance] [--skip-unique-var-name] [--tetrad-graph-json] [--thread  arg ] [--verbose]\n    --alpha  arg                Cutoff for p values (alpha). Default is 0.01.\n -d,--delimiter  arg            Data delimiter either comma, semicolon, space, colon, or tab. Default: comma for *.csv, else tab.\n    --exclude-variables  arg    A file containing variables to exclude.\n -f,--data  arg                 Data file.\n    --faithfulness-assumed      Yes if (one edge) faithfulness should be assumed. Default is false.\n    --help                      Show help.\n    --json                      Create JSON output.\n    --knowledge  arg            A file containing prior knowledge.\n    --max-degree  arg           The maximum degree of the output graph. Use -1 for unbounded.. Default is -1.\n    --no-validation-output      No validation output files created.\n -o,--out  arg                  Output directory.\n    --output-prefix  arg        Prefix name for output files.\n    --penalty-discount  arg     Penalty discount. Default is 4.0.\n    --skip-latest               Skip checking for latest software version\n    --skip-nonzero-variance     Skip check for zero variance variables.\n    --skip-unique-var-name      Skip check for unique variable names.\n    --tetrad-graph-json         Create Tetrad Graph JSON output.\n    --thread  arg               Number of threads.\n    --verbose                   Print additional information.", 
            "title": "causal-cmd usage for GFCI for continuous data"
        }, 
        {
            "location": "/causal-cmd/#prior-knowledge-file-example", 
            "text": "/knowledge\naddtemporal\n1 spending_per_stdt fac_salary stdt_tchr_ratio \n2 rjct_rate stdt_accept_rate \n3 tst_scores stdt_clss_stndng \n4* grad_rate \n\nforbiddirect\nx3 x4\n\nrequiredirect\nx1 x2  The first line must say /knowledge The three sections of knowledge are   forbiddirect - forbidden edges indicated by a list of pairs of variables  requireddirect - required edges indicated by a list of pairs of variables  addtemporal - tiers of variables where the first tier preceeds the last. Adding a asterisk next to the tier id prohibits edges between tier variables.", 
            "title": "Prior knowledge file example"
        }, 
        {
            "location": "/causal-web/", 
            "text": "Causal Web Application Quick Start and User Guide\n\n\nCreating Your Account\n\n\nWe are currently hosting an the Causal Web Application at the Pittsburgh Supercomputing Center. The url for the site is \nhere\n. On the button of the login page, there is a signup link. Click it!\n\n\n\n\nFill your information in the signup page. \nMake sure to read the \nTerms \n Conditions\n agreement and check the agree box before clicking the green signup button.\n\n\n\n\nThe system will send an email with an activation link. After clicking on the link, the Causal Web system shows a confirmation message.\n\n\n\n\nClick on the link in the email. Alternatively, click on the \n\"University\"\n button to login to the Causal Web via your Pitt/Harvard account.\n\n\n\n\nLogin to Causal Web Application\n\n\nInput your email address and password that you want to use to register with the Causal Web system. \nCheck the Remember Me checkbox if you would like the browser automatically log you in next time you visit.\n\n\n\n\nHere we go! You are now in the Causal Web application.\n\n\n\n\nUploading Your Dataset\n\n\nClick on the Data Management link on the navigation bar on the left side. There is a sub menu that will appear. Click on the Import Data link.\n\n\n\n\nYou can \nEITHER\n drag \n drop dataset file(s) into the dash-surrounded canvas \nOR\n \nyou can click on the \nBrowse\n button and choose the dataset file(s) you would like to upload to the Causal Web application. For testing purposes download this dataset: \nRetention.txt\n and upload it.\n\n\n\n\nThe \nImport Data\n panel shows the dataset upload progress as a percentage along with MD5 checksums (confirms that an uploaded file's contents are unchanged after upload) for each of uploaded files.\n\n\n\n\nYou can also pause the upload of files and resume later. In the case of a disrupted connection, you can resume the upload by repeating the previous steps. The Causal Web application will detect the unfinished upload and resume from the latest point of the last attempt.\n\n\nOnce all your dataset file(s) are all uploaded, the progress bar will show the \n(completed)\n sign.\n\n\nSummarizing Your Dataset\n\n\nBefore any analysis can proceed, the datasets need to be summarized. Specifically, you must indicate the delimiter used in the data file (tab vs. comma), and the types of variables found in the file. Once this is done, the Causal Web application will determine the number of columns (features) and rows (records) in the dataset. \n\n\nClick on the \nData Management\n menu on the navigation bar on the left side. The sub menu will slowly appear. Click on the \nDatasets\n menu.\n\n\n\n\nThe dataset page shows a list of datasets and their attributes. On the second \nSummarized\n column from the right, the yellow warning buttons indicate that the system has not yet summarized.\n\n\n\n\nClick on the dataset's name's link to see the dataset information. From this stage, the data summary information is missing: the dataset needs to be summarized before conducting causal analysis.\n\n\n\n\nFrom the dataset page, click on the yellow warning button to summarize a dataset. The data summarization page shows information of the dataset, its basic information, and additional information that will be determined after summarization (a number of rows and columns). The bottom panel has two radio boxes for you to choose variable type (continuous or discrete), and delimiter (tab or comma). The Retention.txt dataset described above is tab-delimited and contains continous variables.\n\n\n\n\nOnce the dataset is summarized, the dataset page changes its sign to be a green button. Click to see the additional information of this summarized dataset.\n\n\n\n\nClick on the dataset's name's link to see the additional information.\n\n\n\n\nUploading the Prior Knowledge\n\n\nClick on the \nData Management\n menu on the navigation bar on the left side. There is a sub menu that will appear. \nClick on the \nImport Data\n menu.\n\n\n\n\nYou can EITHER drag \n drop prior knowledge file(s) into the dash-surrounded canvas OR \nyou can click on the Browse button and choose the prior knowledge file(s) you would like to upload to the CCD Web application. Note that the prior knowledge file needs to have \n.prior\n file extension.\n\n\n\n\nExecuting an Analysis on Your Dataset\n\n\nClick on the Causal Discovery menu on the navigation bar on the left side. The sub menu will slowly appear. \nClick on the desired algorithms. As of November 2016, \nFGES\n \n(Continuous)\n, \nFGES Discrete\n, and \nGFCI\n \n(Continuous)\n are the only supported algorithms.  Note that in previous versions of our software FGES was known as \n\n\n\n\nThe \nDataset\n drop-down box contains a list of datasets that the system recognize. If those datasets are already uploaded and they are not displayed in the dataset drop-down box, it means that the \nData Summarization\n process to be reviewed in the first place prior to execute a causal \nFGES\n \n(Continuous)\n analysis. \nIf a prior knowledge file needs to be computed, Prior Knowledge File drop-down box contains a list of knowledge files. \nBefore clicking the \nNext\n button, the data validation parameters need to be input.\n\n\n\n\nHere, the \nFGES\n algorithm page allows user to modify its parameters. \n\n\n\n\nThe first one is Penalty Discount and its default value is 4. \n\n\nThe second one is Search Maximum Degree and its default value is 100. \n\n\nThe third one is Faithfulness Assumed and its default value is checked.\n\n\nThe fifth one is Verbose output and its default value is checked.\n\n\n\n\nClick \nNext\n to proceed or click Advanced Options (JVM) for the JVM customization.\n\n\n\n\nExpert Mode\n: the JVM parameters allow users to customize JVM parameters such how much maximum memory (in Gigabyte scale) the process would allocate (e.g. 4).\n\n\n\n\nThis is the summary page before the FGES job analysis is put into the queue. \nClick on the number 1 (Select Dataset) or number 2 (Set Parameters) to go back to modify the parameters. \nOnce, everything is set. Click the \nRun Algorithm!\n button.\n\n\n\n\nThe application will redirect the \nJob Queue\n page. The analysis job is added to the queue. The \nQueued\n status means that it waits for the scheduler to run it once the executing slot is available. However, the \nJob Queue\n page does not currently automatically update the jobs' status (at least in this development stage). Refresh the \nJob Queue\n page from time to time to see the latest jobs' status.\n\n\n\n\nOnce the job slot is available, the queued job is then executed and its status changes to \nRunning\n.\n\n\n\n\nWhen the job is finished, it is automatically removed from the \nJob Queue\n page. The result of the analysis is added to the Results page.\n\n\n\n\nIn case the \nqueued\n or \nrunning\n job needs to be killed or removed, click the \nRemove\n button on the first column on the \nJob Queue\n page from the right. \nThe \nRemove Job\n confirmation page is popped up. Click \nYes\n to kill the job or \nNo\n to cancel the kill operation.\n\n\n\n\nAfter the job cancellation is confirmed, the job's status changes to \nKill Request\n. The scheduler will take care of removing of the job from the queue or killing a job in the server.\n\n\n\n\nIf the running job was killed or any error happened during the process, the error result will appear in the \nResults\n page. Its background is highlighted in red.\n\n\n\n\nIf there is an error, you will see the details of the error by clicking on error result link.\n\n\n\n\nReviewing Your Result\n\n\nClick on the \nResults\n menu on the navigation bar on the left side. \nClick on the \nAlgorithm Results\n menu.\n\n\n\n\nThe \nAlgorithm Results\n page shows a list of results, their creation time and their size. On the first column from the right, the green \nSave\n buttons provide the ability for users to download results to their local computers. \nClick on the result's name's link to see a causal graph of the result.\n\n\nCheck the result files on their checkboxes to \ncompare the results\n. \n\nNote\n: a number of comparing datasets can be more than two files.\n\n\n\n\nThe results page details the graph, the original dataset, and its parameters. \nClick on the \nView Full Screen\n button to see the causal graph in more detail.\n\n\n\n\nThe figure shows the causal graph in the full-screen mode.\n\n\n\n\nComparing Your Result\n\n\nClick on the \nResults\n menu on the navigation bar on the left side. To compare two results click on the Algorithm Results item on the left. Select at least two results (place a checkmark next to the results) and click on Compare. Now click on the \nResult Comparisions\n item on the left.\n\n\n\n\nThe Result Comparisons page shows a list of results, their creation time and their size. On the first column from the right, the green Save buttons provide the ability for users to download results to their local computers. Click on the result's name's the link to see the detail of the result comparisons.\n\n\n\n\nThe \nResult Comparisons\n page shows the datasets compared, and the table of edges, their mutual appearance in all comparing datasets, and their mutual endpoint types.\n\n\n\n\nDownloading Your Result And Comparision Result\n\n\nOn the first column from the right of the \nAlgorithm Results\n page, the green \nSave\n buttons provide the ability for users to download results to their local computers.\n\n\n\n\nOn the first column from the right of the \nResult Comparisions\n page, the green \nSave\n buttons provide the ability for users to download result comparisons to their local computers.\n\n\n\n\nSubmit Your Feedback\n\n\nClick the \nFeedback\n menu on the navigation menu bar on the left. \nThe \nFeedback\n page shows the email (optional), and the text area for the user feedback (required). \nOnce, the feedback is filled, click the \nSend Feedback\n button.\n\n\n\n\nThe green \nThank you for you feedback!\n banner shows that the feedback submitted successfully.", 
            "title": "Causal Web"
        }, 
        {
            "location": "/causal-web/#causal-web-application-quick-start-and-user-guide", 
            "text": "", 
            "title": "Causal Web Application Quick Start and User Guide"
        }, 
        {
            "location": "/causal-web/#creating-your-account", 
            "text": "We are currently hosting an the Causal Web Application at the Pittsburgh Supercomputing Center. The url for the site is  here . On the button of the login page, there is a signup link. Click it!   Fill your information in the signup page. \nMake sure to read the  Terms   Conditions  agreement and check the agree box before clicking the green signup button.   The system will send an email with an activation link. After clicking on the link, the Causal Web system shows a confirmation message.   Click on the link in the email. Alternatively, click on the  \"University\"  button to login to the Causal Web via your Pitt/Harvard account.", 
            "title": "Creating Your Account"
        }, 
        {
            "location": "/causal-web/#login-to-causal-web-application", 
            "text": "Input your email address and password that you want to use to register with the Causal Web system. \nCheck the Remember Me checkbox if you would like the browser automatically log you in next time you visit.   Here we go! You are now in the Causal Web application.", 
            "title": "Login to Causal Web Application"
        }, 
        {
            "location": "/causal-web/#uploading-your-dataset", 
            "text": "Click on the Data Management link on the navigation bar on the left side. There is a sub menu that will appear. Click on the Import Data link.   You can  EITHER  drag   drop dataset file(s) into the dash-surrounded canvas  OR  \nyou can click on the  Browse  button and choose the dataset file(s) you would like to upload to the Causal Web application. For testing purposes download this dataset:  Retention.txt  and upload it.   The  Import Data  panel shows the dataset upload progress as a percentage along with MD5 checksums (confirms that an uploaded file's contents are unchanged after upload) for each of uploaded files.   You can also pause the upload of files and resume later. In the case of a disrupted connection, you can resume the upload by repeating the previous steps. The Causal Web application will detect the unfinished upload and resume from the latest point of the last attempt.  Once all your dataset file(s) are all uploaded, the progress bar will show the  (completed)  sign.", 
            "title": "Uploading Your Dataset"
        }, 
        {
            "location": "/causal-web/#uploading-the-prior-knowledge", 
            "text": "Click on the  Data Management  menu on the navigation bar on the left side. There is a sub menu that will appear. \nClick on the  Import Data  menu.   You can EITHER drag   drop prior knowledge file(s) into the dash-surrounded canvas OR \nyou can click on the Browse button and choose the prior knowledge file(s) you would like to upload to the CCD Web application. Note that the prior knowledge file needs to have  .prior  file extension.", 
            "title": "Uploading the Prior Knowledge"
        }, 
        {
            "location": "/causal-web/#executing-an-analysis-on-your-dataset", 
            "text": "Click on the Causal Discovery menu on the navigation bar on the left side. The sub menu will slowly appear. \nClick on the desired algorithms. As of November 2016,  FGES   (Continuous) ,  FGES Discrete , and  GFCI   (Continuous)  are the only supported algorithms.  Note that in previous versions of our software FGES was known as    The  Dataset  drop-down box contains a list of datasets that the system recognize. If those datasets are already uploaded and they are not displayed in the dataset drop-down box, it means that the  Data Summarization  process to be reviewed in the first place prior to execute a causal  FGES   (Continuous)  analysis. \nIf a prior knowledge file needs to be computed, Prior Knowledge File drop-down box contains a list of knowledge files. \nBefore clicking the  Next  button, the data validation parameters need to be input.   Here, the  FGES  algorithm page allows user to modify its parameters.    The first one is Penalty Discount and its default value is 4.   The second one is Search Maximum Degree and its default value is 100.   The third one is Faithfulness Assumed and its default value is checked.  The fifth one is Verbose output and its default value is checked.   Click  Next  to proceed or click Advanced Options (JVM) for the JVM customization.   Expert Mode : the JVM parameters allow users to customize JVM parameters such how much maximum memory (in Gigabyte scale) the process would allocate (e.g. 4).   This is the summary page before the FGES job analysis is put into the queue. \nClick on the number 1 (Select Dataset) or number 2 (Set Parameters) to go back to modify the parameters. \nOnce, everything is set. Click the  Run Algorithm!  button.   The application will redirect the  Job Queue  page. The analysis job is added to the queue. The  Queued  status means that it waits for the scheduler to run it once the executing slot is available. However, the  Job Queue  page does not currently automatically update the jobs' status (at least in this development stage). Refresh the  Job Queue  page from time to time to see the latest jobs' status.   Once the job slot is available, the queued job is then executed and its status changes to  Running .   When the job is finished, it is automatically removed from the  Job Queue  page. The result of the analysis is added to the Results page.   In case the  queued  or  running  job needs to be killed or removed, click the  Remove  button on the first column on the  Job Queue  page from the right. \nThe  Remove Job  confirmation page is popped up. Click  Yes  to kill the job or  No  to cancel the kill operation.   After the job cancellation is confirmed, the job's status changes to  Kill Request . The scheduler will take care of removing of the job from the queue or killing a job in the server.   If the running job was killed or any error happened during the process, the error result will appear in the  Results  page. Its background is highlighted in red.   If there is an error, you will see the details of the error by clicking on error result link.", 
            "title": "Executing an Analysis on Your Dataset"
        }, 
        {
            "location": "/causal-web/#reviewing-your-result", 
            "text": "Click on the  Results  menu on the navigation bar on the left side. \nClick on the  Algorithm Results  menu.   The  Algorithm Results  page shows a list of results, their creation time and their size. On the first column from the right, the green  Save  buttons provide the ability for users to download results to their local computers. \nClick on the result's name's link to see a causal graph of the result.  Check the result files on their checkboxes to  compare the results .  Note : a number of comparing datasets can be more than two files.   The results page details the graph, the original dataset, and its parameters. \nClick on the  View Full Screen  button to see the causal graph in more detail.   The figure shows the causal graph in the full-screen mode.", 
            "title": "Reviewing Your Result"
        }, 
        {
            "location": "/causal-web/#downloading-your-result-and-comparision-result", 
            "text": "On the first column from the right of the  Algorithm Results  page, the green  Save  buttons provide the ability for users to download results to their local computers.   On the first column from the right of the  Result Comparisions  page, the green  Save  buttons provide the ability for users to download result comparisons to their local computers.", 
            "title": "Downloading Your Result And Comparision Result"
        }, 
        {
            "location": "/causal-web/#submit-your-feedback", 
            "text": "Click the  Feedback  menu on the navigation menu bar on the left. \nThe  Feedback  page shows the email (optional), and the text area for the user feedback (required). \nOnce, the feedback is filled, click the  Send Feedback  button.   The green  Thank you for you feedback!  banner shows that the feedback submitted successfully.", 
            "title": "Submit Your Feedback"
        }, 
        {
            "location": "/causal-rest-api/", 
            "text": "Causal REST API v0.0.7\n\n\nThis RESTful API is designed for causal web. And it implements the \nJAX-RS\n specifications using Jersey.\n\n\nTable of Contents\n\n\n\n\nInstallation\n\n\nPrerequisites\n\n\nDependencies\n\n\nConfiguration\n\n\nStart the API Server\n\n\nAPI Usage and Examples\n\n\nGetting JSON Web Token(JWT)\n\n\n1. Data Management\n\n\nUpload small data file\n\n\nResumable data file upload\n\n\nList all dataset files of a user\n\n\nGet the detail information of a dataset file based on ID\n\n\nDelete physical dataset file and all records from database for a given file ID\n\n\nSummarize dataset file\n\n\nList all prior knowledge files of a given user\n\n\nGet the detail information of a prior knowledge file based on ID\n\n\nDelete physical prior knowledge file and all records from database for a given file ID\n\n\n\n\n\n\n2. Causal Discovery\n\n\nList all the available causal discovery algorithms\n\n\nAdd a new job to run the desired algorithm on a given data file\n\n\nList all running jobs\n\n\nCheck the job status for a given job ID\n\n\nCancel a running job\n\n\n\n\n\n\n3. Result Management\n\n\nList all result files generated by the algorithm\n\n\nDownload a speific result file generated by the algorithm based on file name\n\n\nCompare algorithm result files\n\n\nList all the comparison files\n\n\nDownload a speific comparison file based on file name\n\n\n\n\n\n\n\n\nInstallation\n\n\nThe following installation instructions are supposed to be used by the server admin who deploys this API server. API users can skip this section and just start reading from the \nAPI Usage and Examples\n section. \n\n\nPrerequisites\n\n\nYou must have the following installed to build/install Causal REST API:\n\n\n\n\nOracle Java SE Development Kit 8\n\n\nMaven 3.x\n\n\n\n\nDependencies\n\n\nIf you want to run this API server and expose the API to your users, you'll first need to have the \nCausal Web Application\n installed and running. Your API users will use this web app to create their user accounts before they can consume the API. \n\n\nNote: currently new users can also be created using Auth0 login option, but the API doesn't work for these users.\n\n\nIn order to build the API server, you'll need the released version of \nccd-commons-0.3.1\n by going to the repo and checkout this specific release version:\n\n\ngit clone https://github.com/bd2kccd/ccd-commons.git\ncd ccd-commons\ngit checkout tags/v0.3.1\nmvn clean install\n\n\n\n\nYou'll also need to download released \nccd-db-0.6.3\n:\n\n\ngit clone https://github.com/bd2kccd/ccd-db.git\ncd ccd-db\ngit checkout tags/v0.6.3\nmvn clean install\n\n\n\n\nThen you can go get and install \ncausal-rest-api\n:\n\n\ngit clone https://github.com/bd2kccd/causal-rest-api.git\ncd causal-rest-api\nmvn clean package\n\n\n\n\nConfiguration\n\n\nThere are 4 configuration files to configure located at \ncausal-rest-api/src/main/resources\n:\n- \napplication-hsqldb.properties\n: HSQLDB database configurations (for testing only).\n- \napplication-mysql.properties\n: MySQL database configurations\n- \napplication-slurm.properties\n: Slurm setting for HPC\n- \napplication.properties\n: Spring Boot application settings\n- \ncausal.properties\n: Data file directory path and folder settings\n\n\nBefor editing the \ncausal.properties\n file, you need to create a workspace for the application to work in. Create a directory called workspace, for an example \n/home/zhy19/ccd/workspace\n. Inside the workspace directory, create another folder called \nlib\n. Then build the jar file of Tetred using the \nlatest development branch\n. After that, copy the jar file to the \nlib\n folder created earlier.\n\n\nStart the API Server\n\n\nOnce you have all the settings configured, go to \ncausal-rest-api/target\n and you will find the jar file named \ncausal-rest-api.jar\n. Then simply run \n\n\njava -jar causal-rest-api.jar\n\n\n\n\nAPI Usage and Examples\n\n\nIn the following sections, we'll demonstrate the API usage with examples using the API server that is running on Pittsburgh Super Computing. The API base URI is https://ccd2.vm.bridges.psc.edu/ccd-api.\n\n\nThis API requires user to be authenticated. Before using this API, the user will need to go to \nCausal-Web App\n and create an account. \n\n\nGetting JSON Web Token(JWT)\n\n\nAfter registration in Causal Web App, the email and password can be used to authenticate against the Causal REST API to get the access token (we use JWT) via \nHTTP Basic Auth\n. \n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/jwt\n\n\n\n\nIn basic auth, the user provides the username and password, which the HTTP client concatenates (username + \":\" + password), and base64 encodes it. This encoded string is then sent using a \nAuthorization\n header with the \"Basic\" schema. For instance user email \ndemo@pitt.edu\n whose password is \n123\n.\n\n\nPOST /ccd-api/jwt HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Basic ZGVtb0BwaXR0LmVkdToxMjM=\n\n\n\n\nOnce the request is processed successfully, the user ID together with a JWT will be returned in the response for further API queries.\n\n\n{\n  \nuserId\n: 22,\n  \njwt\n: \neyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA0Mjg1OTcsImlhdCI6MTQ3NTg0NjgyODU5N30.FcE7aEpg0u2c-gUVugIjJkzjhlDu5qav_XHtgLu3c6E\n,\n  \nissuedTime\n: 1475846828597,\n  \nlifetime\n: 3600,\n  \nexpireTime\n: 1475850428597\n}\n\n\n\n\nWe'll need to use this \nuserId\n in the URI path of all subsequent requests. And this \njwt\n expires in 3600 seconds(1 hour), so the API consumer will need to request for another JWT otherwise the API query to other API endpoints will be denied. And this JWT will need to be sent via the HTTP \nAuthorization\n header as well, but using the \nBearer\n schema.\n\n\nNote: querying the JWT endpoint again before the current JWT expires will generate a new JWT, which makes the old JWT expired automatically. And this newly generated JWT will be valid in another hour unless there's another new JWT being queried.\n\n\nSince this API is developed with Jersey, which supports \nWADL\n. So you can view the generated WADL by going to \nhttps://ccd2.vm.bridges.psc.edu/ccd-api/application.wadl?detail=true\n and see all resource available in the application. Accessing to this endpoint doesn't require authentication.\n\n\nBasically, all the API usage examples are grouped into three categories: \n\n\n\n\nData Management\n\n\nCausal Discovery\n\n\nResult Management\n\n\n\n\nAnd all the following examples will be issued by user \n22\n whose password is \n123\n.\n\n\n1. Data Management\n\n\nUpload small data file\n\n\nAt this point, you can upload two types of data files: tabular dataset file(either tab delimited or comma delimited) and prior knowledge file.\n\n\nAPI Endpoint URI pattern:\n\n\nPOST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/dataset/upload\n\n\n\n\nThis is a multipart file upload via an HTML form, and the client is required to use \nname=\"file\"\n to name their file upload field in their form.\n\n\nGenerated HTTP request code example:\n\n\nPOST /ccd-api/22/dataset/upload HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW\n\n----WebKitFormBoundary7MA4YWxkTrZu0gW\nContent-Disposition: form-data; name=\nfile\n; filename=\n\nContent-Type: \n\n\n----WebKitFormBoundary7MA4YWxkTrZu0gW\n\n\n\n\nIf the Authorization header is not provided, the response will look like this:\n\n\n{\n  \ntimestamp\n: 1465414501443,\n  \nstatus\n: 401,\n  \nerror\n: \nUnauthorized\n,\n  \nmessage\n: \nUser credentials are required.\n,\n  \npath\n: \n/22/dataset/upload\n\n}\n\n\n\n\nThis POST request will upload the dataset file to the target server location and add corresponding records into database. And the response will contain the following pieces:\n\n\n{\n    \nid\n: 6,\n    \nname\n: \nLung-tetrad_hv.txt\n,\n    \ncreationTime\n: 1466622267000,\n    \nlastModifiedTime\n: 1466622267000,\n    \nfileSize\n: 3309465,\n    \nmd5checkSum\n: \nb1db7511ee293d297e3055d9a7b46c5e\n,\n    \nfileSummary\n: {\n      \nvariableType\n: null,\n      \nfileDelimiter\n: null,\n      \nnumOfRows\n: null,\n      \nnumOfColumns\n: null\n    }\n  }\n\n\n\n\nThe prior knowledge file upload uses a similar API endpoint:\n\n\nPOST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge/upload\n\n\n\n\nDue to there's no need to summarize a prior knowledge file, the response of a successful prior knowledge file upload will look like:\n\n\n{\n    \nid\n: 6,\n    \nname\n: \nLung-tetrad_hv.txt\n,\n    \ncreationTime\n: 1466622267000,\n    \nlastModifiedTime\n: 1466622267000,\n    \nfileSize\n: 3309465,\n    \nmd5checkSum\n: \nugdb7511rt293d29ke3055d9a7b46c9k\n\n  }\n\n\n\n\nResumable data file upload\n\n\nIn addition to the regular file upload described in Example 6, we also provide the option of stable and resumable large file upload. It requires the client side to have a resumable upload implementation. We currently support client integrated with \nResumable.js\n, whihc provides multiple simultaneous, stable \nand resumable uploads via the HTML5 File API. You can also create your own client as long as al the following parameters are set correctly.\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/chunkupload\n\nPOST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/chunkupload\n\n\n\n\nIn this example, the data file is splited into 3 chunks. The upload of each chunk consists of a GET request and a POST request. To handle the state of upload chunks, a number of extra parameters are sent along with all requests:\n\n\n\n\nresumableChunkNumber\n: The index of the chunk in the current upload. First chunk is \n1\n (no base-0 counting here).\n\n\nresumableChunkSize\n: The general chunk size. Using this value and \nresumableTotalSize\n you can calculate the total number of chunks. Please note that the size of the data received in the HTTP might be lower than \nresumableChunkSize\n of this for the last chunk for a file.\n\n\nresumableCurrentChunkSize\n: The size of the current resumable chuck.\n\n\nresumableTotalSize\n: The total file size.\n\n\nresumableType\n: The file type of the resumable chuck, e.e., \"text/plain\".\n\n\nresumableIdentifier\n: A unique identifier for the file contained in the request.\n\n\nresumableFilename\n: The original file name (since a bug in Firefox results in the file name not being transmitted in chunk multipart posts).\n\n\nresumableRelativePath\n: The file's relative path when selecting a directory (defaults to file name in all browsers except Chrome).\n\n\nresumableTotalChunks\n: The total number of chunks.  \n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/chunkupload?resumableChunkNumber=2\nresumableChunkSize=1048576\nresumableCurrentChunkSize=1048576\nresumableTotalSize=3309465\nresumableType=text%2Fplain\nresumableIdentifier=3309465-large-datatxt\nresumableFilename=large-data.txt\nresumableRelativePath=large-data.txt\nresumableTotalChunks=3 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nThis GET request checks if the data chunk is already on the server side. If the target file chunk is not found on the server, the client will issue a POST request to upload the actual data.\n\n\nGenerated HTTP request code example:\n\n\nPOST /ccd-api/22/chunkupload HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundaryMFjgApg56XGyeTnZ\n\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableChunkNumber\n\n\n2\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableChunkSize\n\n\n1048576\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableCurrentChunkSize\n\n\n1048576\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableTotalSize\n\n\n3309465\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableType\n\n\ntext/plain\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableIdentifier\n\n\n3309465-large-datatxt\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableFilename\n\n\nlarge-data.txt\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableRelativePath\n\n\nlarge-data.txt\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nresumableTotalChunks\n\n\n3\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name=\nfile\n; filename=\nblob\n\nContent-Type: application/octet-stream\n\n\n------WebKitFormBoundaryMFjgApg56XGyeTnZ--\n\n\n\n\nEach chunk upload POST will get a 200 status code from response if everything works fine.\n\n\nAnd finally the md5checkSum string of the reassemabled file will be returned once the whole file has been uploaded successfully. In this example, the POST request that uploads the third chunk will response this:\n\n\nb1db7511ee293d297e3055d9a7b46c5e\n\n\n\n\nList all dataset files of a user\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/dataset\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/dataset HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nAccept: application/json\n\n\n\n\nA \nJSON\n formatted list of all the input dataset files that are associated with user \n22\n will be returned.\n\n\n[\n  {\n    \nid\n: 8,\n    \nname\n: \ndata_small.txt\n,\n    \ncreationTime\n: 1467132449000,\n    \nlastModifiedTime\n: 1467132449000,\n    \nfileSize\n: 278428,\n    \nmd5checkSum\n: \ned5f27a2cf94fe3735a5d9ed9191c382\n,\n    \nfileSummary\n: {\n      \nvariableType\n: \ncontinuous\n,\n      \nfileDelimiter\n: \ntab\n,\n      \nnumOfRows\n: 302,\n      \nnumOfColumns\n: 123\n    }\n  },\n  {\n    \nid\n: 10,\n    \nname\n: \nlarge-data.txt\n,\n    \ncreationTime\n: 1467134048000,\n    \nlastModifiedTime\n: 1467134048000,\n    \nfileSize\n: 3309465,\n    \nmd5checkSum\n: \nb1db7511ee293d297e3055d9a7b46c5e\n,\n    \nfileSummary\n: {\n      \nvariableType\n: null,\n      \nfileDelimiter\n: null,\n      \nnumOfRows\n: null,\n      \nnumOfColumns\n: null\n    }\n  },\n  {\n    \nid\n: 11,\n    \nname\n: \nLung-tetrad_hv (copy).txt\n,\n    \ncreationTime\n: 1467140415000,\n    \nlastModifiedTime\n: 1467140415000,\n    \nfileSize\n: 3309465,\n    \nmd5checkSum\n: \nb1db7511ee293d297e3055d9a7b46c5e\n,\n    \nfileSummary\n: {\n      \nvariableType\n: \ncontinuous\n,\n      \nfileDelimiter\n: \ntab\n,\n      \nnumOfRows\n: 302,\n      \nnumOfColumns\n: 608\n    }\n  }\n]\n\n\n\n\nYou can also specify the response format as XML in your request\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/dataset HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nAccept: application/xml\n\n\n\n\nAnd the response will look like this:\n\n\n?xml version=\n1.0\n encoding=\nUTF-8\n standalone=\nyes\n?\n\n\ndatasetFileDTOes\n\n    \ndatasetFile\n\n        \nid\n8\n/id\n\n        \nname\ndata_small.txt\n/name\n\n        \ncreationTime\n2016-06-28T12:47:29-04:00\n/creationTime\n\n        \nlastModifiedTime\n2016-06-28T12:47:29-04:00\n/lastModifiedTime\n\n        \nfileSize\n278428\n/fileSize\n\n        \nmd5checkSum\ned5f27a2cf94fe3735a5d9ed9191c382\n/md5checkSum\n\n        \nfileSummary\n\n            \nfileDelimiter\ntab\n/fileDelimiter\n\n            \nnumOfColumns\n123\n/numOfColumns\n\n            \nnumOfRows\n302\n/numOfRows\n\n            \nvariableType\ncontinuous\n/variableType\n\n        \n/fileSummary\n\n    \n/datasetFile\n\n    \ndatasetFile\n\n        \nid\n10\n/id\n\n        \nname\nlarge-data.txt\n/name\n\n        \ncreationTime\n2016-06-28T13:14:08-04:00\n/creationTime\n\n        \nlastModifiedTime\n2016-06-28T13:14:08-04:00\n/lastModifiedTime\n\n        \nfileSize\n3309465\n/fileSize\n\n        \nmd5checkSum\nb1db7511ee293d297e3055d9a7b46c5e\n/md5checkSum\n\n        \nfileSummary\n\n            \nvariableType xmlns:xsi=\nhttp://www.w3.org/2001/XMLSchema-instance\n xsi:nil=\ntrue\n/\n\n            \nfileDelimiter xmlns:xsi=\nhttp://www.w3.org/2001/XMLSchema-instance\n xsi:nil=\ntrue\n/\n\n            \nnumOfRows xmlns:xsi=\nhttp://www.w3.org/2001/XMLSchema-instance\n xsi:nil=\ntrue\n/\n\n            \nnumOfColumns xmlns:xsi=\nhttp://www.w3.org/2001/XMLSchema-instance\n xsi:nil=\ntrue\n/\n\n        \n/fileSummary\n\n    \n/datasetFile\n\n    \ndatasetFile\n\n        \nid\n11\n/id\n\n        \nname\nLung-tetrad_hv (copy).txt\n/name\n\n        \ncreationTime\n2016-06-28T15:00:15-04:00\n/creationTime\n\n        \nlastModifiedTime\n2016-06-28T15:00:15-04:00\n/lastModifiedTime\n\n        \nfileSize\n3309465\n/fileSize\n\n        \nmd5checkSum\nb1db7511ee293d297e3055d9a7b46c5e\n/md5checkSum\n\n        \nfileSummary\n\n            \nfileDelimiter\ntab\n/fileDelimiter\n\n            \nnumOfColumns\n608\n/numOfColumns\n\n            \nnumOfRows\n302\n/numOfRows\n\n            \nvariableType\ncontinuous\n/variableType\n\n        \n/fileSummary\n\n    \n/datasetFile\n\n\n/datasetFileDTOes\n\n\n\n\n\nForm the above output, we can also tell that data file with ID 10 doesn't have all the \nfileSummary\n field values set, we'll cover this in the dataset summarization section.\n\n\nGet the detail information of a dataset file based on ID\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/dataset/{id}\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/dataset/8 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nAnd the resulting response looks like this:\n\n\n{\n  \nid\n: 8,\n  \nname\n: \ndata_small.txt\n,\n  \ncreationTime\n: 1467132449000,\n  \nlastModifiedTime\n: 1467132449000,\n  \nfileSize\n: 278428,\n  \nfileSummary\n: {\n    \nmd5checkSum\n: \ned5f27a2cf94fe3735a5d9ed9191c382\n,\n    \nvariableType\n: \ncontinuous\n,\n    \nfileDelimiter\n: \ntab\n,\n    \nnumOfRows\n: 302,\n    \nnumOfColumns\n: 123\n  }\n}\n\n\n\n\nDelete physical dataset file and all records from database for a given file ID\n\n\nAPI Endpoint URI pattern:\n\n\nDELETE https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/dataset/{id}\n\n\n\n\nGenerated HTTP request code example:\n\n\nDELETE /ccd-api/22/dataset/8 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nAnd this will result a HTTP 204 No Content status in response on success, which means the server successfully processed the deletion request but there's no content to response.\n\n\nSummarize dataset file\n\n\nSo from the first example we can tell that file with ID 10 doesn't have \nvariableType\n, \nfileDelimiter\n, \nnumOfRows\n, and \nnumOfColumns\n specified under \nfileSummary\n. Among these attributes, variableType\nand\nfileDelimiter` are the ones that users will need to provide during this summarization process.\n\n\nBefore we can go ahead to run the desired algorithm with the newly uploaded data file, we'll need to summarize the data by specifing the variable type and file delimiter.\n\n\n\n\n\n\n\n\nRequired Fields\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nid\n\n\nThe data file ID\n\n\n\n\n\n\nvariableType\n\n\ndiscrete or continuous\n\n\n\n\n\n\nfileDelimiter\n\n\ntab or comma\n\n\n\n\n\n\n\n\nAPI Endpoint URI pattern:\n\n\nPOST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/dataset/summarize\n\n\n\n\nGenerated HTTP request code example:\n\n\nPOST /ccd-api/22/dataset/summarize HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n{\n    \nid\n: 1,\n    \nvariableType\n: \ncontinuous\n,\n    \nfileDelimiter\n: \ncomma\n\n}\n\n\n\n\nThis POST request will summarize the dataset file and generate a response (JSON or XML) like below:\n\n\n{\n  \nid\n: 10,\n  \nname\n: \nlarge-data.txt\n,\n  \ncreationTime\n: 1467134048000,\n  \nlastModifiedTime\n: 1467134048000,\n  \nfileSize\n: 3309465,\n  \nmd5checkSum\n: \nb1db7511ee293d297e3055d9a7b46c5e\n,\n  \nfileSummary\n: {\n    \nvariableType\n: \ncontinuous\n,\n    \nfileDelimiter\n: \ntab\n,\n    \nnumOfRows\n: 302,\n    \nnumOfColumns\n: 608\n  }\n}\n\n\n\n\nList all prior knowledge files of a given user\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/priorknowledge HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nAccept: application/json\n\n\n\n\nA \nJSON\n formatted list of all the input dataset files that are associated with user \n22\n will be returned.\n\n\n[\n  {\n    \nid\n: 9,\n    \nname\n: \ndata_small.prior\n,\n    \ncreationTime\n: 1467132449000,\n    \nlastModifiedTime\n: 1467132449000,\n    \nfileSize\n: 278428,\n    \nmd5checkSum\n: \ned5f27a2cf94fe3735a5d9ed9191c382\n\n  },\n  {\n    \nid\n: 12,\n    \nname\n: \nlarge-data.prior\n,\n    \ncreationTime\n: 1467134048000,\n    \nlastModifiedTime\n: 1467134048000,\n    \nfileSize\n: 3309465,\n    \nmd5checkSum\n: \nb1db7511ee293d297e3055d9a7b46c5e\n\n  }\n]\n\n\n\n\nGet the detail information of a prior knowledge file based on ID\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge/{id}\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/priorknowledge/9 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nAnd the resulting response looks like this:\n\n\n{\n  \nid\n: 9,\n  \nname\n: \ndata_small.prior\n,\n  \ncreationTime\n: 1467132449000,\n  \nlastModifiedTime\n: 1467132449000,\n  \nfileSize\n: 278428,\n  \nmd5checkSum\n: \ned5f27a2cf94fe3735a5d9ed9191c382\n\n}\n\n\n\n\nDelete physical prior knowledge file and all records from database for a given file ID\n\n\nAPI Endpoint URI pattern:\n\n\nDELETE https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge/{id}\n\n\n\n\nGenerated HTTP request code example:\n\n\nDELETE /ccd-api/22/priorknowledge/9 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nAnd this will result a HTTP 204 No Content status in response on success, which means the server successfully processed the deletion request but there's no content to response.\n\n\n2. Causal Discovery\n\n\nOnce the data file is uploaded and summaried, you can start running a Causal Discovery Algorithm on the uploaded data file.\n\n\nList all the available causal discovery algorithms\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/algorithms\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/algorithms HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nCurrently we support \"FGES continuous\" and \"FGES discrete\".\n\n\n[\n  {\n    \nid\n: 1,\n    \nname\n: \nFGESc\n,\n    \ndescription\n: \nFGES continuous\n\n  },\n  {\n    \nid\n: 2,\n    \nname\n: \nFGESd\n,\n    \ndescription\n: \nFGES discrete\n\n  },\n  {\n    \nid\n: 3,\n    \nname\n: \nGFCIc\n,\n    \ndescription\n: \nGFCI continuous\n\n  }\n]\n\n\n\n\nCurrently we support \"FGES continuous\", \"FGES discrete\" and \"GFCI continuous\". They also share a common JSON structure as of their input, for example:\n\n\n\n\n\n\n\n\nInput JSON Fields\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndatasetFileId\n\n\nThe dataset file ID, integer\n\n\n\n\n\n\npriorKnowledgeFileId\n\n\nThe optional prior knowledge file ID, integer\n\n\n\n\n\n\ndataValidation\n\n\nAlgorithm specific input data validation flags, JSON object\n\n\n\n\n\n\nalgorithmParameters\n\n\nAlgorithm specific parameters, JSON object\n\n\n\n\n\n\njvmOptions\n\n\nAdvanced Options For Java Virtual Machine (JVM), JSON object. Currently only support \nmaxHeapSize\n (Gigabyte, max value is 100)\n\n\n\n\n\n\n\n\nBelow are the data validation flags and parameters that you can use for each algorithm.\n\n\nFGES continuous\n \n\n\nData validation:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nskipNonzeroVariance\n\n\nSkip check for zero variance variables\n\n\nfalse\n\n\n\n\n\n\nskipUniqueVarName\n\n\nSkip check for unique variable names\n\n\nfalse\n\n\n\n\n\n\n\n\nAlgorithm parameters:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nfaithfulnessAssumed\n\n\nYes if (one edge) faithfulness should be assumed\n\n\ntrue\n\n\n\n\n\n\nmaxDegree\n\n\nThe maximum degree of the output graph, at least -1\n\n\n-1\n\n\n\n\n\n\npenaltyDiscount\n\n\nPenalty discount\n\n\n4.0\n\n\n\n\n\n\nverbose\n\n\nPrint additional information\n\n\ntrue\n\n\n\n\n\n\n\n\nFGES discrete\n \n\n\nData validation:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nskipUniqueVarName\n\n\nSkip check for unique variable names\n\n\nfalse\n\n\n\n\n\n\nskipCategoryLimit\n\n\nSkip 'limit number of categories' check\n\n\nfalse\n\n\n\n\n\n\n\n\nAlgorithm parameters:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nstructurePrior\n\n\nStructure prior\n\n\n1.0\n\n\n\n\n\n\nsamplePrior\n\n\nSample prior\n\n\n1.0\n\n\n\n\n\n\nfaithfulnessAssumed\n\n\nYes if (one edge) faithfulness should be assumed\n\n\ntrue\n\n\n\n\n\n\nmaxDegree\n\n\nThe maximum degree of the output graph, at least -1\n\n\n-1\n\n\n\n\n\n\nverbose\n\n\nPrint additional information\n\n\ntrue\n\n\n\n\n\n\n\n\nGFCI continuous\n \n\n\nData validation:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nskipNonzeroVariance\n\n\nSkip check for zero variance variables\n\n\nfalse\n\n\n\n\n\n\nskipUniqueVarName\n\n\nSkip check for unique variable names\n\n\nfalse\n\n\n\n\n\n\n\n\nAlgorithm parameters:\n\n\n\n\n\n\n\n\nParameters\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nalpha\n\n\nSearch depth. Integer value\n\n\n1.0\n\n\n\n\n\n\npenaltyDiscount\n\n\nPenalty discount\n\n\n4.0\n\n\n\n\n\n\nmaxDegree\n\n\nThe maximum degree of the output graph, at least -1\n\n\n-1\n\n\n\n\n\n\nfaithfulnessAssumed\n\n\nYes if (one edge) faithfulness should be assumed\n\n\ntrue\n\n\n\n\n\n\nverbose\n\n\nPrint additional information\n\n\ntrue\n\n\n\n\n\n\n\n\nAdd a new job to run the desired algorithm on a given data file\n\n\nThis is a POST request and the algorithm details and data file id will need to be specified in the POST body as a JSON when you make the request.\n\n\nAPI Endpoint URI pattern:\n\n\nPOST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/jobs/FGESc\n\n\n\n\nGenerated HTTP request code example:\n\n\nPOST /ccd-api/22/jobs/FGESc HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n{\n    \ndatasetFileId\n: 8,\n    \npriorKnowledgeFileId\n: 9,\n    \ndataValidation\n: {\n      \nskipNonzeroVariance\n: true,\n      \nskipUniqueVarName\n: true\n    },\n    \nalgorithmParameters\n: {\n      \npenaltyDiscount\n: 5.0,\n      \nmaxDegree\n: 100\n    },\n    \njvmOptions\n: {\n      \nmaxHeapSize\n: 100\n    }\n}\n\n\n\n\nIn this example, we are running the \"FGES continuous\" algorithm on the file with ID 8. And this call will return the job info with a 201 Created response status code.\n\n\n{\n  \nid\n: 5,\n  \nalgorithmName\n: \nFGESc\n,\n  \nstatus\n: 0,\n  \naddedTime\n: 1472742564355,\n  \nresultFileName\n: \nFGESc_data_small.txt_1472742564353.txt\n,\n  \nerrorResultFileName\n: \nerror_FGESc_data_small.txt_1472742564353.txt\n\n}\n\n\n\n\nFrom this response we can tell that the job ID is 5, and the result file name will be \nFGESc_data_small.txt_1472742564353.txt\n if everything goes well. If something is wrong an error result file with name \nerror_FGEsc_data_small.txt_1472742564353.txt\n will be created.\n\n\nWhen you need to run \"FGES discrete\", just send the request to a different endpont URI:\n\n\nAPI Endpoint URI pattern:\n\n\nPOST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/jobs/FGESd\n\n\n\n\nGenerated HTTP request code example:\n\n\nPOST /ccd-api/22/jobs/FGESd HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n{\n    \ndatasetFileId\n: 10,\n    \npriorKnowledgeFileId\n: 12,\n    \ndataValidation\n: {\n      \nskipUniqueVarName\n: true,\n      \nskipCategoryLimit\n: true\n    },\n    \nalgorithmParameters\n: {\n      \nstructurePrior\n: 1.0,\n      \nsamplePrior\n: 1.0,\n      \nmaxDegree\n: 102\n    },\n    \njvmOptions\n: {\n      \nmaxHeapSize\n: 100\n    }\n}\n\n\n\n\nList all running jobs\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/jobs\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/jobs/ HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n\n\n\n\nThen you'll see the information of all jobs that are currently running:\n\n\n[\n  {\n    \nid\n: 32,\n    \nalgorithmName\n: \nFGESc\n,\n    \naddedTime\n: 1468436085000\n  },\n  {\n    \nid\n: 33,\n    \nalgorithmName\n: \nFGESd\n,\n    \naddedTime\n: 1468436087000\n  }\n]\n\n\n\n\nCheck the job status for a given job ID\n\n\nOnce the new job is submitted, it takes time and resources to run the algorithm on the server. During the waiting, you can check the status of a given job ID:\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/jobs/{id}\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/jobs/32 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nThis will either return \"Pending\" or \"Completed\".\n\n\nCancel a running job\n\n\nSometimes you may want to cancel a submitted job.\n\n\nAPI Endpoint URI pattern:\n\n\nDELETE https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/jobs/{id}\n\n\n\n\nGenerated HTTP request code example:\n\n\nDELETE /ccd-api/22/jobs/8 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nThis call will response either \"Job 8 has been canceled\" or \"Unable to cancel job 8\". It's not guranteed that the system can always cencal a job successfully.\n\n\n3. Result Management\n\n\nList all result files generated by the algorithm\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/results\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/results HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nThe response to this request will look like this:\n\n\n[\n  {\n    \nname\n: \nFGESc_sim_data_20vars_100cases.csv_1466171729046.txt\n,\n    \ncreationTime\n: 1466171732000,\n    \nlastModifiedTime\n: 1466171732000,\n    \nfileSize\n: 1660\n  },\n  {\n    \nname\n: \nFGESc_data_small.txt_1466172140585.txt\n,\n    \ncreationTime\n: 1466172145000,\n    \nlastModifiedTime\n: 1466172145000,\n    \nfileSize\n: 39559\n  }\n]\n\n\n\n\nDownload a speific result file generated by the algorithm based on file name\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/results/{result_file_name}\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/results/FGESc_data_small.txt_1466172140585.txt HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nOn success, you will get the result file back as text file content. If there's a typo in file name of the that file doesn't exist, you'll get either a JSON or XML message based on the \naccept\n header in your request:\n\n\nThe response to this request will look like this:\n\n\n{\n  \ntimestamp\n: 1467210996233,\n  \nstatus\n: 404,\n  \nerror\n: \nNot Found\n,\n  \nmessage\n: \nResource not found.\n,\n  \npath\n: \n/22/results/FGESc_data_small.txt_146172140585.txt\n\n}\n\n\n\n\nCompare algorithm result files\n\n\nSince we can list all the algorithm result files, based on the results, we can also choose multiple files and run a comparison. \n\n\nAPI Endpoint URI pattern:\n\n\nPOST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/results/compare\n\n\n\n\nThe request body is a JSON that contains an array of result files to be compared.\n\n\nGenerated HTTP request code example:\n\n\nPOST /ccd-api/22/results/compare HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n{\n  \nresultFiles\n: [\n    \nFGESc_sim_data_20vars_100cases.csv_1466171729046.txt\n,\n    \nFGESc_data_small.txt_1467305104859.txt\n\n  ]\n}\n\n\n\n\nWhen you specify multiple file names, use the \n!!\n as a delimiter. This request will generate a result comparison file with the following content (shortened version):\n\n\nFGESc_sim_data_20vars_100cases.csv_1466171729046.txt  FGESc_data_small.txt_1467305104859.txt\nEdges In All  Same End Point\nNR4A2,FOS 0 0\nX5,X17  0 0\nMMP11,ASB5  0 0\nX12,X8  0 0\nhsa_miR_654_3p,hsa_miR_337_3p 0 0\nRND1,FGA  0 0\nHHLA2,UBXN10  0 0\nHS6ST2,RND1 0 0\nSCRG1,hsa_miR_377 0 0\nCDH3,diag 0 0\nSERPINI2,FGG  0 0\nhsa_miR_451,hsa_miR_136_  0 0\n\n\n\n\nFrom this comparison, you can see if the two algorithm graphs have common edges and endpoints.\n\n\nList all the comparison files\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/results/comparisons\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/results/comparisons HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nThe response will show a list of comparison files:\n\n\n[\n  {\n    \nname\n: \nresult_comparison_1467385923407.txt\n,\n    \ncreationTime\n: 1467385923000,\n    \nlastModifiedTime\n: 1467385923000,\n    \nfileSize\n: 7505\n  },\n  {\n    \nname\n: \nresult_comparison_1467387034358.txt\n,\n    \ncreationTime\n: 1467387034000,\n    \nlastModifiedTime\n: 1467387034000,\n    \nfileSize\n: 7505\n  },\n  {\n    \nname\n: \nresult_comparison_1467388042261.txt\n,\n    \ncreationTime\n: 1467388042000,\n    \nlastModifiedTime\n: 1467388042000,\n    \nfileSize\n: 7533\n  }\n]\n\n\n\n\nDownload a speific comparison file based on file name\n\n\nAPI Endpoint URI pattern:\n\n\nGET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/results/comparisons/{comparison_file_name}\n\n\n\n\nGenerated HTTP request code example:\n\n\nGET /ccd-api/22/results/comparisons/result_comparison_1467388042261.txt HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n\n\n\nThen it returns the content of that comparison file (shorted version):\n\n\nFGESc_sim_data_20vars_100cases.csv_1466171729046.txt  FGESc_data_small.txt_1467305104859.txt\nEdges In All  Same End Point\nNR4A2,FOS 0 0\nX5,X17  0 0\nMMP11,ASB5  0 0\nX12,X8  0 0\nhsa_miR_654_3p,hsa_miR_337_3p 0 0\nRND1,FGA  0 0\nHHLA2,UBXN10  0 0\nHS6ST2,RND1 0 0\nSCRG1,hsa_miR_377 0 0\nCDH3,diag 0 0\nSERPINI2,FGG  0 0", 
            "title": "Causal REST API"
        }, 
        {
            "location": "/causal-rest-api/#causal-rest-api-v007", 
            "text": "This RESTful API is designed for causal web. And it implements the  JAX-RS  specifications using Jersey.  Table of Contents   Installation  Prerequisites  Dependencies  Configuration  Start the API Server  API Usage and Examples  Getting JSON Web Token(JWT)  1. Data Management  Upload small data file  Resumable data file upload  List all dataset files of a user  Get the detail information of a dataset file based on ID  Delete physical dataset file and all records from database for a given file ID  Summarize dataset file  List all prior knowledge files of a given user  Get the detail information of a prior knowledge file based on ID  Delete physical prior knowledge file and all records from database for a given file ID    2. Causal Discovery  List all the available causal discovery algorithms  Add a new job to run the desired algorithm on a given data file  List all running jobs  Check the job status for a given job ID  Cancel a running job    3. Result Management  List all result files generated by the algorithm  Download a speific result file generated by the algorithm based on file name  Compare algorithm result files  List all the comparison files  Download a speific comparison file based on file name", 
            "title": "Causal REST API v0.0.7"
        }, 
        {
            "location": "/causal-rest-api/#installation", 
            "text": "The following installation instructions are supposed to be used by the server admin who deploys this API server. API users can skip this section and just start reading from the  API Usage and Examples  section.", 
            "title": "Installation"
        }, 
        {
            "location": "/causal-rest-api/#prerequisites", 
            "text": "You must have the following installed to build/install Causal REST API:   Oracle Java SE Development Kit 8  Maven 3.x", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/causal-rest-api/#dependencies", 
            "text": "If you want to run this API server and expose the API to your users, you'll first need to have the  Causal Web Application  installed and running. Your API users will use this web app to create their user accounts before they can consume the API.   Note: currently new users can also be created using Auth0 login option, but the API doesn't work for these users.  In order to build the API server, you'll need the released version of  ccd-commons-0.3.1  by going to the repo and checkout this specific release version:  git clone https://github.com/bd2kccd/ccd-commons.git\ncd ccd-commons\ngit checkout tags/v0.3.1\nmvn clean install  You'll also need to download released  ccd-db-0.6.3 :  git clone https://github.com/bd2kccd/ccd-db.git\ncd ccd-db\ngit checkout tags/v0.6.3\nmvn clean install  Then you can go get and install  causal-rest-api :  git clone https://github.com/bd2kccd/causal-rest-api.git\ncd causal-rest-api\nmvn clean package", 
            "title": "Dependencies"
        }, 
        {
            "location": "/causal-rest-api/#configuration", 
            "text": "There are 4 configuration files to configure located at  causal-rest-api/src/main/resources :\n-  application-hsqldb.properties : HSQLDB database configurations (for testing only).\n-  application-mysql.properties : MySQL database configurations\n-  application-slurm.properties : Slurm setting for HPC\n-  application.properties : Spring Boot application settings\n-  causal.properties : Data file directory path and folder settings  Befor editing the  causal.properties  file, you need to create a workspace for the application to work in. Create a directory called workspace, for an example  /home/zhy19/ccd/workspace . Inside the workspace directory, create another folder called  lib . Then build the jar file of Tetred using the  latest development branch . After that, copy the jar file to the  lib  folder created earlier.", 
            "title": "Configuration"
        }, 
        {
            "location": "/causal-rest-api/#start-the-api-server", 
            "text": "Once you have all the settings configured, go to  causal-rest-api/target  and you will find the jar file named  causal-rest-api.jar . Then simply run   java -jar causal-rest-api.jar", 
            "title": "Start the API Server"
        }, 
        {
            "location": "/causal-rest-api/#api-usage-and-examples", 
            "text": "In the following sections, we'll demonstrate the API usage with examples using the API server that is running on Pittsburgh Super Computing. The API base URI is https://ccd2.vm.bridges.psc.edu/ccd-api.  This API requires user to be authenticated. Before using this API, the user will need to go to  Causal-Web App  and create an account.", 
            "title": "API Usage and Examples"
        }, 
        {
            "location": "/causal-rest-api/#getting-json-web-tokenjwt", 
            "text": "After registration in Causal Web App, the email and password can be used to authenticate against the Causal REST API to get the access token (we use JWT) via  HTTP Basic Auth .   API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/jwt  In basic auth, the user provides the username and password, which the HTTP client concatenates (username + \":\" + password), and base64 encodes it. This encoded string is then sent using a  Authorization  header with the \"Basic\" schema. For instance user email  demo@pitt.edu  whose password is  123 .  POST /ccd-api/jwt HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Basic ZGVtb0BwaXR0LmVkdToxMjM=  Once the request is processed successfully, the user ID together with a JWT will be returned in the response for further API queries.  {\n   userId : 22,\n   jwt :  eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA0Mjg1OTcsImlhdCI6MTQ3NTg0NjgyODU5N30.FcE7aEpg0u2c-gUVugIjJkzjhlDu5qav_XHtgLu3c6E ,\n   issuedTime : 1475846828597,\n   lifetime : 3600,\n   expireTime : 1475850428597\n}  We'll need to use this  userId  in the URI path of all subsequent requests. And this  jwt  expires in 3600 seconds(1 hour), so the API consumer will need to request for another JWT otherwise the API query to other API endpoints will be denied. And this JWT will need to be sent via the HTTP  Authorization  header as well, but using the  Bearer  schema.  Note: querying the JWT endpoint again before the current JWT expires will generate a new JWT, which makes the old JWT expired automatically. And this newly generated JWT will be valid in another hour unless there's another new JWT being queried.  Since this API is developed with Jersey, which supports  WADL . So you can view the generated WADL by going to  https://ccd2.vm.bridges.psc.edu/ccd-api/application.wadl?detail=true  and see all resource available in the application. Accessing to this endpoint doesn't require authentication.  Basically, all the API usage examples are grouped into three categories:    Data Management  Causal Discovery  Result Management   And all the following examples will be issued by user  22  whose password is  123 .", 
            "title": "Getting JSON Web Token(JWT)"
        }, 
        {
            "location": "/causal-rest-api/#1-data-management", 
            "text": "", 
            "title": "1. Data Management"
        }, 
        {
            "location": "/causal-rest-api/#upload-small-data-file", 
            "text": "At this point, you can upload two types of data files: tabular dataset file(either tab delimited or comma delimited) and prior knowledge file.  API Endpoint URI pattern:  POST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/dataset/upload  This is a multipart file upload via an HTML form, and the client is required to use  name=\"file\"  to name their file upload field in their form.  Generated HTTP request code example:  POST /ccd-api/22/dataset/upload HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW\n\n----WebKitFormBoundary7MA4YWxkTrZu0gW\nContent-Disposition: form-data; name= file ; filename= \nContent-Type: \n\n\n----WebKitFormBoundary7MA4YWxkTrZu0gW  If the Authorization header is not provided, the response will look like this:  {\n   timestamp : 1465414501443,\n   status : 401,\n   error :  Unauthorized ,\n   message :  User credentials are required. ,\n   path :  /22/dataset/upload \n}  This POST request will upload the dataset file to the target server location and add corresponding records into database. And the response will contain the following pieces:  {\n     id : 6,\n     name :  Lung-tetrad_hv.txt ,\n     creationTime : 1466622267000,\n     lastModifiedTime : 1466622267000,\n     fileSize : 3309465,\n     md5checkSum :  b1db7511ee293d297e3055d9a7b46c5e ,\n     fileSummary : {\n       variableType : null,\n       fileDelimiter : null,\n       numOfRows : null,\n       numOfColumns : null\n    }\n  }  The prior knowledge file upload uses a similar API endpoint:  POST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge/upload  Due to there's no need to summarize a prior knowledge file, the response of a successful prior knowledge file upload will look like:  {\n     id : 6,\n     name :  Lung-tetrad_hv.txt ,\n     creationTime : 1466622267000,\n     lastModifiedTime : 1466622267000,\n     fileSize : 3309465,\n     md5checkSum :  ugdb7511rt293d29ke3055d9a7b46c9k \n  }", 
            "title": "Upload small data file"
        }, 
        {
            "location": "/causal-rest-api/#resumable-data-file-upload", 
            "text": "In addition to the regular file upload described in Example 6, we also provide the option of stable and resumable large file upload. It requires the client side to have a resumable upload implementation. We currently support client integrated with  Resumable.js , whihc provides multiple simultaneous, stable \nand resumable uploads via the HTML5 File API. You can also create your own client as long as al the following parameters are set correctly.  API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/chunkupload\n\nPOST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/chunkupload  In this example, the data file is splited into 3 chunks. The upload of each chunk consists of a GET request and a POST request. To handle the state of upload chunks, a number of extra parameters are sent along with all requests:   resumableChunkNumber : The index of the chunk in the current upload. First chunk is  1  (no base-0 counting here).  resumableChunkSize : The general chunk size. Using this value and  resumableTotalSize  you can calculate the total number of chunks. Please note that the size of the data received in the HTTP might be lower than  resumableChunkSize  of this for the last chunk for a file.  resumableCurrentChunkSize : The size of the current resumable chuck.  resumableTotalSize : The total file size.  resumableType : The file type of the resumable chuck, e.e., \"text/plain\".  resumableIdentifier : A unique identifier for the file contained in the request.  resumableFilename : The original file name (since a bug in Firefox results in the file name not being transmitted in chunk multipart posts).  resumableRelativePath : The file's relative path when selecting a directory (defaults to file name in all browsers except Chrome).  resumableTotalChunks : The total number of chunks.     Generated HTTP request code example:  GET /ccd-api/22/chunkupload?resumableChunkNumber=2 resumableChunkSize=1048576 resumableCurrentChunkSize=1048576 resumableTotalSize=3309465 resumableType=text%2Fplain resumableIdentifier=3309465-large-datatxt resumableFilename=large-data.txt resumableRelativePath=large-data.txt resumableTotalChunks=3 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  This GET request checks if the data chunk is already on the server side. If the target file chunk is not found on the server, the client will issue a POST request to upload the actual data.  Generated HTTP request code example:  POST /ccd-api/22/chunkupload HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundaryMFjgApg56XGyeTnZ\n\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableChunkNumber \n\n2\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableChunkSize \n\n1048576\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableCurrentChunkSize \n\n1048576\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableTotalSize \n\n3309465\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableType \n\ntext/plain\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableIdentifier \n\n3309465-large-datatxt\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableFilename \n\nlarge-data.txt\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableRelativePath \n\nlarge-data.txt\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= resumableTotalChunks \n\n3\n------WebKitFormBoundaryMFjgApg56XGyeTnZ\nContent-Disposition: form-data; name= file ; filename= blob \nContent-Type: application/octet-stream\n\n\n------WebKitFormBoundaryMFjgApg56XGyeTnZ--  Each chunk upload POST will get a 200 status code from response if everything works fine.  And finally the md5checkSum string of the reassemabled file will be returned once the whole file has been uploaded successfully. In this example, the POST request that uploads the third chunk will response this:  b1db7511ee293d297e3055d9a7b46c5e", 
            "title": "Resumable data file upload"
        }, 
        {
            "location": "/causal-rest-api/#list-all-dataset-files-of-a-user", 
            "text": "API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/dataset  Generated HTTP request code example:  GET /ccd-api/22/dataset HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nAccept: application/json  A  JSON  formatted list of all the input dataset files that are associated with user  22  will be returned.  [\n  {\n     id : 8,\n     name :  data_small.txt ,\n     creationTime : 1467132449000,\n     lastModifiedTime : 1467132449000,\n     fileSize : 278428,\n     md5checkSum :  ed5f27a2cf94fe3735a5d9ed9191c382 ,\n     fileSummary : {\n       variableType :  continuous ,\n       fileDelimiter :  tab ,\n       numOfRows : 302,\n       numOfColumns : 123\n    }\n  },\n  {\n     id : 10,\n     name :  large-data.txt ,\n     creationTime : 1467134048000,\n     lastModifiedTime : 1467134048000,\n     fileSize : 3309465,\n     md5checkSum :  b1db7511ee293d297e3055d9a7b46c5e ,\n     fileSummary : {\n       variableType : null,\n       fileDelimiter : null,\n       numOfRows : null,\n       numOfColumns : null\n    }\n  },\n  {\n     id : 11,\n     name :  Lung-tetrad_hv (copy).txt ,\n     creationTime : 1467140415000,\n     lastModifiedTime : 1467140415000,\n     fileSize : 3309465,\n     md5checkSum :  b1db7511ee293d297e3055d9a7b46c5e ,\n     fileSummary : {\n       variableType :  continuous ,\n       fileDelimiter :  tab ,\n       numOfRows : 302,\n       numOfColumns : 608\n    }\n  }\n]  You can also specify the response format as XML in your request  Generated HTTP request code example:  GET /ccd-api/22/dataset HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nAccept: application/xml  And the response will look like this:  ?xml version= 1.0  encoding= UTF-8  standalone= yes ?  datasetFileDTOes \n     datasetFile \n         id 8 /id \n         name data_small.txt /name \n         creationTime 2016-06-28T12:47:29-04:00 /creationTime \n         lastModifiedTime 2016-06-28T12:47:29-04:00 /lastModifiedTime \n         fileSize 278428 /fileSize \n         md5checkSum ed5f27a2cf94fe3735a5d9ed9191c382 /md5checkSum \n         fileSummary \n             fileDelimiter tab /fileDelimiter \n             numOfColumns 123 /numOfColumns \n             numOfRows 302 /numOfRows \n             variableType continuous /variableType \n         /fileSummary \n     /datasetFile \n     datasetFile \n         id 10 /id \n         name large-data.txt /name \n         creationTime 2016-06-28T13:14:08-04:00 /creationTime \n         lastModifiedTime 2016-06-28T13:14:08-04:00 /lastModifiedTime \n         fileSize 3309465 /fileSize \n         md5checkSum b1db7511ee293d297e3055d9a7b46c5e /md5checkSum \n         fileSummary \n             variableType xmlns:xsi= http://www.w3.org/2001/XMLSchema-instance  xsi:nil= true / \n             fileDelimiter xmlns:xsi= http://www.w3.org/2001/XMLSchema-instance  xsi:nil= true / \n             numOfRows xmlns:xsi= http://www.w3.org/2001/XMLSchema-instance  xsi:nil= true / \n             numOfColumns xmlns:xsi= http://www.w3.org/2001/XMLSchema-instance  xsi:nil= true / \n         /fileSummary \n     /datasetFile \n     datasetFile \n         id 11 /id \n         name Lung-tetrad_hv (copy).txt /name \n         creationTime 2016-06-28T15:00:15-04:00 /creationTime \n         lastModifiedTime 2016-06-28T15:00:15-04:00 /lastModifiedTime \n         fileSize 3309465 /fileSize \n         md5checkSum b1db7511ee293d297e3055d9a7b46c5e /md5checkSum \n         fileSummary \n             fileDelimiter tab /fileDelimiter \n             numOfColumns 608 /numOfColumns \n             numOfRows 302 /numOfRows \n             variableType continuous /variableType \n         /fileSummary \n     /datasetFile  /datasetFileDTOes   Form the above output, we can also tell that data file with ID 10 doesn't have all the  fileSummary  field values set, we'll cover this in the dataset summarization section.", 
            "title": "List all dataset files of a user"
        }, 
        {
            "location": "/causal-rest-api/#get-the-detail-information-of-a-dataset-file-based-on-id", 
            "text": "API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/dataset/{id}  Generated HTTP request code example:  GET /ccd-api/22/dataset/8 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  And the resulting response looks like this:  {\n   id : 8,\n   name :  data_small.txt ,\n   creationTime : 1467132449000,\n   lastModifiedTime : 1467132449000,\n   fileSize : 278428,\n   fileSummary : {\n     md5checkSum :  ed5f27a2cf94fe3735a5d9ed9191c382 ,\n     variableType :  continuous ,\n     fileDelimiter :  tab ,\n     numOfRows : 302,\n     numOfColumns : 123\n  }\n}", 
            "title": "Get the detail information of a dataset file based on ID"
        }, 
        {
            "location": "/causal-rest-api/#delete-physical-dataset-file-and-all-records-from-database-for-a-given-file-id", 
            "text": "API Endpoint URI pattern:  DELETE https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/dataset/{id}  Generated HTTP request code example:  DELETE /ccd-api/22/dataset/8 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  And this will result a HTTP 204 No Content status in response on success, which means the server successfully processed the deletion request but there's no content to response.", 
            "title": "Delete physical dataset file and all records from database for a given file ID"
        }, 
        {
            "location": "/causal-rest-api/#summarize-dataset-file", 
            "text": "So from the first example we can tell that file with ID 10 doesn't have  variableType ,  fileDelimiter ,  numOfRows , and  numOfColumns  specified under  fileSummary . Among these attributes, variableType and fileDelimiter` are the ones that users will need to provide during this summarization process.  Before we can go ahead to run the desired algorithm with the newly uploaded data file, we'll need to summarize the data by specifing the variable type and file delimiter.     Required Fields  Description      id  The data file ID    variableType  discrete or continuous    fileDelimiter  tab or comma     API Endpoint URI pattern:  POST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/dataset/summarize  Generated HTTP request code example:  POST /ccd-api/22/dataset/summarize HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n{\n     id : 1,\n     variableType :  continuous ,\n     fileDelimiter :  comma \n}  This POST request will summarize the dataset file and generate a response (JSON or XML) like below:  {\n   id : 10,\n   name :  large-data.txt ,\n   creationTime : 1467134048000,\n   lastModifiedTime : 1467134048000,\n   fileSize : 3309465,\n   md5checkSum :  b1db7511ee293d297e3055d9a7b46c5e ,\n   fileSummary : {\n     variableType :  continuous ,\n     fileDelimiter :  tab ,\n     numOfRows : 302,\n     numOfColumns : 608\n  }\n}", 
            "title": "Summarize dataset file"
        }, 
        {
            "location": "/causal-rest-api/#list-all-prior-knowledge-files-of-a-given-user", 
            "text": "API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge  Generated HTTP request code example:  GET /ccd-api/22/priorknowledge HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nAccept: application/json  A  JSON  formatted list of all the input dataset files that are associated with user  22  will be returned.  [\n  {\n     id : 9,\n     name :  data_small.prior ,\n     creationTime : 1467132449000,\n     lastModifiedTime : 1467132449000,\n     fileSize : 278428,\n     md5checkSum :  ed5f27a2cf94fe3735a5d9ed9191c382 \n  },\n  {\n     id : 12,\n     name :  large-data.prior ,\n     creationTime : 1467134048000,\n     lastModifiedTime : 1467134048000,\n     fileSize : 3309465,\n     md5checkSum :  b1db7511ee293d297e3055d9a7b46c5e \n  }\n]", 
            "title": "List all prior knowledge files of a given user"
        }, 
        {
            "location": "/causal-rest-api/#get-the-detail-information-of-a-prior-knowledge-file-based-on-id", 
            "text": "API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge/{id}  Generated HTTP request code example:  GET /ccd-api/22/priorknowledge/9 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  And the resulting response looks like this:  {\n   id : 9,\n   name :  data_small.prior ,\n   creationTime : 1467132449000,\n   lastModifiedTime : 1467132449000,\n   fileSize : 278428,\n   md5checkSum :  ed5f27a2cf94fe3735a5d9ed9191c382 \n}", 
            "title": "Get the detail information of a prior knowledge file based on ID"
        }, 
        {
            "location": "/causal-rest-api/#delete-physical-prior-knowledge-file-and-all-records-from-database-for-a-given-file-id", 
            "text": "API Endpoint URI pattern:  DELETE https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/priorknowledge/{id}  Generated HTTP request code example:  DELETE /ccd-api/22/priorknowledge/9 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  And this will result a HTTP 204 No Content status in response on success, which means the server successfully processed the deletion request but there's no content to response.", 
            "title": "Delete physical prior knowledge file and all records from database for a given file ID"
        }, 
        {
            "location": "/causal-rest-api/#2-causal-discovery", 
            "text": "Once the data file is uploaded and summaried, you can start running a Causal Discovery Algorithm on the uploaded data file.", 
            "title": "2. Causal Discovery"
        }, 
        {
            "location": "/causal-rest-api/#list-all-the-available-causal-discovery-algorithms", 
            "text": "API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/algorithms  Generated HTTP request code example:  GET /ccd-api/22/algorithms HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  Currently we support \"FGES continuous\" and \"FGES discrete\".  [\n  {\n     id : 1,\n     name :  FGESc ,\n     description :  FGES continuous \n  },\n  {\n     id : 2,\n     name :  FGESd ,\n     description :  FGES discrete \n  },\n  {\n     id : 3,\n     name :  GFCIc ,\n     description :  GFCI continuous \n  }\n]  Currently we support \"FGES continuous\", \"FGES discrete\" and \"GFCI continuous\". They also share a common JSON structure as of their input, for example:     Input JSON Fields  Description      datasetFileId  The dataset file ID, integer    priorKnowledgeFileId  The optional prior knowledge file ID, integer    dataValidation  Algorithm specific input data validation flags, JSON object    algorithmParameters  Algorithm specific parameters, JSON object    jvmOptions  Advanced Options For Java Virtual Machine (JVM), JSON object. Currently only support  maxHeapSize  (Gigabyte, max value is 100)     Below are the data validation flags and parameters that you can use for each algorithm.  FGES continuous    Data validation:     Parameters  Description  Default Value      skipNonzeroVariance  Skip check for zero variance variables  false    skipUniqueVarName  Skip check for unique variable names  false     Algorithm parameters:     Parameters  Description  Default Value      faithfulnessAssumed  Yes if (one edge) faithfulness should be assumed  true    maxDegree  The maximum degree of the output graph, at least -1  -1    penaltyDiscount  Penalty discount  4.0    verbose  Print additional information  true     FGES discrete    Data validation:     Parameters  Description  Default Value      skipUniqueVarName  Skip check for unique variable names  false    skipCategoryLimit  Skip 'limit number of categories' check  false     Algorithm parameters:     Parameters  Description  Default Value      structurePrior  Structure prior  1.0    samplePrior  Sample prior  1.0    faithfulnessAssumed  Yes if (one edge) faithfulness should be assumed  true    maxDegree  The maximum degree of the output graph, at least -1  -1    verbose  Print additional information  true     GFCI continuous    Data validation:     Parameters  Description  Default Value      skipNonzeroVariance  Skip check for zero variance variables  false    skipUniqueVarName  Skip check for unique variable names  false     Algorithm parameters:     Parameters  Description  Default Value      alpha  Search depth. Integer value  1.0    penaltyDiscount  Penalty discount  4.0    maxDegree  The maximum degree of the output graph, at least -1  -1    faithfulnessAssumed  Yes if (one edge) faithfulness should be assumed  true    verbose  Print additional information  true", 
            "title": "List all the available causal discovery algorithms"
        }, 
        {
            "location": "/causal-rest-api/#add-a-new-job-to-run-the-desired-algorithm-on-a-given-data-file", 
            "text": "This is a POST request and the algorithm details and data file id will need to be specified in the POST body as a JSON when you make the request.  API Endpoint URI pattern:  POST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/jobs/FGESc  Generated HTTP request code example:  POST /ccd-api/22/jobs/FGESc HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n{\n     datasetFileId : 8,\n     priorKnowledgeFileId : 9,\n     dataValidation : {\n       skipNonzeroVariance : true,\n       skipUniqueVarName : true\n    },\n     algorithmParameters : {\n       penaltyDiscount : 5.0,\n       maxDegree : 100\n    },\n     jvmOptions : {\n       maxHeapSize : 100\n    }\n}  In this example, we are running the \"FGES continuous\" algorithm on the file with ID 8. And this call will return the job info with a 201 Created response status code.  {\n   id : 5,\n   algorithmName :  FGESc ,\n   status : 0,\n   addedTime : 1472742564355,\n   resultFileName :  FGESc_data_small.txt_1472742564353.txt ,\n   errorResultFileName :  error_FGESc_data_small.txt_1472742564353.txt \n}  From this response we can tell that the job ID is 5, and the result file name will be  FGESc_data_small.txt_1472742564353.txt  if everything goes well. If something is wrong an error result file with name  error_FGEsc_data_small.txt_1472742564353.txt  will be created.  When you need to run \"FGES discrete\", just send the request to a different endpont URI:  API Endpoint URI pattern:  POST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/jobs/FGESd  Generated HTTP request code example:  POST /ccd-api/22/jobs/FGESd HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json\n\n{\n     datasetFileId : 10,\n     priorKnowledgeFileId : 12,\n     dataValidation : {\n       skipUniqueVarName : true,\n       skipCategoryLimit : true\n    },\n     algorithmParameters : {\n       structurePrior : 1.0,\n       samplePrior : 1.0,\n       maxDegree : 102\n    },\n     jvmOptions : {\n       maxHeapSize : 100\n    }\n}", 
            "title": "Add a new job to run the desired algorithm on a given data file"
        }, 
        {
            "location": "/causal-rest-api/#list-all-running-jobs", 
            "text": "API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/jobs  Generated HTTP request code example:  GET /ccd-api/22/jobs/ HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\nContent-Type: application/json  Then you'll see the information of all jobs that are currently running:  [\n  {\n     id : 32,\n     algorithmName :  FGESc ,\n     addedTime : 1468436085000\n  },\n  {\n     id : 33,\n     algorithmName :  FGESd ,\n     addedTime : 1468436087000\n  }\n]", 
            "title": "List all running jobs"
        }, 
        {
            "location": "/causal-rest-api/#check-the-job-status-for-a-given-job-id", 
            "text": "Once the new job is submitted, it takes time and resources to run the algorithm on the server. During the waiting, you can check the status of a given job ID:  API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/jobs/{id}  Generated HTTP request code example:  GET /ccd-api/22/jobs/32 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  This will either return \"Pending\" or \"Completed\".", 
            "title": "Check the job status for a given job ID"
        }, 
        {
            "location": "/causal-rest-api/#cancel-a-running-job", 
            "text": "Sometimes you may want to cancel a submitted job.  API Endpoint URI pattern:  DELETE https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/jobs/{id}  Generated HTTP request code example:  DELETE /ccd-api/22/jobs/8 HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  This call will response either \"Job 8 has been canceled\" or \"Unable to cancel job 8\". It's not guranteed that the system can always cencal a job successfully.", 
            "title": "Cancel a running job"
        }, 
        {
            "location": "/causal-rest-api/#3-result-management", 
            "text": "", 
            "title": "3. Result Management"
        }, 
        {
            "location": "/causal-rest-api/#list-all-result-files-generated-by-the-algorithm", 
            "text": "API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/results  Generated HTTP request code example:  GET /ccd-api/22/results HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  The response to this request will look like this:  [\n  {\n     name :  FGESc_sim_data_20vars_100cases.csv_1466171729046.txt ,\n     creationTime : 1466171732000,\n     lastModifiedTime : 1466171732000,\n     fileSize : 1660\n  },\n  {\n     name :  FGESc_data_small.txt_1466172140585.txt ,\n     creationTime : 1466172145000,\n     lastModifiedTime : 1466172145000,\n     fileSize : 39559\n  }\n]", 
            "title": "List all result files generated by the algorithm"
        }, 
        {
            "location": "/causal-rest-api/#download-a-speific-result-file-generated-by-the-algorithm-based-on-file-name", 
            "text": "API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/results/{result_file_name}  Generated HTTP request code example:  GET /ccd-api/22/results/FGESc_data_small.txt_1466172140585.txt HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  On success, you will get the result file back as text file content. If there's a typo in file name of the that file doesn't exist, you'll get either a JSON or XML message based on the  accept  header in your request:  The response to this request will look like this:  {\n   timestamp : 1467210996233,\n   status : 404,\n   error :  Not Found ,\n   message :  Resource not found. ,\n   path :  /22/results/FGESc_data_small.txt_146172140585.txt \n}", 
            "title": "Download a speific result file generated by the algorithm based on file name"
        }, 
        {
            "location": "/causal-rest-api/#compare-algorithm-result-files", 
            "text": "Since we can list all the algorithm result files, based on the results, we can also choose multiple files and run a comparison.   API Endpoint URI pattern:  POST https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/results/compare  The request body is a JSON that contains an array of result files to be compared.  Generated HTTP request code example:  POST /ccd-api/22/results/compare HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY\n\n{\n   resultFiles : [\n     FGESc_sim_data_20vars_100cases.csv_1466171729046.txt ,\n     FGESc_data_small.txt_1467305104859.txt \n  ]\n}  When you specify multiple file names, use the  !!  as a delimiter. This request will generate a result comparison file with the following content (shortened version):  FGESc_sim_data_20vars_100cases.csv_1466171729046.txt  FGESc_data_small.txt_1467305104859.txt\nEdges In All  Same End Point\nNR4A2,FOS 0 0\nX5,X17  0 0\nMMP11,ASB5  0 0\nX12,X8  0 0\nhsa_miR_654_3p,hsa_miR_337_3p 0 0\nRND1,FGA  0 0\nHHLA2,UBXN10  0 0\nHS6ST2,RND1 0 0\nSCRG1,hsa_miR_377 0 0\nCDH3,diag 0 0\nSERPINI2,FGG  0 0\nhsa_miR_451,hsa_miR_136_  0 0  From this comparison, you can see if the two algorithm graphs have common edges and endpoints.", 
            "title": "Compare algorithm result files"
        }, 
        {
            "location": "/causal-rest-api/#list-all-the-comparison-files", 
            "text": "API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/results/comparisons  Generated HTTP request code example:  GET /ccd-api/22/results/comparisons HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  The response will show a list of comparison files:  [\n  {\n     name :  result_comparison_1467385923407.txt ,\n     creationTime : 1467385923000,\n     lastModifiedTime : 1467385923000,\n     fileSize : 7505\n  },\n  {\n     name :  result_comparison_1467387034358.txt ,\n     creationTime : 1467387034000,\n     lastModifiedTime : 1467387034000,\n     fileSize : 7505\n  },\n  {\n     name :  result_comparison_1467388042261.txt ,\n     creationTime : 1467388042000,\n     lastModifiedTime : 1467388042000,\n     fileSize : 7533\n  }\n]", 
            "title": "List all the comparison files"
        }, 
        {
            "location": "/causal-rest-api/#download-a-speific-comparison-file-based-on-file-name", 
            "text": "API Endpoint URI pattern:  GET https://ccd2.vm.bridges.psc.edu/ccd-api/{userId}/results/comparisons/{comparison_file_name}  Generated HTTP request code example:  GET /ccd-api/22/results/comparisons/result_comparison_1467388042261.txt HTTP/1.1\nHost: ccd2.vm.bridges.psc.edu\nAuthorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2Nsb3VkLmNjZC5waXR0LmVkdS8iLCJuYW1lIjoiemh5MTkiLCJleHAiOjE0NzU4NTA2NzY4MDQsImlhdCI6MTQ3NTg0NzA3NjgwNH0.8azVEoNPfETczXb-vn7dfyDd98eRt7iiLBXehGpPGzY  Then it returns the content of that comparison file (shorted version):  FGESc_sim_data_20vars_100cases.csv_1466171729046.txt  FGESc_data_small.txt_1467305104859.txt\nEdges In All  Same End Point\nNR4A2,FOS 0 0\nX5,X17  0 0\nMMP11,ASB5  0 0\nX12,X8  0 0\nhsa_miR_654_3p,hsa_miR_337_3p 0 0\nRND1,FGA  0 0\nHHLA2,UBXN10  0 0\nHS6ST2,RND1 0 0\nSCRG1,hsa_miR_377 0 0\nCDH3,diag 0 0\nSERPINI2,FGG  0 0", 
            "title": "Download a speific comparison file based on file name"
        }, 
        {
            "location": "/py-causal/", 
            "text": "py-causal\n\n\nPython APIs\n for causal modeling algorithms developed by the University of Pittsburgh/Carnegie Mellon University \nCenter for Causal Discovery\n. \n\n\nThis code is distributed under the LGPL 2.1 license.\n\n\nRequirements:\n\n\nPython 2.7 (does not work with Python 3)\n\n\n\n\njavabridge\n=1.0.11\n\n\npandas\n\n\nnumpy \n\n\npydot\n\n\nGraphViz\n\n\nJDK 1.7+\n\n\n\n\nInstallation overview:\n\n\nWe have found two approaches to be useful:\n\n Direct python installation with pip, possibly including use of \nJupyter\n. This approach is likely best for users who have Python installed and are familiar with installing Python modules.\n\n Installation via \nAnaconda\n, which  installs Python and related utilities.\n\n\nDirections for both approaches are given below...\n\n\nInstallation with pip\n\n\nIf you do not have pip installed already, try \nthese instructions\n.\n\n\nOnce pip is installed, execute these commands\n\n\npip install numpy\npip install pandas\npip install javabridge\npip install pydot \npip install GraphViz\n\n\n\nNote: you also need to install the GraphViz engine by following \nthese instructions\n.\n\n\nWe have observed that on some OS X installations, pydot may provide the following response\n    Couldn't import dot_parser, loading of dot files will not be possible.\n\n\nIf you see this, try the following\n\n\n pip uninstall pydot\n pip install pyparsing==1.5.7\n pip install pydot\n\n\n\nThen, from within the py-causal directory, run the following command:\n\n\npython setup.py install\n\n\n\nAfter running this command, enter a python shell and attempt the follwing import\n    import pandas as pd\n    import pydot\n    from tetrad import search as s\n\n\nFinally, try to run the python \nexample\n\n\npython py-causal-fgs-continuous-example.py\n\n\n\nBe sure to run this from within the py-causal directory.\n\n\nThis program will create a file named tetrad.svg, which should be viewable in any SVG capable program. If you see a causal graph, everything is working correctly.\n\n\nRunning Jupyter/IPython\n\n\nWe have found \nJupyter\n notebooks to be helpful. (Those who have run IPython in the past should know that Jupyter is simply a new name for IPython). To add Jupyter to your completed python install, simply run\n\n\npip -U jupyter\njupyter notebook\n\n\n\nand then load one of the Jupyter notebooks found in this installation. \n\n\nAnaconda/Jupyter\n\n\nInstalling Python with Anaconda and Jupyter may be easier for some users:\n\n\n\n\nDownload and install Anaconda\n\n\nconda install python-javabridge\n\n\n\n\nFor OS X, this default install does not seem to work well. try the following instead:\n\n\nconda install --channel https://conda.anaconda.org/david_baddeley python-javabridge\n\n\n\nThen run the following to configure anacoda\n\n\nconda install pandas  \nconda install numpy\nconda install pydot\nconda install graphviz \nconda install -c https://conda.anaconda.org/chirayu pycausal \njupyter notebook\n\n\n\nand then load one of the Jupyter notebooks.\n\n\nDocker Image\n\n\nThe pre-installed py-causal Docker image is also available at \nDocker Hub", 
            "title": "Py-causal"
        }, 
        {
            "location": "/py-causal/#py-causal", 
            "text": "Python APIs  for causal modeling algorithms developed by the University of Pittsburgh/Carnegie Mellon University  Center for Causal Discovery .   This code is distributed under the LGPL 2.1 license.", 
            "title": "py-causal"
        }, 
        {
            "location": "/py-causal/#requirements", 
            "text": "Python 2.7 (does not work with Python 3)   javabridge =1.0.11  pandas  numpy   pydot  GraphViz  JDK 1.7+", 
            "title": "Requirements:"
        }, 
        {
            "location": "/py-causal/#installation-overview", 
            "text": "We have found two approaches to be useful:  Direct python installation with pip, possibly including use of  Jupyter . This approach is likely best for users who have Python installed and are familiar with installing Python modules.  Installation via  Anaconda , which  installs Python and related utilities.  Directions for both approaches are given below...", 
            "title": "Installation overview:"
        }, 
        {
            "location": "/py-causal/#installation-with-pip", 
            "text": "If you do not have pip installed already, try  these instructions .  Once pip is installed, execute these commands  pip install numpy\npip install pandas\npip install javabridge\npip install pydot \npip install GraphViz  Note: you also need to install the GraphViz engine by following  these instructions .  We have observed that on some OS X installations, pydot may provide the following response\n    Couldn't import dot_parser, loading of dot files will not be possible.  If you see this, try the following   pip uninstall pydot\n pip install pyparsing==1.5.7\n pip install pydot  Then, from within the py-causal directory, run the following command:  python setup.py install  After running this command, enter a python shell and attempt the follwing import\n    import pandas as pd\n    import pydot\n    from tetrad import search as s  Finally, try to run the python  example  python py-causal-fgs-continuous-example.py  Be sure to run this from within the py-causal directory.  This program will create a file named tetrad.svg, which should be viewable in any SVG capable program. If you see a causal graph, everything is working correctly.", 
            "title": "Installation with pip"
        }, 
        {
            "location": "/py-causal/#running-jupyteripython", 
            "text": "We have found  Jupyter  notebooks to be helpful. (Those who have run IPython in the past should know that Jupyter is simply a new name for IPython). To add Jupyter to your completed python install, simply run  pip -U jupyter\njupyter notebook  and then load one of the Jupyter notebooks found in this installation.", 
            "title": "Running Jupyter/IPython"
        }, 
        {
            "location": "/py-causal/#anacondajupyter", 
            "text": "Installing Python with Anaconda and Jupyter may be easier for some users:   Download and install Anaconda  conda install python-javabridge   For OS X, this default install does not seem to work well. try the following instead:  conda install --channel https://conda.anaconda.org/david_baddeley python-javabridge  Then run the following to configure anacoda  conda install pandas  \nconda install numpy\nconda install pydot\nconda install graphviz \nconda install -c https://conda.anaconda.org/chirayu pycausal \njupyter notebook  and then load one of the Jupyter notebooks.", 
            "title": "Anaconda/Jupyter"
        }, 
        {
            "location": "/py-causal/#docker-image", 
            "text": "The pre-installed py-causal Docker image is also available at  Docker Hub", 
            "title": "Docker Image"
        }, 
        {
            "location": "/r-causal/", 
            "text": "r-causal\n\n\nR Wrapper\n for Tetrad Library\n\n\nR Library Requirement\n\n\nR \n= 3.2.0, \n\nstringr\n,\n\nrJava\n, \n\ngraph\n, \n\nRBGL\n, \n\nRgraphviz\n\n\nInstallation\n\n\n\n\nInstall the R library requirements:\n\n\n\n\ninstall.packages(\nstringr\n)\ninstall.packages(\nrJava\n)\n## try http:// if https:// URLs are not supported\nsource(\nhttps://bioconductor.org/biocLite.R\n) \nbiocLite(\ngraph\n)\nbiocLite(\nRBGL\n)\nbiocLite(\nRgraphviz\n) # For plotting graph\n\n\n\n\n\n\nInstall r-causal from github:\n\n\n\n\nlibrary(devtools)\ninstall_github(\nbd2kccd/r-causal\n)\n\n\n\n\nExample\n\n\nContinuous Dataset\n\n\nlibrary(rcausal)\ndata(\ncharity\n)   #Load the charity dataset\n\n#Compute FGES search\nfgs \n- fgs(df = charity, penaltydiscount = 2, maxDegree = -1,  \nfaithfulnessAssumed = TRUE, numOfThreads = 2, verbose = TRUE)    \n\nfgs$parameters #Show the FGES's parameters\nfgs$datasets #Show the dataset\nfgs$nodes #Show the result's nodes\nfgs$edges #Show the result's edges\n\nlibrary(Rgraphviz)\nplot(fgs$graphNEL) #Plot the causal model\n\n\n\n\nDiscrete Dataset\n\n\nlibrary(rcausal)\ndata(\naudiology\n)    #Load the charity dataset\n#Compute FGES search\nfgs.discrete \n- fgs.discrete(df=audiology,structurePrior=1.0,samplePrior=1.0, \nmaxDegree = -1, faithfulnessAssumed = TRUE, numOfThreads = 2,verbose = TRUE)\nfgs.discrete$parameters #Show the FGES Discrete's parameters\nfgs.discrete$datasets #Show the dataset\nfgs.discrete$nodes #Show the result's nodes\nfgs.discrete$edges #Show the result's edges\nlibrary(Rgraphviz)\nplot(fgs.discrete$graphNEL) #Plot the causal model\n\n\n\n\nPrior Knowledge\n\n\nCreate PriorKnowledge Object\n\n\nforbid \n- list(c('TangibilityCondition','Impact')) # List of forbidden directed edges\nrequire \n- list(c('Sympathy','TangibilityCondition')) # List of required directed edges\nforbiddenWithin \n- c('TangibilityCondition','Imaginability')\nclass(forbiddenWithin) \n- 'forbiddenWithin' # Make this tier forbidden within\ntemporal \n- list(forbiddenWithin, c('Sympathy','AmountDonated'),c('Impact')) # List of temporal node tiers\nprior \n- priorKnowledge(forbiddirect = forbid, requiredirect = require, addtemporal = temporal)\nfgs \n- fgs(df = charity, penaltydiscount = 2, depth = -1, ignoreLinearDependence = TRUE, \nheuristicSpeedup = TRUE, numOfThreads = 2, verbose = TRUE, priorKnowledge = prior)\n\n\n\n\nLoad Knowledge File\n\n\n# knowledge file: audiology.prior\n# /knowledge\n# forbiddirect\n# class tymp\n# class age_gt_60\n# class notch_at_4k\n# \n# requiredirect\n# history_noise class\n#\n# addtemporal\n# 0* bser late_wave_poor tymp notch_at_4k o_ar_c ar_c airBoneGap air bone o_ar_u airBoneGap\n# 1 history_noise history_dizziness history_buzzing history_roaring history_recruitment history_fluctuating history_heredity history_nausea\n# 2 class\n\nprior \n- priorKnowledgeFromFile('audiology.prior')\nfgs.discrete \n- fgs.discrete(df=audiology,structurePrior=1.0,samplePrior=1.0, \ndepth = -1, heuristicSpeedup = TRUE, numOfThreads = 2,verbose = TRUE, priorKnowledge = prior)\n\n\n\n\nConvert Rgraphviz to igraph one\n\n\nlibrary(igraph)\nigraph \n- igraph.from.graphNEL(fgs.discrete$graphNEL)\nplot(igraph)\n\n\n\n\nUseful \nrJava\n Trouble-shooting Installation in Mac OS X Links\n\n\n\n\nhttp://stackoverflow.com/questions/26948777/how-can-i-make-rjava-use-the-newer-version-of-java-on-osx/32544358#32544358\n\n\nhttp://andrewgoldstone.com/blog/2015/02/03/rjava/", 
            "title": "R-causal"
        }, 
        {
            "location": "/r-causal/#r-causal", 
            "text": "R Wrapper  for Tetrad Library", 
            "title": "r-causal"
        }, 
        {
            "location": "/r-causal/#r-library-requirement", 
            "text": "R  = 3.2.0,  stringr , rJava ,  graph ,  RBGL ,  Rgraphviz", 
            "title": "R Library Requirement"
        }, 
        {
            "location": "/r-causal/#installation", 
            "text": "Install the R library requirements:   install.packages( stringr )\ninstall.packages( rJava )\n## try http:// if https:// URLs are not supported\nsource( https://bioconductor.org/biocLite.R ) \nbiocLite( graph )\nbiocLite( RBGL )\nbiocLite( Rgraphviz ) # For plotting graph   Install r-causal from github:   library(devtools)\ninstall_github( bd2kccd/r-causal )", 
            "title": "Installation"
        }, 
        {
            "location": "/r-causal/#example", 
            "text": "", 
            "title": "Example"
        }, 
        {
            "location": "/r-causal/#continuous-dataset", 
            "text": "library(rcausal)\ndata( charity )   #Load the charity dataset\n\n#Compute FGES search\nfgs  - fgs(df = charity, penaltydiscount = 2, maxDegree = -1,  \nfaithfulnessAssumed = TRUE, numOfThreads = 2, verbose = TRUE)    \n\nfgs$parameters #Show the FGES's parameters\nfgs$datasets #Show the dataset\nfgs$nodes #Show the result's nodes\nfgs$edges #Show the result's edges\n\nlibrary(Rgraphviz)\nplot(fgs$graphNEL) #Plot the causal model", 
            "title": "Continuous Dataset"
        }, 
        {
            "location": "/r-causal/#discrete-dataset", 
            "text": "library(rcausal)\ndata( audiology )    #Load the charity dataset\n#Compute FGES search\nfgs.discrete  - fgs.discrete(df=audiology,structurePrior=1.0,samplePrior=1.0, \nmaxDegree = -1, faithfulnessAssumed = TRUE, numOfThreads = 2,verbose = TRUE)\nfgs.discrete$parameters #Show the FGES Discrete's parameters\nfgs.discrete$datasets #Show the dataset\nfgs.discrete$nodes #Show the result's nodes\nfgs.discrete$edges #Show the result's edges\nlibrary(Rgraphviz)\nplot(fgs.discrete$graphNEL) #Plot the causal model", 
            "title": "Discrete Dataset"
        }, 
        {
            "location": "/r-causal/#prior-knowledge", 
            "text": "", 
            "title": "Prior Knowledge"
        }, 
        {
            "location": "/r-causal/#create-priorknowledge-object", 
            "text": "forbid  - list(c('TangibilityCondition','Impact')) # List of forbidden directed edges\nrequire  - list(c('Sympathy','TangibilityCondition')) # List of required directed edges\nforbiddenWithin  - c('TangibilityCondition','Imaginability')\nclass(forbiddenWithin)  - 'forbiddenWithin' # Make this tier forbidden within\ntemporal  - list(forbiddenWithin, c('Sympathy','AmountDonated'),c('Impact')) # List of temporal node tiers\nprior  - priorKnowledge(forbiddirect = forbid, requiredirect = require, addtemporal = temporal)\nfgs  - fgs(df = charity, penaltydiscount = 2, depth = -1, ignoreLinearDependence = TRUE, \nheuristicSpeedup = TRUE, numOfThreads = 2, verbose = TRUE, priorKnowledge = prior)", 
            "title": "Create PriorKnowledge Object"
        }, 
        {
            "location": "/r-causal/#load-knowledge-file", 
            "text": "# knowledge file: audiology.prior\n# /knowledge\n# forbiddirect\n# class tymp\n# class age_gt_60\n# class notch_at_4k\n# \n# requiredirect\n# history_noise class\n#\n# addtemporal\n# 0* bser late_wave_poor tymp notch_at_4k o_ar_c ar_c airBoneGap air bone o_ar_u airBoneGap\n# 1 history_noise history_dizziness history_buzzing history_roaring history_recruitment history_fluctuating history_heredity history_nausea\n# 2 class\n\nprior  - priorKnowledgeFromFile('audiology.prior')\nfgs.discrete  - fgs.discrete(df=audiology,structurePrior=1.0,samplePrior=1.0, \ndepth = -1, heuristicSpeedup = TRUE, numOfThreads = 2,verbose = TRUE, priorKnowledge = prior)", 
            "title": "Load Knowledge File"
        }, 
        {
            "location": "/r-causal/#convert-rgraphviz-to-igraph-one", 
            "text": "library(igraph)\nigraph  - igraph.from.graphNEL(fgs.discrete$graphNEL)\nplot(igraph)", 
            "title": "Convert Rgraphviz to igraph one"
        }, 
        {
            "location": "/r-causal/#useful-rjava-trouble-shooting-installation-in-mac-os-x-links", 
            "text": "http://stackoverflow.com/questions/26948777/how-can-i-make-rjava-use-the-newer-version-of-java-on-osx/32544358#32544358  http://andrewgoldstone.com/blog/2015/02/03/rjava/", 
            "title": "Useful rJava Trouble-shooting Installation in Mac OS X Links"
        }
    ]
}